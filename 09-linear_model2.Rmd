```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1)
```


# 線形モデルの注意点

前の章で，線形モデルの全体像を見てきた。  
次に，線形モデルを扱う上で注意すべき点について見ていく。  


## 準備
  
いつもどおり，`tidyverse`パッケージをロードしよう。

```{r, eval=FALSE}
library(tidyverse)
```


## 予測変数がカテゴリカル変数の場合

前の章では，予測変数が量的変数の場合を例として扱ったが，予測変数はカテゴリカル変数（質的変数）でも構わない。ただし，予測変数がカテゴリカル変数の場合は，予測変数を0か1のどちらかの値を取る**ダミー変数**(dummy variable)に変換する必要がある。  
  
Rに入っている`sleep`データを少し変えたもの使って，カテゴリカル変数を使って線形モデルの解析をしてみよう。

```{r}
dat = sleep #データを別の名前datに保存し直す

#変数の名前を変える
dat$x = ifelse(dat$group == 1, "control", "treatment") 
dat$y = dat$extra 
dat = dat %>% dplyr::select(y, x)
str(dat) #datの構成を確認する

```

`x`はグループを意味する変数で，カテゴリカル変数である（統制群`control`もしくは実験群`treatment`）。まずこれを，「`treatment`なら1，`control`なら0」とする新たな変数`x_1`を作る。  

```{r}
dat$x_1 = ifelse(dat$x == "treatment", 1, 0)
str(dat)
```

`ifelse()`関数は，`ifelse(XXX, A, B)`と表記することで，「XXXの条件に当てはまればA，当てはまらなければB」という処理をしてくれる。ここでは，予測変数のベクトルxについて，treatmentならば1, それ以外なら0に変換し，0か1を取る変数$x_{1}$を新たに作った。  
  
この$x_{1}$が*ダミー変数*である。  
  
解析に用いるモデルを確認すると，以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta x_{1} \\ \tag{1}\\
  y \sim Normal(\hat{y}, \sigma)
\end{equation}
$$

$x_{1}$は0か1のどちらかを取る変数で，$x_{1} = 0$のとき，つまり統制群のとき，応答変数の予測値は$\hat{y} = \alpha$となる。$x_{1} = 1$のとき，つまり実験群のとき，応答変数の予測値は$\hat{y} = \alpha + \beta$となる。すなわち，切片$\alpha$は統制群のときの効果，傾き$\beta$は実験群の時に加わる実験群特有の効果を意味する。  

`lm()`を使って，上のモデル式のパラメータの推定をしよう。

```{r}

result = lm(data = dat, y ~ x_1)
summary(result)

```

2つの群間で平均値を比較するときにはt検定がよく使われる。`t.test()`関数を使って$x_{1}=0$と$x_{1}=1$との間で$y$の値の平均値を比較したときのt値及びp値の結果が，`lm()`の傾きのt値及びp値と一致することを確認しよう。

```{r}

t.test(data = dat, y ~ x_1)

```

`lm()`の傾きの検定は，「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは，予測変数$x_{1}$が1単位増えたときの応答変数$y$の変化量であった。傾きの検定は，「$x_{1}=0$ から $x_{1}=1$ に変化することによって， $y$ が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は，「$x_{1}=0$と$x_{1}=1$の間で$y$の値に差があるか」を検定しているのと論理的に同じである。  
  
このように，*予測変数が1つで，予測変数が二値（0もしくは1）であるときの線形モデルは，t検定に対応する*。  


## グループが複数ある場合

先ほどの例は，統制群と実験群の二つのグループの場合であった。例えば実験で統制群，実験群1，実験群2といったように三つ以上のグループを設定した場合は，どうダミー変数を作成すればよいのか？  
  
Rに入っている`PlantGrowth`を例として見ていこう。例えばやり方としては，以下の方法がある。

```{r}
dat = PlantGrowth

dat$y = dat$weight #名前をyに変える

dat$x_c = ifelse(dat$group == "ctrl", 1, 0)
dat$x_t1 = ifelse(dat$group == "trt1", 1, 0)
dat$x_t2 = ifelse(dat$group == "trt2", 1, 0)

str(dat)
head(dat)
```

3種類のダミー変数を作った。それぞれ，`x_c`は「`ctrl`ならば1，それ以外なら0」，`x_t1`は「`trt1`ならば1，それ以外なら0」，`x_t2`は「`trt2`ならば1，それ以外なら0」となっている。これら3つのダミー変数を使ってモデルを作り，パラメータを推定する。  

$$
\begin{equation}
  \hat{y} = \beta_{c} x_{c} + \beta_{t1} x_{t1} + \beta_{t2} x_{t2}  \\ \tag{2}
  y \sim Normal(\hat{y}, \sigma)
\end{equation}
$$

ここで注意が必要なのは，今回のモデルでは切片$\alpha$が省かれていることである。その理由は後ほど説明する。  
  
モデルを`lm()`で記述して，推定してみよう。以下のプログラムを実行する。`lm(data = dat, y ~ x_c + x_t1 + x_t2 - 1)`の中に-1が加わっている点に注意。これは「モデルから切片を除け」という命令である。  

```{r}

result = lm(data = dat, y ~ x_c + x_t1 + x_t2 - 1)
summary(result)

```

それぞれのダミー変数に係る傾きの係数，すなわち式(2)における$\beta_{c}$，$\beta_{t1}$，$\beta_{t2}$の推定結果が出力される。それぞれ，`ctrl`，`trt1`, `trt2`における応答変数(`y`)の推定値を意味している。  

    
### 変数の中心化

上記の例で出力される係数の推定値$\beta_{c}$，$\beta_{t1}$，$\beta_{t2}$は，各条件の平均値と一致している。  
  
```{r}

dat %>% group_by(group) %>% summarise(M = mean(y))

```
  
つまり，このモデルの場合，係数が意味することは各条件での応答変数の推定値（平均）であって，その条件の効果の強さを反映しているわけではない。各係数の有意性検定の結果を見るとp値が非常に低く「有意」であるが，*これらの結果は何の意味も持たない*。係数の有意性検定は係数がゼロから有意に離れているかを検定していて，今回のデータならば応答変数（植物の重量）は正の値を取りうるので，「0より有意に大きい」という結果はある意味当然である。  
  
その条件の効果の強さ（例えば実験条件は植物の生長を促す効果があるのかなど）を係数から直接解釈したいのならば，モデルを組み直す必要がある。  
  
例えば，応答変数から応答変数の平均値を引く**中心化(centering)**という処理を事前に行う。

```{r}

dat$y_2 = dat$y - mean(dat$y) #yからyの平均値を引いた新たな変数y_2を作る

summary(lm(data = dat, y_2 ~ x_c + x_t1 + x_t2 - 1)) #y_2を応答変数として解析する

```

係数の推定結果及び有意性検定の結果が変わった。今度は，x_t1の係数が負でp値も$p<.05$に，x_t2の係数は正でp値も$p<.05$となった。これらが意味していることは，「`x_t1` = 1のときに，`y_2`の値は有意に-0.41下がる」と「`x_t2` = 1のときに，`y_2`の値は有意に0.45上がる」ということである。言い換えれば，「実験条件1では平均よりも植物の重量の値が低く」，「実験条件2では平均よりも植物の重量の値が高い」傾向にあることを示している。  
  
図でも条件別に`y_2`の分布を確認してみよう。分布を見ても同様の傾向があるが，線形モデルの解析の結果その効果が有意であることが確認できた。
  
```{r}

ggplot() + 
  geom_boxplot(data = dat, aes(x = group, y = y_2))

```


このように，係数が持つ意味を直感的に理解しやすくするために，ここでは応答変数を変換した。モデルによっては，応答変数だけではなく，予測変数も中心化する必要がある。  

### 基準となるグループと比較する

もう一つの方法としては，グループの数が$K$個あるのならば，基準となるグループを定めてダミー変数を$K-1$個作る方法である。  
  
以下のプログラムを実行して，データを作り直そう。

```{r}
dat = PlantGrowth

dat$y = dat$weight #名前をyに変える

dat$x_t1 = ifelse(dat$group == "trt1", 1, 0)
dat$x_t2 = ifelse(dat$group == "trt2", 1, 0)

str(dat)
head(dat)
```

今度は，ダミー変数は2つで各条件を表している。`ctrl`のときは「`x_t1` = 0, `x_t2` = 0」,`trt1`のときは「`x_t1` = 1, `x_t2` = 0」,`trt2`のときは「`x_t1` = 0, `x_t2` = 1」となる。

これら2つのダミー変数を予測変数として，`lm()`で`y`を推定しよう。ただし，今度は切片$\alpha$を入れたモデルで推定する。モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{t1} x_{t1} + \beta_{t2} x_{t2}  \\ \tag{3}
  y \sim Normal(\hat{y}, \sigma)
\end{equation}
$$


```{r}
summary(lm(data = dat, y ~ x_t1 + x_t2 + 1)) #1は省略しても可（デフォルトで切片を加えた結果を出力してくれる）

```


式（3）より，切片の推定値は$x_{t1}=0$かつ$x_{t2}=0$のときの$\hat{y}$，つまり統制群(ctrl)のときの応答変数$y$の推定値を意味している。各ダミー変数の係数（傾き）は，切片に加わる各条件の効果を意味している。例えば，`x_t2`の係数は0.49であるが，これは$x_{t2}=1$のとき（つまりtrt2のとき）の応答変数の予測値は， 5.03 + 0.49 = 5.52となることを示している。  
  
このように，グループが$K$個ある場合（$K > 2$），$K-1$個のダミー変数を作って推定する方法もある。係数の意味することは，基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。  
  
  
このように，モデルを組み直すことにより，係数が意味することも変化してくる。モデル（式）を意識しながら，係数の意味を解釈することを意識しよう。

## 予測変数が複数ある場合

前の章でも述べたように，予測変数は2つ以上入れても良い。予測変数が複数ある場合の注意点を見ていく。  
  
### 変数の効果の統制

予測変数を複数加えた線形モデルの解析のメリットは，ある予測変数について他の予測変数の効果を**統制**(control)したときの効果を検討できることにある。  
  
Rで標準で入っている`attitude`データを使って，予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。

```{r}
head(attitude)
```

以下のように，`complaints`, `privileges`, `learning`, `raises`の4つを予測変数として，`rating`の値の推定を行ってみよう。

```{r}
result = lm(data = attitude, rating ~ complaints + privileges + learning + raises)
summary(result)
```


切片（Intercept）は全ての予測変数の値がゼロのときの応答変数の予測値であり，各予測変数の係数は予測変数が1単位増えた場合の応答変数の変化量を意味している。例えば，complaintsの係数は`r round(result$coefficients[2], 2)`であるが，これは「`complaints`が1増えると`rating`は`r round(result$coefficients[2], 2)`増える傾向にある」ことを意味している。  
  
各係数の値は「他の変数の値がゼロであるときの効果」を意味している。先程の`complaints`の係数`r round(result$coefficients[2], 2)`は，その他の予測変数`privileges`, `learning`, `raises`がゼロのときの，`complaints`が`rating`に与えるそのものの効果を示している。  
  
このように複数の予測変数を入れたモデルで推定される係数は，他の予測変数の効果を統制した上での予測変数が応答変数に及ぼす効果を意味する。   


### 交互作用


以下のプログラムを実行して，サンプルデータ`d`を作ろう。


```{r}

set.seed(1)
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")

x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

d = rbind(d_M, d_F)

str(d)


```


このデータ`d`には，`x`, `y`, `gender`の3つの変数が含まれている。`gender`は性別を意味する変数としよう。M（男性）かF（女性）のいずれかである。男女別に，実験で2つの変数を測定したとしよう。

応答変数を`y`，予測変数を`x`として線形モデルで切片及び`x`の傾きのパラメータを推定する。モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta x  \\ \tag{4}
  y \sim Normal(\hat{y}, \sigma)
\end{equation}
$$


`lm()`関数を使って推定しよう（$x$と$y$の散布図及び係数の信頼区間も図示する）。

```{r}
result = lm(data = d, y ~ x)
summary(result)

newdat = data.frame(x = seq(1,10,0.1))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(x = seq(1,10,0.1), result_conf)

ggplot() + 
  geom_point(data = d, aes(x = x, y = y), size = 3) + 
  geom_line(data = plot_conf, aes(x = x, y = fit)) + 
  geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 
  

```

予測変数`x`の傾きはほぼフラットで，`y`に対してあまり効果がないようにみえる。  
  
しかし，このデータ`d`にはもう一つ性別を意味する`gender`という変数が含まれていた。`gender`を区別して，また`x`と`y`の散布図を見てみよう。

```{r}
ggplot() + 
  geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 

```

性別が女性（F）か男性（M）かで，`x`と`y`の関係が違うようである。  
このように，別の変数との組み合わせにより，変数間の関係が変化することを**交互作用(interaction)**という。このデータでも，応答変数`y`に対して性別`gender`と`x`の交互作用がありそうである。  
  
交互作用のあるモデルは，以下のように表現する。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x + \beta_{2} M + \beta_{3} xM  \\ \tag{5}
  y \sim Normal(\hat{y}, \sigma)
\end{equation}
$$

$M$は性別`gender`のダミー変数で，`M`（男性）ならば1，`F`（女性）ならば0の変数とする。  
線形モデルでは，**交互作用は予測変数同士の積**で扱う。男性（M=1）の場合のyの推定値は，$\alpha +(\beta_{1} + \beta_{3}) x +\beta_{2}$となる。一方，女性（M=0）の場合は，$\alpha +\beta_{1} x$となる。$\beta_{3}$は，男性のときの$x$に係る傾きの変化量を意味することになる。このように，交互作用を考慮する予測変数の積をモデルに加えることで，男性か女性かで切片及び傾きが変化することを表現できる。

```{r}

d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る
result = lm(data = d, y ~ x*M)
summary(result)

```

2つの予測変数の積の傾き（$\beta_{3}$）は，`x:M`である。p値も小さく，有意な効果を持っているようである。  
  
ここで注意が必要なのは，交互作用を含む線形モデルの係数は解釈が複雑になることである。  

  
男性(M = 1)の予測値は，線形モデルの式に推定された傾きと切片及び$M=1$を代入して，(`r as.numeric(round(result$coefficients[1],2))` + `r as.numeric(round(result$coefficients[3],2))`) + (`r as.numeric(round(result$coefficients[2],2))` +  `r as.numeric(round(result$coefficients[4],2))`) $x$ となる。女性(M = 0)の場合は，`r as.numeric(round(result$coefficients[1],2))` `r as.numeric(round(result$coefficients[2],2))` $x$ となる。  
  
`x`と`M`の傾きの推定値は，`x`や`M`そのものの効果（いわゆる主効果）を反映しなくなる。  
  
交互作用効果が見られた場合は，解釈は慎重に行う必要がある。  
  
サンプルデータについて，推定されたパラメータを元に，男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。  
  
```{r}

new_x = seq(1,10,0.1)
newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x))))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(newdat, result_conf)
plot_conf$gender = ifelse(plot_conf$M == 1, "M", "F")

ggplot() + 
  geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + 
  geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + 
  geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 

```



## 線形モデルが抱える問題

予測変数を増やすと，他の予測変数を統制することによって，その予測変数が応答変数に及ぼすそのものの効果を検討することができる。ただし，予測変数を加えることで生じる問題もある。以降では，**多重共線性**と**過学習**の問題について触れる。

### 多重共線性

予測変数同士が非常に強く相関しあっている場合，予測変数の係数の推定結果が信頼できなくなる恐れがある。この問題は，**多重共線性(multicollinearity)**と呼ばれる。  
  
サンプルデータを使って確認してみよう。Rには多重共線性の例として`longley`というサンプルデータがある。  
  
```{r}
head(longley)
```

まず，このデータに入っている変数間の相関を確認してみよう。

```{r}

cor(longley)

```

`Employed`，`GNP.deflator`を予測変数としたモデル（model01）と，`Employed`を応答変数，`GNP`を予測変数としたモデル（model02）でそれぞれ解析してみよう。

```{r}

model01 = lm(data = longley, Employed ~ GNP.deflator)
summary(model01)

model02 = lm(data = longley, Employed ~ GNP)
summary(model02)


```

次に，`Employed`を応答変数，`GNP`と`GNP.deflator`の両方を予測変数として入れて解析をしてみよう。


```{r}

model03 = lm(data = longley, Employed ~ GNP.deflator + GNP)
summary(model03)

```

それぞれの予測変数の係数を見てみると，一つずつ予測変数として入れたときと比べて値が変わっており，p値も低くなっている。  
  
`GNP`と`GNP.deflator`同士は相関係数`r round(cor(longley$GNP, longley$GNP.deflator), 2)`とかなり強く相関している。このように，強く相関し合う変数を入れると係数の効果について信頼できる結果が得られなくなってしまう。  
  
なぜ強く相関しあっている変数を入れるとまずいのか？モデルから考えてみよう。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x_{1} + \beta_{2} x_{2} \\ \tag{6}
\end{equation}
$$

2つの予測変数$x_{1}$と$x_{2}$が強く相関している場合，つまり$x_{1}=x_{2}$だとすると，式(6)は以下のように置き換えることができる。

$$
\begin{equation}
  \hat{y} = \alpha + (\beta_{1} + \beta_{2}) x_{1} \\ \tag{7}
\end{equation}
$$

$(\beta_{1} + \beta_{2})$について，パラメータ$\beta_{1}$と$\beta_{2}$の組み合わせは無限に考えられる。このように，強く相関する予測変数を入れると２つの予測変数のパラメータについて推定することが難しくなってしまう（パラメータの信頼区間が大きくなってしまう）。  
  
#### 多重共線性への対処 {-}

多重共線性の対策として，**VIF(variance inflation factor)**という指標がよく用いられる。一般的に，$VIF > 10$の場合は，多重共線性を疑った方が良いといわれている。VIFの高い変数同士のうちどちらか一方を予測変数から除くといった対処をして，解析し直してみるのが良い。

`car`パッケージの`vif()`関数を使えば，VIFを算出することができる。

```{r}

library(car)
vif(model03)

```



### 過学習

以下のプログラムを実行して，サンプルデータ`d`を作成しよう。

```{r}

set.seed(10)

N = 10
x = seq(1,N,1)
y = runif(N, min = 1, max = 5)
d = data.frame(x = x, y = y)
str(d)

ggplot() + 
  geom_point(data =d, aes(x=x, y = y))

```

このデータについて，以下の線形モデルを当てはめ，パラメータを推定しよう。図に線形モデルの直線及び信頼区間を図示するところまでやってみる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x\\ \tag{8}
\end{equation}
$$

```{r}

result_1 = lm(data = d, y ~ x)

newdat = data.frame(x = x)
conf.int_1 = predict(result_1, newdata = newdat, interval = "confidence", level = 0.95)
conf_dat = data.frame(d, newdat, conf.int_1)


ggplot() + 
  geom_point(data = conf_dat, aes(x = x, y = y))+
  geom_line(data = conf_dat, aes(x = x, y = fit)) +
  geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2)

```

直線はほとんど観測値から外れており，当てはまりが悪いようである。  
  
そこで，予測変数を増やして検討してみる。`lm()`では，予測変数$x$のn乗を含む多項式のモデルを考慮することも可能である。例えば，以下は3次の多項式の例である。  

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x  + \beta_{2} x^{2}  + \beta_{3} x^{3}\\ \tag{9}
\end{equation}
$$

n次式のモデルは**多項式回帰**(polynomial regression)と呼ばれる。  
    
`lm()`では，`I()`の中に書くかたちでn次の予測変数を入れることができる。  

```{r}

result_3 = lm(data = d, y ~ x + I(x^2) + I(x^3))

```

同じく，3次の多項式による予測の結果を図で確認しよう。

```{r}

newdat = data.frame(x = x)
conf.int_3 = predict(result_3, newdata = newdat, interval = "confidence", level = 0.95)
conf_dat = data.frame(d, newdat, conf.int_3)


ggplot() + 
  geom_point(data = conf_dat, aes(x = x, y = y))+
  geom_line(data = conf_dat, aes(x = x, y = fit)) + 
  geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2)

```


9次の式でも推定してみよう。  

```{r, warning=FALSE, message=FALSE}

result_9 = lm(data = d, y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9))


newdat = data.frame(x = x)
conf.int_9 = predict(result_9, newdata = newdat, interval = "confidence", level = 0.95)
conf_dat = data.frame(d, newdat, conf.int_9)


ggplot() + 
  geom_point(data = conf_dat, aes(x = x, y = y))+
  geom_line(data = conf_dat, aes(x = x, y = fit)) + 
  geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2)

```

線は全てのデータ点を完全に通っている。当然ながら，データの観測値の分だけパラメータがあれば，そのモデルはデータ点を全て通る線を引くことができる。現在のデータ点全てを予測することができる。  
  
しかし，そのモデルは現在のデータを全て当てられても，*将来得られる未知のデータを当てられるとは限らない*。予測変数を多くすると現在のデータには当てはまるが，当てはまりすぎて未知のデータの予測力が低下してしまうことを，**過学習(overfitting)**という。  
  
複雑なモデルが現在のデータによく当てはまるのは，ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし，複雑なモデルは現在のデータに当てはまっても，未知のデータにもうまく当てはまるとは限らない。  
  
理想的なモデルは，「**予測力が高く，かつ予測変数ができるだけ少なくてシンプルなモデル**」となる。


## モデルの予測力の評価

モデルの予測力を評価するための指標について説明する。

### 決定係数 

線形モデルでは，データに対する回帰分析のモデルの予測力を表す指標として，**決定係数**（R-squared）がある。  
  
サンプルデータ`attitude`を例に見てみよう。

```{r, echo=TRUE}

model_01 = lm(data=attitude, rating ~ complaints + learning)
summary(model_01)$r.squared #r.squaredで決定係数のみを取り出すことができる。

```

これは，モデルから求めた予測値と実測値の分散が，実際のデータの分散に占める割合を意味する指標である。つまり，そのモデルでどれだけ全データの分散を説明できているかを意味する。

$$
R^2 = \sum_{i=1}^{n} \frac {(y_{i}-\hat{y}_{i})^2}{(y_{i}-\bar{y})^2} \tag{10}
$$

ただし，決定係数は単純に，予測変数が増えるほど大きくなる（説明できる分散の量が増える）。  
  
例えば`attitude`データ内の全ての変数を予測変数に使ってみる

```{r, echo=TRUE}

model_full = lm(data=attitude, rating ~ .) #線形予測子を入力するところにドットを入力すると，そのデータに含まれる全ての変数を予測変数として扱う
summary(model_full)$r.squared 

```

予測変数に影響を及ぼさない変数を含めても，決定係数は上昇してしまう。  
  
決定係数は，「予測力が高く，シンプルなモデル」を探すには常に適切な指標であるとは言えない。

### 赤池情報量基準（AIC）

予測変数の少なさとモデルの予測力の高さのバランスを取った指標の一つとして，**赤池情報量基準(Akaike inoformation criteria: AIC)**がよく知られている。AICは以下の式で計算される。

$$
AIC = -2 \log L + 2k \tag{11}\\
$$

$\log L$は最大対数尤度，$k$はモデルのパラメータ数である。  

第9章で，モデルのパラメータを推定する方法として「最尤法」を紹介した。最尤法は，モデルのもっともらしさ（データが生じる確率）を意味する「対数尤度」が最大となるときのパラメータを求める方法であった。最大対数尤度は，現在のモデルに対する当てはまりの良さを反映している。その最大対数尤度に対し，パラメータ数$k$に応じてペナルティ(penalty term)を加える。  
  
AICの値が低いほど，モデルの予測力が高いと評価する。AICは余計なパラメータが多くなる（$k$が大きくなる）ほど大きい値を取る。つまり，データをうまく予測しつつ，かつパラメータ数を抑えてシンプルなモデルを探る目的にかなっている。  
  
`AIC()`関数でモデルをカッコ内に入れると，AICを算出してくれる。さきほどの`attitude`に当てはめた2つのモデルのAICを見てみよう。

```{r}

AIC(model_full)
AIC(model_01)

```

`model_full`よりも`model_01`のAICが低く，`model_01`の予想力の方が高いことを示している。

  
## 確認問題 {-}
  
### 問1 {-}

Rで標準で入っているデータ`warpbreaks`を使って練習をする。  


```{r}

prac_dat_1 = warpbreaks #別の名前で保存する

head(prac_dat_1)
ggplot() + 
  geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks))
ggplot() + 
  geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks))

```


#### 1-1 {-}


変数`wool`について, 「`A`を1, それ以外を0」としたダミー変数を作成し，そのダミー変数を予測変数，`breaks`を応答変数として線形モデルを行い，切片及びダミー変数に係る傾きの推定値を報告せよ。  
  
また，ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。

```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy = ifelse(prac_dat_1$wool == "A", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ dummy))

```

#### 1-2 {-}

変数`tension`について, 「`L`を1, それ以外を0」，「`M`を1, それ以外を0」とした2種類のダミー変数を作成し，それら2つのダミー変数を予測変数，`breaks`を応答変数として線形モデルを行い，切片及び各ダミー変数に係る傾きの推定値を報告せよ。    
更に，そのときの切片及び各ダミー変数の係数が意味することを説明せよ。  


```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ dummy_1 + dummy_2))

```


#### 1-3 {-}


1-2で作ったダミー変数に加え，更に「`H`を1, それ以外を0」としたダミー変数を追加で作成する。  
更に，`breaks`から全体の`breaks`の平均を引いた変数`breaks_2`を作成する。  
  
それら3つのダミー変数を予測変数，`breaks_2`を応答変数として線形モデルを行い，各ダミー変数に係る傾きの推定値を報告せよ。ただし，モデルには切片の項は加えないものとする。      
  
更に，そのときの各ダミー変数の係数が意味することを説明せよ。  


```{r eval=FALSE, include=FALSE}

prac_dat_1$breaks_2 = prac_dat_1$breaks - mean(prac_dat_1$breaks)

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)
prac_dat_1$dummy_3 = ifelse(prac_dat_1$tension == "H", 1, 0)

summary(lm(data = prac_dat_1, breaks_2 ~ dummy_1 + dummy_2 + dummy_3 - 1))

```

  
### 問2 {-}

問1に引き続き，Rで標準で入っているデータ`warpbreaks`を使って練習をする。ただし，`tension`が`H`の部分を除いたデータを用いる。 


```{r}

library(tidyverse)
prac_dat_2 = warpbreaks %>% filter(tension != "H") #tension == Hは除き，別の名前で保存する

head(prac_dat_2)
ggplot() + 
  geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension))

```


breaksを応答変数，wool, tension, woolとtensionの交互作用項を予測変数とした線形モデルを行い，切片，woolの傾き，tensionの傾き，交互作用項の推定値を報告せよ。  

```{r eval=FALSE, include=FALSE}

summary(lm(data = prac_dat_2, breaks ~ wool * tension))

```

### 問3 {-}

Rで標準で入っている`airquality`を使う。

```{r}

prac_dat_3 = airquality %>% na.omit() #欠損値を除き，別の名前で保存する

head(prac_dat_3)

```


#### 3-1 {-}

`Ozone`を応答変数，`Solar.R`, `Wind`, `Temp`を予測変数とした線形モデルを行う。そして，切片及び傾きの推定値を報告せよ。

```{r eval=FALSE, include=FALSE}

summary(lm(data = prac_dat_3, Ozone ~ Solar.R + Wind + Temp))

```

#### 3-2 {-}

3-1で行った線形モデルについて，決定係数を報告せよ（Multiple R-squared）。

#### 3-3 {-}

以下の3種類の線形モデルの解析を行い，  
モデル1: `Ozone`を応答変数，`Solar.R`, `Wind`, `Temp`を予測変数とした線形モデル  
モデル2: `Ozone`を応答変数，`Solar.R`,  `Temp`を予測変数とした線形モデル  
モデル3: `Ozone`を応答変数，`Temp`を予測変数とした線形モデル  
  
それぞれのモデルのAICを報告するとともに，3つのモデルのうち予測力が高いと考えられるものはどれかを報告せよ。

```{r eval=FALSE, include=FALSE}

AIC(lm(data = prac_dat_3, Ozone ~ Solar.R + Wind + Temp))
AIC(lm(data = prac_dat_3, Ozone ~ Solar.R + Temp))
AIC(lm(data = prac_dat_3, Ozone ~ Temp))

```