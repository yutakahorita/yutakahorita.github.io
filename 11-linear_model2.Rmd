```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(knitr)
set.seed(1)
```


# 線形モデル（t検定, 分散分析, 共分散分析）{#chap11_lmmodels}

前の章では、線形モデルの概要を見てきた。前の章で線形モデルを用いて行った分析は、要は「単回帰分析（一つの連続変量から、もう一方の連続変量の値を予測する分析）」であった。  
  
この章では、線形モデルを用いて他の分析を行う。予測変数のパターンが異なるケースの演習を通して、線形モデルは正規分布を扱うあらゆるタイプの分析を内包したモデルであることを確認していく。  

* 変数の標準化  
* 予測変数がカテゴリの場合（ダミー変数）  
* 予測変数がカテゴリの場合（複数のダミー変数）  
* 交互作用    


## 準備{#chap11_preparation}
  
必要なパッケージをロードする。

```{r, eval=FALSE}
library(dplyr)
library(ggplot2)
```

  
## 線形モデルに含まれる統計解析{#chap11_lmgroup}  
  
線形モデルとは特定の解析を指すものではなく、正規分布を扱う様々な統計解析を包括的に扱う統計モデルである。例えば、変数の正規性を前提とするt検定や分散分析も線形モデルの中に含まれる。予測変数の種類や個数の違いによって、線形モデルは以下のそれぞれの統計解析と一致する。
  
```{r, echo=FALSE}

knitr::kable(data.frame(分析 = c("t検定", "分散分析", "共分散分析", "単回帰分析", "重回帰分析"),
                          予測変数 = c("二値(0 or 1)", "二値", "二値及び連続量", "連続量", "連続量（二値を含んでも可）"),
                          予測変数の個数 = c("1個", "2個以上", "二値が2個以上、連続量が1個以上", "1個", "2個以上")))

```

前章では単回帰分析（予測変数が連続量で1個）を例として演習を行った。以下では、t検定や分散分析と似たことを`lm()`関数で行う練習を通して、**予測変数がカテゴリーの場合**の扱い方を学んでいく。


## 変数の標準化{#chap11_scaling}

解析の演習に入る前に、**標準化(standardizing)**について確認しておこう。標準化とは、元の値を「ゼロが平均値、1が標準偏差」と等しくなるように値を変換する処理のことをいう。変数を標準化しておくと、線形モデルの係数の解釈が直感的に理解しやすくなる事が多い。  
  
例えば、前の章では`iris`データを使って単回帰分析を行った。

```{r}

dat = iris #ここではirisを別の名前（dat）に変えて練習に用いる
result = lm(data = dat, Petal.Length ~ 1 + Sepal.Length)
summary(result)

```

切片の値は`Sepal.Length`がゼロのときの`Petal.Length`の予測値である。しかし、アヤメの花弁の長さがマイナスやゼロの値を取るというのはありえない。また、切片は予測変数がゼロのときの応答変数の値であるが、このデータも予測変数（すなわちがくの長さ）がゼロの場合も論理的には有り得ない。このように、元の値をそのまま使っても係数の解釈にこまる場面がある。  
  
  
応答変数及び予測変数を標準化したものを使って同じ解析をして、結果を比較してみよう。具体的には、元の得点から**平均値を引いて**差の得点を求め、その差の得点を**標準偏差で割る**。


```{r}

dat = dat |> dplyr::mutate(Petal.Length_std = (Petal.Length - mean(Petal.Length))/sd(Petal.Length),
                           Sepal.Length_std = (Sepal.Length - mean(Sepal.Length))/sd(Sepal.Length))

head(dat)

```

標準化した得点を用いて線形モデルで解析を行う。

```{r}

result_std = lm(data = dat, Petal.Length_std ~ 1 + Sepal.Length_std)
summary(result_std)

```


標準化しない得点を使ったときの解析結果と比べると、係数の値が変わっている。しかし、傾きのt値及びp値は変わっていない（標準化は単にデータを平行移動させただけなので、回帰直線の傾きは変わらない）。  
  
切片は`r as.numeric(round(result_std$coefficients[1],2))`、`Sepal.Length_std`の効果は`r as.numeric(round(result_std$coefficients[2],2))`である。切片の値は、`Sepal.Length_std`がゼロのとき、つまり`Sepal.Length`が平均値と等しいとき、`Petal.Length_std`はほぼゼロの値を取る、つまり`Petal.Length`の平均値であることを意味している。また、`Sepal.Length_std`の傾きは、`Sepal.Length_std`が1のとき（つまり`Sepal.Length`が1標準偏差分増加したとき）、`Petal.Length_std`が`r as.numeric(round(result_std$coefficients[2],2))`増えることを意味する。  
  
このように、標準化することで算出された係数の解釈がしやすくなることが多い。以降の線形モデルを扱う分析の例でも、応答変数は標準化したものを用いる。

***
Rには標準化を行うための関数も用意されている。`scale()`を用いる。

```{r}

dat = iris |> dplyr::mutate(Petal.Length_std = scale(Petal.Length)) 

head(dat)

```


## 予測変数がカテゴリカル変数の場合（ダミー変数）{#chap11_dummy}

予測変数はカテゴリカル変数（質的変数）でも構わない。ただし、予測変数を0か1のどちらかの値を取る**ダミー変数**(dummy variable)に変換する必要がある。  
  
Rに入っている`sleep`データを少し変えたもの使って、カテゴリカル変数を予測変数に含む線形モデルの解析をしてみよう。

```{r}
sleep_1 = sleep |> dplyr::select(-ID) |> 
  dplyr::mutate(group = ifelse(group == 1, "control", "treatment"))
sleep_1
```

`group`はグループを意味する変数である（統制群`control`もしくは実験群`treatment`）。まずこれを、「`treatment`なら1、`control`なら0」とする新たな変数`dgroup`を作る。  

```{r}
sleep_1 = sleep_1 |> dplyr::mutate(dgroup = ifelse(group == "treatment", 1, 0))
sleep_1
```

`ifelse()`関数は、`ifelse(XXX, A, B)`と表記することで、「XXXの条件に当てはまればA、当てはまらなければB」という処理をしてくれる。ここでは、変数`group`について、`treatment`ならば1, それ以外なら0に変換し、0か1を取る変数`dgroup`を新たに作った。  
  
この`dgroup`が*ダミー変数*である。  
  
解析に用いるモデルを確認すると、以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta dgroup \\ \tag{1}\\
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$

$dgroup$は0か1のどちらかを取る変数で、$dgroup = 0$のとき、つまり統制群のとき、応答変数の予測値は$\mu = \alpha$となる。$dgroup = 1$のとき、つまり実験群のとき、応答変数の予測値は$\mu = \alpha + \beta$となる。すなわち、切片$\alpha$は統制群のときの効果、傾き$\beta$は実験群の時に加わる実験群特有の効果を意味する。  

`lm()`を使って、上のモデル式のパラメータの推定をしよう。

```{r}
sleep_1 = sleep_1 |> dplyr::mutate(extra_std = scale(extra)) #応答変数を標準化したものを用いる

result = lm(data = sleep_1, extra_std ~ 1 + dgroup)
summary(result)

```

2つの群間で平均値を比較するときにはt検定がよく使われる。`t.test()`関数を使って$dgroup=0$と$dgroup=1$との間で$y$の値の平均値を比較したときのt値及びp値の結果が、`lm()`の傾きのt値及びp値と一致することを確認しよう。

```{r}

t.test(data = sleep_1, extra_std ~ dgroup)

```

`lm()`の傾きの検定は、「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは、予測変数$dgroup$が1単位増えたときの応答変数$y$の変化量であった。傾きの検定は、「$dgroup=0$ から $dgroup=1$ に変化することによって、 応答変数（`extra`） が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は、「$dgroup=0$と$dgroup=1$の間で$y$の値に差があるか」を検定しているのと論理的に同じである。  
  
このように、*予測変数が1つで、予測変数が二値（0もしくは1）であるときの線形モデルは、t検定に対応する*。  


## グループが複数ある場合（一要因の分散分析）{#chap11_onewayANOVA}

先ほどの例は、統制群と実験群の二つのグループの場合であった。例えば実験で統制群、実験群1、実験群2といったように三つ以上のグループを設定した場合は、どうダミー変数を作成すればよいのか？  
  
Rに入っている`PlantGrowth`を例として見ていこう。以下のプログラムを実行して、データを作ろう。


```{r}
dat = PlantGrowth |>
  dplyr::mutate(t1 = ifelse(group == "trt1", 1, 0),
                t2 = ifelse(group == "trt2", 1, 0)
                )
dat
```

ダミー変数を2つ作成した。`ctrl`のときは「`t1` = 0, `t2` = 0」,`trt1`のときは「`t1` = 1, `t2` = 0」,`trt2`のときは「`t1` = 0, `t2` = 1」となっている。これら２つのダミー変数を線形予測子に加えたモデルは、モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{t1} t_1 + \beta_{t2} t_2  \\ \tag{3}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$
切片$\alpha$は「`t1` = 0, `t2` = 0（すなわち、`ctrl`のとき）」の$\hat{y}$の値と等しい。  
傾き$\beta_{t1}$は「`t1` = 1, `t2` = 0（すなわち、`trt1`のとき）」の$\hat{y}$の値と等しい。  
傾き$\beta_{t2}$は「`t1` = 0, `t2` = 1（すなわち、`trt2`のとき）」の$\hat{y}$の値と等しい。  
  
このように、グループの数が$K$個ある場合には、ダミー変数を$K-1$個作れば全てのグループの予測値$\hat{y}$を線形予測子で表すことができる。

`lm()`で傾き及び切片のパラメータを推定しよう。

```{r}

dat = dat |> dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる
result = lm(data = dat, weight_std ~ t1 + t2 + 1) 
summary(result)

```


式（3）より、切片の推定値は$t_1=0$かつ$t_2=0$のときの$\mu$、つまり統制群(ctrl)のときの応答変数`weight_std`の推定値を意味している。各ダミー変数の係数（傾き）は、切片に加わる各条件の効果を意味している。例えば、`t2`の係数は`r round(as.numeric(result$coefficients[3]),2)`であるが、これは$t_2$のとき（つまりtrt2のとき）の応答変数の予測値は、 `r round(as.numeric(result$coefficients[1]) + as.numeric(result$coefficients[3]),2)`(= 切片 + `t2`の傾き)となることを示している。  
  
係数の意味することは、基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。  
  

図でも条件別に`weight_std`の分布を確認してみよう。分布を見ても同様の傾向があるが、線形モデルの解析の結果その効果が有意であることが確認できた。
  
```{r}

ggplot() + 
  geom_boxplot(data = dat, aes(x = group, y = weight_std))

```


モデル（式）を確認しながら、係数が何を意味しているのかを常に意識するようにしよう。

***

このテキストでは練習のためにダミー変数を自分でプログラムを書いて作っているが、`lm()`関数にカテゴリカル変数をそのまま入れても結果を出力してくれる。カテゴリのうちアルファベット順で最初に出てくるカテゴリ(以下の`PlantGrowth`の例では`ctrl`)を基準として、残りのダミー変数を自動で作ってくれている。

```{r}

dat = PlantGrowth |> dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる
result = lm(data = dat, weight_std ~ group) 
summary(result)

```



## 交互作用（2要因の分散分析）{#chap11_interaction}

次は、線形モデルで**交互作用**を扱う方法について確認する。2要因以上の分散分析と同様のことを線形モデルで行う。  
  
以下のプログラムを実行して、サンプルデータ`d`を作ろう。


```{r}

set.seed(1)
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")

x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

d = rbind(d_M, d_F)

head(d)

```


このデータ`d`には、`x`, `y`, `gender`の3つの変数が含まれている。`gender`は性別を意味する変数とする。M（男性）かF（女性）のいずれかである。男女別に、実験で2つの変数を測定したとしよう。

応答変数を`y`、予測変数を`x`として線形モデルで切片及び`x`の傾きのパラメータを推定する。モデルは以下のようになる。

$$
\begin{equation}
  \hat{y} = \alpha + \beta x  \\ \tag{4}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$


`lm()`関数を使って推定しよう（$x$と$y$の散布図及び係数の信頼区間も図示する）。

```{r}
result = lm(data = d, y ~ 1 + x)
summary(result)

newdat = data.frame(x = seq(1,10,0.1))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(x = seq(1,10,0.1), result_conf)

ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y), size = 3) + 
  ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit)) + 
  ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 
  
```

予測変数`x`の傾きはほぼフラットで、`y`に対してあまり効果がないようにみえる。  
  
しかし、このデータ`d`にはもう一つ性別を意味する`gender`という変数が含まれていた。`gender`を区別して、また`x`と`y`の散布図を見てみよう。

```{r}
ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 

```

性別が女性（F）か男性（M）かで、`x`と`y`の関係が違うようである。  
このように、別の変数との組み合わせにより、変数間の関係が変化することを**交互作用(interaction)**という。このデータでも、応答変数`y`に対して性別`gender`と`x`の交互作用がありそうである。  
  
交互作用のあるモデルは、以下のように表現する。

$$
\begin{equation}
  \hat{y} = \alpha + \beta_{1} x + \beta_{2} M + \beta_{3} xM  \\ \tag{5}
  y \sim \text{Normal}(\hat{y}, \sigma)
\end{equation}
$$

$M$は性別`gender`のダミー変数で、`M`（男性）ならば1、`F`（女性）ならば0の変数とする。  
線形モデルでは、**交互作用は予測変数同士の積**で扱う。男性（M=1）の場合の$\mu$の推定値は、$\mu = \alpha +(\beta_{1} + \beta_{3}) x +\beta_{2}$となる。一方、女性（M=0）の場合は、$mu = \alpha +\beta_{1} x$となる。$\beta_{3}$は、男性のときの$x$に係る傾きの変化量を意味することになる。このように、交互作用を考慮する予測変数の積をモデルに加えることで、男性か女性かで切片及び傾きが変化することを表現できる。

```{r}

d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る
result = lm(data = d, y ~ 1 + x*M)
summary(result)

```

2つの予測変数の積の傾き（$\beta_{3}$）は、`x:M`である。p値も小さく、有意な効果を持っているようである。  
  
サンプルデータについて、推定されたパラメータを元に、男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。  
  
```{r}

new_x = seq(1,10,0.1)
newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x))))
result_conf = predict(result, new = newdat, interval = "confidence", level = 0.95)
plot_conf = data.frame(newdat, result_conf)
plot_conf$gender = ifelse(plot_conf$M == 1, "M", "F")

ggplot2::ggplot() + 
  ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + 
  ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + 
  ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 

```

### 交互作用を扱うモデルの解釈{#chap11_NotingInteraction}  
  
2要因以上の分散分析と同様に、線形モデルでも交互作用の有無を検討することができる。しかし、ここで注意が必要なのは、交互作用を含む線形モデルは解釈が複雑になることである。
　　
分散分析では、変数の組み合わせの効果である交互作用とは別に、その変数そのものの効果である主効果についての検定も行われる。では、上述の線形モデルの$\beta_{1}$及び$\beta_{2}$のパラメータはそれぞれ、性別$M$及び予測変数$x$の主効果として解釈できるのだろうか？  
  
例えば$\beta_{1}$について見てみよう。先述のように、男性（M=1）の場合の$y$の推定値は$\hat{y} = \alpha +(\beta_{1} + \beta_{3}) x +\beta_{2}$、女性（M=0）の場合は$\hat{y} = \alpha +\beta_{1} x$となる。すなわち、$\beta_{1}$は「女性のときのxの傾き」を意味し、性別が無関係なxの効果を必ずしも意味しない。 

このように、交互作用を含むモデルの場合、**予測変数の傾きは必ずしも主効果を意味するわけではない**。次の節の補足で、この問題に対する対処法について説明する。


## 予測変数が複数ある場合（共分散分析または重回帰分析）{#chap11_ANCOVA}

予測変数は連続量もカテゴリカル変数でも何でも含めても良い。  
  
### 変数の効果の統制{#chap11_Controlling}

予測変数を複数加えた線形モデルの解析のメリットは、ある予測変数について他の予測変数の効果を**統制(control)**したときの効果を検討できることにある。  
  
Rで標準で入っている`attitude`データを使って、予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。

```{r}
head(attitude)
```

以下のように、`complaints`, `privileges`, `learning`, `raises`の4つを予測変数として、`rating`の値の推定を行ってみよう。

```{r}
dat = attitude |> dplyr::mutate(rating_std = scale(rating),
                                complaints_std = scale(complaints),
                                privileges_std = scale(privileges),
                                learning_std = scale(learning),
                                raises_std = scale(raises)
                                ) #変数を標準化したものを使う

result = lm(data = dat, rating_std ~ 1 + complaints_std + privileges_std + learning_std + raises_std)
summary(result)
```


切片（Intercept）は全ての予測変数の値がゼロ（ここでは標準化しているので平均）のときの応答変数の予測値であり、各予測変数の係数は予測変数が1単位（1標準偏差）増えた場合の応答変数の変化量を意味している。例えば、`complaints_std`の係数は`r round(as.numeric(result$coefficients[2]), 2)`であるが、これは「`complaints`が1増えると`rating`は`r round(as.numeric(result$coefficients[2]), 2)`増える傾向にある」ことを意味している。  
  
各係数の値は「他の変数の値がゼロであるときの効果」を意味している。先程の`complaints`の係数`r round(as.numeric(result$coefficients[2]), 2)`は、その他の予測変数`privileges`, `learning`, `raises`がゼロのときの、`complaints`が`rating`に与えるそのものの効果を示している。  
  
このように複数の予測変数を入れたモデルで推定される係数は、他の予測変数の効果を統制した上での予測変数が応答変数に及ぼす効果を意味する。   


***

交互作用を含むモデルの場合も、（ダミー変数も含めて）予測変数を標準化すると係数を解釈しやすくなる。  
  

```{r}

#データを再度作成する。
set.seed(1)
x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.1 + 0.4 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_M = data.frame(x = x, y = y, gender = "M")

x = round(runif(n = 20, min = 1, max = 10),0) 
mu = 0.3 + -0.6 * x
y = rnorm(n = 20, mean = mu, sd = 1)
d_F = data.frame(x = x, y = y, gender = "F")

d = rbind(d_M, d_F)
d$M = ifelse(d$gender == "M", 1, 0) #genderがMならば1, Fならば1のダミー変数を作る

```

```{r}

#標準化する前の結果
result = lm(data = d, y ~ 1 + x*M)
summary(result)

```

応答変数と予測変数を標準化する。ダミー変数も標準化する。

```{r}

#変数を標準化
d = d |> dplyr::mutate(y_std = scale(y),
                       x_std = scale(x),
                       M_std = scale(M)
                       )
head(d)

result = lm(data = d, y_std ~ 1 + x_std*M_std)
summary(result)

```

それぞれの係数とp値が変わった。それぞれの係数の値は、他の変数がゼロのときの応答変数の増減分を意味する、すなわち他の全ての変数が平均（=0）であるときの、その変数そのものの効果（平均的効果）を意味することになる。  
    
変数を標準化する前のモデルでは、パラメータ$\beta_{1}$は「女性のときの`x`の傾き」であり、予測変数`x`そのものの効果を意味するものではなかった。  
これに対し、標準化した後のモデルでは`x_std`の傾きを「変数`x`の平均的効果」として捉えることができる。`x_std`の傾きが意味することは、「他の変数がゼロのとき、つまり平均であるときに、`x_std`が1単位（1標準偏差）変化したときの応答変数の変化量」を意味することになり、係数の値をそのまま用いて直感的に解釈することができる。  
交互作用項の係数も、一方の変数の効果が一定の場合（0のとき）、`M`または`x`に追加分で係る効果として理解することができる。  



## まとめ{#chap11_Summary}

この章では、予測変数がカテゴリカル変数の場合及び交互作用を含むモデルの場合を学んできた。

- 予測変数がカテゴリカル変数の場合は、0か1の値を取るダミー変数にして線形予測子に投入する。  
- 3つ以上のカテゴリの場合は、カテゴリの数-1個分のダミー変数を線形予測子に投入する。  
- 2つの予測変数の組み合わせの効果（交互作用）を見たい場合は、2つの予測変数の積を線形予測子に投入する。  
- 予測変数を複数加えたときの各予測変数の傾きは、他の予測変数がゼロのときのその予測変数が応答変数に及ぼす効果を意味する。  

線形予測子を拡張することで、正規分布を扱う様々な統計解析を線形モデルを扱う関数(`lm`)のみで行うことができることを学んできた。  
  
この章では、応答変数は正規分布に従うという前提をおいてきたが、応答変数が従う確率分布を正規分布以外にすることも可能である。以降の章では、応答変数が従う確率分布を変更して一般化した「一般化線形モデル」を扱っていく。

  
## 確認問題 {#chap11_Practice}
  
### 問1 {-}

Rで標準で入っているデータ`warpbreaks`を使って練習をする。  


```{r}

prac_dat_1 = warpbreaks #別の名前で保存する

head(prac_dat_1)
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks))
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks))

```


#### 1-1 {-}


変数`wool`について, 「`A`を1, それ以外を0」としたダミー変数を作成し、そのダミー変数を予測変数、`breaks`を応答変数として線形モデルを行い、切片及びダミー変数に係る傾きの推定値を報告せよ。  
  
また、ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。

```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy = ifelse(prac_dat_1$wool == "A", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ 1 + dummy))

```

#### 1-2 {-}

変数`tension`について, 「`L`を1, それ以外を0」、「`M`を1, それ以外を0」とした2種類のダミー変数を作成し、それら2つのダミー変数を予測変数、`breaks`を応答変数として線形モデルを行い、切片及び各ダミー変数に係る傾きの推定値を報告せよ。    
更に、そのときの切片及び各ダミー変数の係数が意味することを説明せよ。  


```{r eval=FALSE, include=FALSE}

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)

summary(lm(data = prac_dat_1, breaks ~ 1 + dummy_1 + dummy_2))

```


#### 1-3 {-}


1-2で作ったダミー変数に加え、更に「`H`を1, それ以外を0」としたダミー変数を追加で作成する。  
更に、`breaks`から全体の`breaks`の平均を引いた変数`breaks_2`を作成する。  
  
それら3つのダミー変数を予測変数、`breaks_2`を応答変数として線形モデルを行い、各ダミー変数に係る傾きの推定値を報告せよ。ただし、モデルには切片の項は加えないものとする。      
  
更に、そのときの各ダミー変数の係数が意味することを説明せよ。  


```{r eval=FALSE, include=FALSE}

prac_dat_1$breaks_2 = prac_dat_1$breaks - mean(prac_dat_1$breaks)

prac_dat_1$dummy_1 = ifelse(prac_dat_1$tension == "L", 1, 0)
prac_dat_1$dummy_2 = ifelse(prac_dat_1$tension == "M", 1, 0)
prac_dat_1$dummy_3 = ifelse(prac_dat_1$tension == "H", 1, 0)

summary(lm(data = prac_dat_1, breaks_2 ~ 1 + dummy_1 + dummy_2 + dummy_3 - 1))

```

  
### 問2 {-}

問1に引き続き、Rで標準で入っているデータ`warpbreaks`を使って練習をする。ただし、`tension`が`H`の部分を除いたデータを用いる。 


```{r}

prac_dat_2 = subset(warpbreaks, tension != "H") #tension == Hは除き、別の名前で保存する

head(prac_dat_2)

```

```{r}
ggplot2::ggplot() + 
  ggplot2::geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension))

```


breaksを応答変数、wool, tension, wool及びtensionの交互作用項を予測変数とした線形モデルを行い、切片、woolの傾き、tensionの傾き、交互作用項の推定値を報告せよ。  

```{r eval=FALSE, include=FALSE}

summary(lm(data = prac_dat_2, breaks ~ 1 + wool * tension))

```


## 補足(emmeans){#chap11_emmeans}

この章では、ダミー変数を複数加えたモデル及び交互作用を加えたモデルで一要因の分散分析や二要因の分散分析と同様のことができることを学んできた。ただし、分散分析では条件の効果が有意だった場合には、どの条件間で差があるかを多重比較補正をした上で比較するプロセスが必要となる。  
  
ここでは、`emmeans`パッケージを用いて`lm()`関数で多重比較補正を行う手順を紹介する（詳細は`emmeans`パッケージのヘルプを参照のこと）。

```{r, message = FALSE, warning = FALSE}
library(emmeans)
```

### 一要因の分散分析における多重比較{#chap11_emmeans1}

```{r, message = FALSE, warning = FALSE}
dat = PlantGrowth |> dplyr::mutate(weight_std = scale(weight))
result = lm(data = dat, weight_std ~ group) 

emm = emmeans::emmeans(result, ~ group) #モデルから推定された各カテゴリのmarginal mean(emmeans)を算出する
emm

emmeans::contrast(emm, method = "pairwise", adjust = "tukey")    #pairwiseで条件の組み合わせごとのemmeansの比較の結果が表示される。adjustで多重比較補正の方法を指定できる。

```


### 二要因の分散分析における単純主効果検定{#chap11_emmeans2}

```{r, message = FALSE, warning = FALSE}
dat = warpbreaks |> dplyr::mutate(breaks_std = scale(breaks))
result = lm(data = dat, breaks_std ~ wool * tension)

emm = emmeans::emmeans(result, ~ wool * tension)
emm

pairs(emm, simple = "wool")    #wool内でtension間の比較を行う
pairs(emm, simple = "tension")     #tension内でwool間の比較を行う

```
