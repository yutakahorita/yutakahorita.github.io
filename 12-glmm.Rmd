

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(lmerTest)
library(knitr)
```

# マルチレベルモデル

一般化線形モデルを拡張し，個人差や集団差を扱うモデルについて学ぶ。  

## 準備

tidyverseパッケージに加え，新たに`lme4`及び`lmerTest`というパッケージを使う。`lme4`と`lmerTest`は初めて使うので，インストールした上でロードしよう。


```{r, eval=FALSE}
library(tidyverse)

install.packages("lme4", "lmerTest")
library(lme4)
library(lmerTest)
```



## 個人差や集団差の問題

以下では，Rにデフォルトで入っている `iris` データを例として使う。
  

```{r}

head(iris) #irisデータの上数行を表示

```

まず，がくの長さ（`Sepal.Length`）とがくの幅（`Sepal.Width`）の関係を散布図で示してみよう。


```{r, echo=TRUE, message=FALSE, warning=FALSE}

graph_1 = ggplot() +
  geom_point(data=iris, aes(x=Sepal.Length, y=Sepal.Width),size = 3)
graph_1

```


まず，`lm()`を使って，がくの幅を応答変数，がくの長さを予測変数とした線形モデルで係数を推定する。


```{r}
iris_lm = lm(data = iris, Sepal.Width ~ 1 + Sepal.Length)
summary(iris_lm)
```

推定された切片及び傾きの値から予測直線を引くと，以下のようになる。  


```{r, echo=TRUE, message=FALSE, warning=FALSE}

graph_lm = ggplot()+
  geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width), size = 3)  +
  geom_smooth(data = iris, aes(x = Sepal.Length, y = Sepal.Width), formula = y~ 1 + x, method = "lm", se = FALSE)
graph_lm

```

がくの長さ（`Sepal.Length`）は，がくの幅に対してあまり影響を持っていない可能性にあることがうかがえる。  
  
  
では，この散布図を種（`Species`）ごとに色わけして示してみる。

```{r}

graph_2 = ggplot() +
  geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species, shape = Species), size = 3) 
graph_2

```


種を無視して検討したところ，がくの長さとがくの幅の間には関係がないようにみえたが，種ごとに分けてみると「がくの長さが大きくなるほど，がくの幅が大きくなる」関係にあるように見える。  
  
このあやめのデータのように，いくつかのデータが同じグループに属している構造の場合，グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には，**統計的独立性が保証されていない**ためである。つまり，同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。  
    
**独立(independence)**とは，各データが他のデータに影響されないという意味である。これまで学んできた確率分布では，**独立同分布(independent and identically distributed: i.i.d.)**が前提とされている。例えば，コインを数回投げて表が出る回数は二項分布に従うが，表が出るかどうかは前の試行に影響されることはない（前回表が出たら，次も表が出やすいということはありえないという前提を置く）。  
  
しかし，現実のデータでは，データ間の相関などにより，事象の独立性が保たれていないケースもありえる。その場合，統計的独立性を前提とした解析を行うと，上の例のように誤った結論を導いてしまう恐れがある。  
  

この例に限らず，**階層構造を持つデータや繰り返し測定データ**にも，同じことがいえる。例えば，学校ごとに学力テストを行った場合，同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合，その参加者のデータは似たような傾向になる可能性も考えられる。    

このようなデータに対して，**個人や集団の影響を考慮した**統計モデルとして，**マルチレベルモデル(multilevel model)**が提案されている。  
  
***

マルチレベルモデルは，「階層モデル(hierarchical model)」，「混合モデル(mixied model)」など，色々な呼ばれ方がされている。  


***


## マルチレベルモデルの概要  

マルチレベルモデルでは，予測変数が応答変数に及ぼす効果だけではなく，個人や集団の効果を扱う。予測変数そのものの効果は**固定効果（fixed effect）**と呼ばれ，個人や集団ごとの効果は**ランダム効果（random effect）**と呼ばれて区別される。前章まで扱ってきた，一般化線形モデルは固定効果のみを含むモデルである。  
  
例として， **繰り返し測定されたデータ**を扱う。以下のプログラムを実行して，サンプルデータ`example`を作ろう。

```{r}

set.seed(1)
example = data.frame(i = 1:6, j = c(1, 1, 2, 2, 3, 3), y = round(rnorm(6), 2), x = rep(c(0, 1),3) )

example

```

$i$がデータを意味する番号（何行目か），$j$を個人もしくはグループを意味する番号とする。例えば，個人$j$が$x=0$の場合と$x=1$の場合の2回$y$を測定している，あるいは同じ集団$j$から2人が選ばれてそれぞれの人について$y$が測定された，といったケースが当てはまる。  
  
一般化線形モデルの線形予測子は，以下のような数式で表現できた。  

$$
\hat{y_{i}} = \alpha + \beta x_{i} \tag{1}\\
y_{i} \sim Normal(\hat{y_{i}}, \sigma)
$$

$\alpha$が切片，$\beta$が予測変数$x$に係る傾きであった。  
  
これに対し，マルチレベルモデルでは，以下のように線形予測子に$\alpha_{j}$が加わる。

$$
y_{i} = \alpha_{0} + \beta x_{i} + \alpha_{j} \tag{2} \\
\alpha_{j} \sim Normal(0, \sigma_{\alpha})\\
y_{i} \sim Normal(\hat{y_{i}}, \sigma)
$$

すべての個人に共通して影響する切片$\alpha_{0}$に加え，個人ないしはグループごとに異なる切片$\alpha_{j}$を考慮する。  
更に，$\alpha_{j} \sim Normal(0, \sigma_{\alpha})$にあるように，個人ごとの切片$\alpha_{j}$が「平均をゼロ，$\sigma_{\alpha}$を標準偏差とする正規分布から生成される」という仮定を置く。  
これにより，同じグループに属するデータ（例えば$j=1$）には同じ効果（$\alpha_{1}$）が共通して係ることを表現できる。  
  

***  

傾きを$\beta_{ j}$にする，すなわち個人ごとに予測変数に係る効果が異なるという前提を置くこともできる。しかし，実際に傾きをランダム効果としたモデルは複雑で推定するのは困難であるため（最尤推定法では解が求まらない場合がある），多くの場合，個人差の影響（ランダム効果）は切片のみを考慮したモデルで表現されることが多い。ランダム傾きを含むマルチレベルモデルを扱う際には，ベイズ統計の手法が必要になる。

***

## Rでのマルチレベルモデル

Rでマルチレベルモデルで解析を行うためには，外部パッケージが必要になる。様々なパッケージがあるが，`lme4`パッケージが扱いやすい。以下では，`lme4`パッケージに含まれる`glmer()`を使った解析の例を示す。  
  
基本的に，`lm()`関数と似た表記で使うことができる。ランダム切片は，`(1|グループを意味する変数名)`のかたちで線形予測子に入れる。

```{r, message=FALSE, warning=FALSE}

model_lmm = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) #(1|Species)を加える
summary(model_lmm)

```


出力結果を見てみると，`Fixed effects`という部分がある。ここに，固定効果の推定結果が表示される。見方は一般化線形モデルのときと同じである。切片(intercept)と予測変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず，すべての個体共通に係る予測変数の効果）。 
    
がくの幅（`Sepal.Width`)の回帰係数（Estimate）を見ると，`lm()`での推定結果とは逆に，プラスになっている。やはり，グループの違いを統制すると，実際にはがくの幅が大きくなるほど，がくの長さも大きくなる関係にあることが，`lmer()`による推定結果からわかる。
 

*** 

`lmer()`では，デフォルトで係数のp値は表示されない。p値も出したいならば，`lmerTest()`パッケージをインストールしておく必要がある。

*** 


## 正規分布以外を扱う例

### ロジスティック回帰

応答変数が正規分布以外に従う場合のマルチレベルモデルについても見ていこう。  
`lme4`パッケージの`glmer()`で，正規分布以外の確率分布を指定したマルチレベルモデルの解析を行うことができる。以下では，ランダム効果を加えたロジスティック回帰分析の例を示す。  
  
  
まず，以下のプログラムを実行してサンプルデータ`data_sample`を作ろう。


```{r, echo=TRUE}
x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0)
y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1)
ID = c(rep("a",5),rep("b",5),rep("c",5))
data_sample = data.frame(ID, x1, y1)
str(data_sample)
```

`x1`を予測変数（量的変数），`y1`を応答変数（0か1のいずれかを取る），`ID`が個体を示す変数とする。1つの個体から`x1`を変えて5回，`y1`が計測がされた実験をイメージしてほしい。  

予測変数と応答変数の関係に，個体特有の効果を加えたモデルは以下となる。  

$$
q = \frac{\exp(\alpha_{0} + \beta x + \alpha_{j})}{1+\exp(\alpha_{0} + \beta x + \alpha_{j})} \tag{3} \\
\alpha_{j} \sim Normal(0, \sigma_{\alpha})\\
y \sim Bernoulli(q)
$$

線形予測子をロジット（逆ロジット）変換して，$y=1$が生じる確率$q$を求める。応答変数$y$は，$q$をパラメータとするベルヌーイ分布から生成される。これらの点は，一般化線形モデルで学んだ。  
更に，線形予測子に，ランダム切片$\alpha_{j}$を加えた。$\alpha_{j}$は，平均ゼロ，標準偏差$\sigma_{\alpha}$の正規分布に従って生成されるとする。  
  

正規分布以外の確率分布を扱うマルチレベルは，Rでは`lme4`パッケージの`glmer()`で扱うことができる。さっきの`lmer()`と同じ要領で，線形予測子に個体を識別する変数（`ID`）を加える。以下のように，`(1|ID)`というかたちで入れる。  
  
あとは，確率分布とリンク関数を指定する。指定の仕方は，`glm()`のときと同じ要領である。確率分布は`binomial`（二項分布），リンク関数は`logit`（ロジット関数）を指定する。リンク関数の指定は省略しても構わない（二項分布を指定すれば，デフォルトでロジット関数を選択してくれる）。


```{r, echo=TRUE}

model_logistic_glmm = glmer(data = data_sample, y1 ~ 1 + x1 + (1|ID), family = binomial(link="logit"))
summary(model_logistic_glmm)

```


### ポアソン回帰


同じく，`lme4`パッケージの`glmer()`を使う。確率分布は`poisson`（ポアソン分布），リンク関数は`log`（対数）を指定する。リンク関数の指定は省略しても構わない（ポアソン分布を指定すれば，デフォルトで対数関数を選択してくれる）。


```{r}

set.seed(1)
alpha = 0.5
beta = 0.2
x = rnorm(n=50, mean = 0, sd = 1)
alpha_0 = rnorm(n=5, mean = 0, sd = 0.2)


lambda_1 = exp(alpha + beta * x[1:10] + alpha_0[1])
lambda_2 = exp(alpha + beta * x[11:20] + alpha_0[2])
lambda_3 = exp(alpha + beta * x[21:30] + alpha_0[3])
lambda_4 = exp(alpha + beta * x[31:40] + alpha_0[4])
lambda_5 = exp(alpha + beta * x[41:50] + alpha_0[5])
y_1 = rpois(n = 10, lambda_1)
y_2 = rpois(n = 10, lambda_2)
y_3 = rpois(n = 10, lambda_3)
y_4 = rpois(n = 10, lambda_4)
y_5 = rpois(n = 10, lambda_5)

dat = data.frame(y = c(y_1, y_2, y_3, y_4, y_5), 
           x = x + 20,
           ID = sort(rep(1:5, 10)))
dat$x_std = dat$x - mean(dat$x)

ggplot() + 
  geom_point(data = dat, aes(x = x, y = y)) +
  facet_wrap(vars(factor(ID)))

```



```{r}
result = glmer(data = dat, y ~ 1 + x_std + (1|ID), family = poisson(link="log"))
summary(result)
```


### マルチレベルモデルによる過分散への対処

ポアソン分布の理論的な分散よりも実際のデータの分散が大きい「過分散」がある場合には，ポアソン回帰の結果が信頼できなくなる問題があった。

```{r}

#サンプルデータの作成
set.seed(1)
N= 50
x = rnorm(n=N, mean = 2, sd = 1)
e = rnorm(n=N, mean = 0, sd = 1)
lambda = exp(0.01 + 0.1*x + e)
y = rpois(n=N, lambda = lambda)
dat_dis = data.frame(y=y, x=x)

result_dis = glm(data = dat_dis, y ~ 1 + x, family = poisson(link = "log"))
summary(result_dis)

library(performance)
performance::check_overdispersion(result_dis) #過分散のチェック

```


マルチレベルモデルでは，過分散の問題にも対処することができる。

$$
\lambda_{i} = \alpha_{0} + \beta x_{i} + \alpha_{i} \tag{2} \\
\alpha_{i} \sim Normal(0, \sigma_{\alpha})\\
y_{i} \sim Poisson(\lambda_{i})
$$
観測値ごとのランダム切片（$\alpha_{i}$）を加えて，余分な分散を別のパラメータ（$\sigma_{\alpha}$）で表現する。

```{r}

dat_dis$ID = 1:nrow(dat_dis) #観測値ごとに番号を割り振る（1からデータ数までの数値の連続）
head(dat_dis)

result_poisson_glmm = glmer(data = dat_dis, y ~ 1 + x + (1|ID), family = poisson(link = "log"))
summary(result_poisson_glmm)

```

ランダム切片を入れない通常のポアソン回帰では予測変数`x`は`y`に対して有意な効果を有していたが，観測値ごとのランダム切片を加えたマルチレベルモデルでは，`x`の係数が小さくなり有意な効果も見られなくなった。通常のポアソン回帰では過分散の影響で`x`の効果を過剰に評価していたことが伺える。余計な分散をランダム切片に吸収させることで，予測変数が持つ効果を推定することができた。

## 確認問題{-}

### 問１{-}

`car`パッケージに入っているカナダにおける職業の威信度に関する調査データ`Prestige`を使う。102業種に関する調査結果が入っている。

```{r, echo=TRUE, message=FALSE, warning=FALSE}

library(car)
head(Prestige)

```



`prestige`を応答変数，`education`, `income`及び `women`を予測変数，`type`をランダム効果（切片）としたマルチレベルモデルで解析せよ。応答変数が従う確率分布は，正規分布を用いるものとする。  
  
各予測変数が応答変数に及ぼす効果について述べよ（その予測変数が1単位変化すると，応答変数がどう変化するか）。 
    
なお，変数の意味は以下の通りである。  
`prestige`：職業威信度（値が高いほど威信度が高い）  
`education`：在職者の平均教育年数  
`income`：平均所得（単位はドル）  
`women`：女性の割合  
`type`：職業のカテゴリ（bc=ブルーカラー，wc=ホワイトカラー，prof=専門職）  
  
    
ヒント：正規分布を扱うマルチレベルの場合は，`lme4`パッケージの`lmer()`を使えば良い。なお，出力時にメッセージが出ても無視して良い（中心化せよという命令だが，無視して良い）。

```{r, eval=FALSE, include=FALSE}

result = lmer(data = Prestige, prestige ~ 1 + education + income + women + (1|type))
summary(result)

```


