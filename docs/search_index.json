[
["index.html", "心理学データ応用解析法 Chapter 1 はじめに 1.1 このテキストについて 1.2 テキストの構成", " 心理学データ応用解析法 帝京大学文学部心理学科 堀田結孝 2020-10-22 Chapter 1 はじめに 1.1 このテキストについて このテキストでは，Rを使いながら心理学における応用的なデータ解析について学んでいく。 心理学を専攻していて基礎統計学を学修済みの学生を対象としている。Rを全く使ったことがない人を対象として，Rの基本的な使い方から解説する。 このテキストでは，t検定や分散分析などの心理統計で学んだ手法を線形モデルという一つの枠組みで包括的に理解することを目的とする。具体的には線形モデル，一般化線形モデル，マルチレベルモデルを順に学んでいき，様々な種類のデータに対して柔軟に解析を行う技術を身に着けていく。 1.2 テキストの構成 このテキストでは，R及びRStudioを用いて解析の練習をしていく。 第2章から第5章にかけては，Rの使い方について学んでいく。 第2章ではRのインストール，Rでのプログラムの書き方，パッケージのインストールの仕方など，Rを使ってデータ解析をする上で基本的なことについて，Rを使うのが初めての人向けに解説している。 第3章では，平均値や標準偏差，相関など，心理学における基礎統計学で学んだ内容を扱う。Rでこれらの基礎統計量を計算する方法について解説する。 第4章と第5章では，Rでデータを加工する方法やグラフの作成などについて解説する。 第6章以降で，データ解析について学んでいく。 まずは，心理統計学でも学んだ内容の復習も交え，統計モデルを用いる上で必要な知識を順番に学んでいく。第6章では確率分布について学ぶ。第7章では統計的仮説検定とそれが抱える問題を学ぶ。第8章では，t検定，χ二乗検定など，心理統計学で学んだ解析をRで行う方法について解説する。 第9章からは，統計モデルについて学んでいく。第9章では線形モデルを学ぶ。この章は心理統計学における回帰分析の復習を通して，これまで心理統計学で学んできたt検定，分散分析などの解析法が線形モデルという一つの枠組みで扱えることを理解する。第10章では，多重共線性や過学習の問題など，線形モデルで解析を行う上での注意点を挙げる。 第11章では，一般化線形モデルを学ぶ。質的変数など，正規分布に従わないデータを解析する手法を学んでいく。線形モデルを応用することで，様々な種類のデータに対して柔軟な解析が可能であることを理解する。 第12章ではマルチレベルモデルを学んでいく。同じ集団や個人から得られたデータなど，個人差や集団差を含むデータを解析する手法について学ぶ。 第13章と第14章にかけてはベイズ統計など，線形モデルを応用したより詳細な解析について解説する。 1.2.1 テキストの読み方 各章の最後に，確認問題を設ける。 "],
["01-intro.html", "Chapter 2 Rの使い方 2.1 Rのインストール 2.2 RStudioのインストール 2.3 プログラムの書き方 2.4 単純な演算 2.5 変数 2.6 データ構造 2.7 欠損値 2.8 パッケージのインストールとロード 2.9 データの読み込み 2.10 Rを終わらせる 確認問題", " Chapter 2 Rの使い方 Rのインストールから使い方までを解説する。 R及びRStudioのインストール （このテキストの手順でうまくいかない場合は，「R インストール」などで検索してみよう） プログラムの書き方 変数 データ構造 欠損値の扱い パッケージ データの読み込み 2.1 Rのインストール インストールは，https://cran.r-project.orgから可能。自分のOSにあったインストーラを選ぶ。 インストーラを実行したら，あとは指示に従ってインストールをすすめる。 2.2 RStudioのインストール RStudioとは，Rの使いやすさを向上させる目的で開発されているアプリケーションである。Rをインストールしたら，RStudioもインストールしておくこともすすめる。このテキストでも，RStudioを使って解析することを前提に説明する。 インストールは，https://rstudio.com/products/rstudio/#rstudio-desktopからできる。 「DOWNLOAD RSTUDIO DESKTOP」を選択（無料版で良い）。 RとRstudioの両方をインストールできたら，RStudioの方を開く。 以下のような画面が表示される。 注意: ここまでの手順で「そもそも自分のマシンにR及びRStudioをインストールできない」あるいは「R及びRStudioはインストールできたが，起動できない」という人は，以下の可能性を考えてみてほしい。 OSがWindowsの場合，管理者権限のあるアカウントでR及びRStudioをインストールする必要がある。インストールする際は，管理者権限として実行しよう。 同じくOSがWindowsの場合，アカウント名にマルチバイト文字（全角文字）を含んでいるとRが正常に起動しない。つまり，マシンにログインする時の名前を「ほげ」など全角文字（日本語）にしてしまっていると，うまくいかない。この場合は面倒ではあるが，「既にあるアカウントの名前を半角英数に変更する（[hoge]など）」，あるいは「もう一つ別の半角のアカウントを作る」といった方法で対処してみよう。 Rに限らず，ファイルやフォルダ名に全角文字が含まれていると障害になる場合がある。ファイル名やフォルダ名には，なるべく日本語（全角文字）は使わない習慣を身に着けよう。 2.3 プログラムの書き方 Rの画面構成について確認する。 2.3.1 コンソール（Console） コンソール（Console）という部分にプログラムを入力すると，結果が出力される。 ためしに，コンソールの&gt;の部分に，以下のプログラムを入力して，Enterを押してみよう。 このテキストでは以下のように，背景が灰色の箇所にプログラムとその出力結果（行頭に##が付いている部分）が示されている。 1 + 1 ## [1] 2 同じコンソールに，答えである2が出力されたはずである。 このように，コンソールに直接プログラムを入力すると，結果を返してくれる。 2.3.2 スクリプト コンソールに入力したプログラムや出力結果は，Rを閉じると消えてしまう。これでは復習できないので，プログラムは別のファイルに残しておいた方が良い。 プログラムを書き込んだ別ファイルのことを「スクリプト」と呼ぶ。プログラムはなるべく，スクリプトファイルに残しておく習慣をつけよう。 「File」から「New Script」を選ぶ。何も書かれていないファイル（R Editor）が開かれる。 名前をつけて保存する。「File」から「Save as..」を選び，名前をつけて保存する。拡張子が「.R」のファイルとして保存される。 スクリプトに，試しに以下のプログラムを入力してみよう。 1 + 1 プログラムを選択し，Ctrl+Rで実行する（「Run line or selection」を選んでも可）。すると，「R Console」にプログラムの結果が出力される。 スクリプトファイルを開きたいときは，RStudioを立ち上げて，「File」から「Open File」を選び，スクリプトのファイルを選ぶ。 初心者が戸惑いやすい点について説明する。 ためしに，コンソールに以下のプログラムを入力してEnterを押してみよう。 1 + 何も表示されないし，冒頭が&gt;ではなく+が表示される。Enterを押してももとに戻らない。 これはプログラムが不完全であるためである。1 +と中途半端な状態で入力したので，Rはプログラムの続きがあるものと思って入力を待っている状態なのである。プログラムの続きを入力すれば，結果が出力される。例えばこの例ならば，1を入力してEnterを押せば，答えである2が出力される。 他にもカッコの閉じ忘れなどでも，同じようなことが生じる。 なお，Esc（エスケープ）キーを押せば，プログラムを中止することができる。困ったときには，Escキーを押そう。 他にも，エラーが生じた場合は，エラーメッセージを読んで，プログラムの書き方に間違いがないかを確認しよう。たいてい，「変数の入力間違い」など大したことのないミスが原因である。ちょっとプログラムを間違えたくらいでRが壊れるということは決してないので，冷静に対処しよう。 2.4 単純な演算 四則演算（足し算・引き算・割り算・掛け算）をしてみよう。それぞれ，+，-，*，/を使う。 1 + 1 1 - 1 2 * 3 10 / 2 また，カッコ()内の演算が優先される。 (1 + 3)/2 累乗は^で計算できる。 2^3 2.4.1 コメント文 行頭に#を挿入すると，#から改行まではコメント文として理解され，プログラムが実行されない。スクリプトにメモを残しておきたいときに便利なので覚えておこう。 #1 + 1 2.5 変数 数値を変数に代入して扱うことができる。 x = 5 + 8 x ## [1] 13 y = x - 2 y ## [1] 11 なお，=の代わりに&lt;-を使っても良い。 x &lt;- 5 + 8 2.5.1 変数の使い方の注意 Rは小文字と大文字を区別する。たとえば，x（小文字）と入力して実行すると結果が出力されるが，X（大文字）では出力されない。 x = 2 #小文字のxに2を代入する。 x - 2 #ゼロが出力されるはず。 X - 2 #大文字のXでは答えが表示されない。大文字のXの変数は作られていないので。 また，数値を全角で入力していないかにも注意すること。全角文字は数字ではなく，文字として認識する。数字は常に半角で入力すること。 x = 2 #半角の2 x = ２ #全角の２ 2.5.2 変数の型 Rでは変数の種類として，数値型，文字列，日付，論理型の区別をする。 数値型(numeric) 数値型として格納した変数は，数値として扱うことができる。数値型の変数同士で，演算（足し算・引き算・掛け算・割り算）を行うことができる。 x_num = 5 class(x_num) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; y_num = 1.2 class(y_num) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; x_num + y_num #数値型同士は演算することができる ## [1] 6.2 文字列(character) 文字として扱われる。文字列同士は演算をすることができない。 文字を変数として代入したい場合は，文字をクオテーションマーク(&quot;&quot;)で囲む。 x_char = &quot;hello&quot; class(x_char) #class()でその変数の型を確認することができる ## [1] &quot;character&quot; y_char = &quot;1&quot; class(y_char) #数値でもクオテーションで囲めば文字列として扱われる。 ## [1] &quot;character&quot; 日付(date) Dateは日付のみを保存し，POSIXctは日付と時間を保存する。 日付型同士で日数や秒数などの演算をすることができる。 x_date = as.Date(&quot;2020-06-15&quot;) class(x_date) #class()でその変数の型を確認することができる ## [1] &quot;Date&quot; x_date_1 = as.POSIXct(&quot;2020-06-14 12:00&quot;) class(x_date_1) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; x_date_2 = as.POSIXct(&quot;2020-06-15 12:00&quot;) x_date_2- x_date_1 ## Time difference of 1 days 論理型(logical) TRUEかFALSEの2つの値のどちらかを取る変数の型である。 a = TRUE class(a) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; b = FALSE class(b) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; 論理式 数値の大小関係などを扱うときに用いる。 2 == 1 #2 と 1 は同じか？ ## [1] FALSE 2 != 1 #2 と 1 は同じではないか？ ## [1] TRUE 2 &lt; 1 #2 は 1 よりも小さいか？ ## [1] FALSE 2 &lt;= 1 #2 は 1 以下か？ ## [1] FALSE 2 &gt; 1 #2 は 1 より大きいか？ ## [1] TRUE 2 &gt;= 1 #2 は 1 以上か？ ## [1] TRUE 2.6 データ構造 複数の数値などをまとめたものをデータと呼ぶ。Rには，いくつかのデータ構造が用意されている。 2.6.1 ベクトル 同じ型の要素を集めたもの。Rでは，c()関数でベクトルを作成することができる。 x = c(1, 2, 3, 4, 5) y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) ベクトル[x]の表記でカッコの中に数値を入れると，そのベクトルのx番目の要素を取り出せる。 x[2] ## [1] 2 y[3] ## [1] &quot;c&quot; ベクトルが数値の場合は，演算をすることもできる。 x * 2 #ベクトル内の全ての要素に2を掛ける ## [1] 2 4 6 8 10 x_2 = c(6, 7, 8, 9, 10) x + x_2 #（ベクトルに格納されている変数の数が同じならば，ベクトル同士で演算ができる） ## [1] 7 9 11 13 15 2.6.2 データフレーム 複数のベクトルを行列でまとめたデータ構造を，Rではデータフレームと呼ぶ。データフレームは頻繁に使うので，構造を覚えよう。 まず，2つのベクトルを作成する。 x_vec = c(1, 2, 3, 4, 5) y_vec = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) 次に，以下のプログラムを実行して，データフレームを作る。data.frame()は，データフレームを作るための関数である。 以下では，ベクトルx_vecとy_vecをそれぞれ，xとyという名前にしてdatという名前の行列データを作成している。 dat = data.frame(x=x_vec, y=y_vec) dat ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e 以下のように，データフレーム$変数名で，データフレームの変数をベクトルとして取り出すができる。 dat$x ## [1] 1 2 3 4 5 データフレームに新たに変数を加えることも出来る。 dat$x_2 = c(6, 7, 8, 9, 10) dat ## x y x_2 ## 1 1 a 6 ## 2 2 b 7 ## 3 3 c 8 ## 4 4 d 9 ## 5 5 e 10 dat$x_3 = dat$x + dat$x_2 dat ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 データフレーム$変数名でデータ内の変数にアクセスする方法は，今後もよく使うので覚えておこう。 2.7 欠損値 心理学ならば実験の失敗や質問への無回答など，データが取得できなかいケースも生じ得る。そのような場合には，データの一部を欠損値として扱う。 Rでは，欠損値はNAで扱う。 先程の例で作ったデータフレームdatに，欠損値を含むベクトルx_4を入れてみよう。 dat$x_4 &lt;- c(1, 2, NA, 4, 5) dat ## x y x_2 x_3 x_4 ## 1 1 a 6 7 1 ## 2 2 b 7 9 2 ## 3 3 c 8 11 NA ## 4 4 d 9 13 4 ## 5 5 e 10 15 5 dat$x + dat$x_4 ## [1] 2 4 NA 8 10 欠損値を含む変数は，計算に用いることができない。例えば，Rには平均値を計算するためのmean()という関数がある。しかし，欠損値を含むベクトルの場合は計算がなされない。 mean(dat$x_4) ## [1] NA 関数によっては，欠損値を含むデータを使うときには欠損値の処理をする必要がある。例えば，mean()ならば，オプションとしてna.rm =TRUEを入れると欠損値を除いた上で平均値を計算してくれる。 mean(dat$x_4, na.rm = TRUE) ## [1] 3 2.8 パッケージのインストールとロード パッケージとは，Rの機能を拡張するためにインターネットからインストールして使うものである。 Rに標準で入っている機能に加え，追加でtidyverseパッケージをインストールして使おう。 2.8.1 パッケージのインストール パッケージをインストールする。install.packages()で，インストールしたいパッケージを入力する。 ここではtidyverseというパッケージをインストールするのを例として，パッケージのインストール方法について示す。 install.packages(&quot;tidyverse&quot;) もし「Please select a CRAN mirror …」というのが表示されたら，Japan (Tokyo)を選んで「OK」を押す。 2.8.2 パッケージのロード インストールしただけではパッケージは使えない。使う前にロードする必要がある。library()で，括弧内に使いたいパッケージ名を入力する。 library(&quot;tidyverse&quot;) 一度インストールしておけば，今後は最初にlibrary()でロードするだけで使うことができる。毎回インストールする必要はない。 マシンにインストールされているパッケージの情報は，RStudioの右下の「Packages」というタブから確認することができる。 パッケージは世界中で開発され，アップデータもなされている。RStudioならば同じく「Packages」の「Update」を選ぶことでアップデートすることができる。 2.9 データの読み込み 大抵，データはExcelファイルなどに入力して使う場合が多い。RでExcelなどの外部ファイルを読み込んで解析するには，以下の手順を踏む必要がある。 ワーキングディレクトリを指定する。 データを読み込む 読み込みたいデータをデスクトップに置いた場合を例として，外部データの読み込み方について確認していこう。 2.9.1 ワーキングディレクトリを指定する。 ワーキングディレクトリとは，「現在居る場所」のことである。 試しに，現在のワーキングディレクトリを確認しよう。以下のプログラムをコンソールに入力して実行する。 getwd() 出力された場所が，現在のワーキングディレクトリである。Rは読み込むファイルをワーキングディレクトリを起点として探す。読み込むファイルは，ワーキングディレクトリに置くことにしよう。 ワーキングディレクトリの指定 以下に，デスクトップをワーキングディレクトリに指定する方法について説明する。 方法１ コンソールに以下のプログラムを直接書き込んで実行する。 #Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 #Macの場合 setwd(&quot;~/Desktop&quot;) #正しく設定されたかを確認する getwd() 方法２ RStudioならば，右の方にある「File」からデスクトップを表示し，「Set As Working Directory」を選ぶ。 方法３ RStudioならば，「Session」から「Set Working Directory」，「Choose Working Directory」を選び，デスクトップを選ぶ。 2.9.2 データを読み込む csvファイルの場合 read.csv()関数で読み込むことが出来る。 dat = read.csv(&quot;data.csv&quot;) #ファイル名をクオテーションで囲んで入れる。ここでは読み込んだデータを「dat」という名前で保存した。 dat #データの中身がコンソールに出力される Excelファイルの場合 tidyverseパッケージをロードした上で，read_excel()を使う。 dat = readxl::read_excel(&quot;data.xlsx&quot;) dat 相対パス ワーキングディレクトリを起点として指定されるファイルの場所のことを相対パスという。 上記の例は，デスクトップ上に読み込みたいファイルを保存し，デスクトップをワーキングディレクトリとして指定して読み込むという手順であった。しかし，例えばデスクトップにあるフォルダの中にデータを保存してあってそのファイルを読み込みたい場合，いちいちワーキングディレクトリを設定し直すのが面倒くさくなる。 このような場合，相対パスでファイルを指定するのが便利である。 「sample_data」のフォルダをダウンロードしてデスクトップに保存し，フォルダの中にある「0_sample.csv」を読み込んでみよう。 #デスクトップをワーキングディレクトリに指定する ##Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 ##Macの場合 setwd(&quot;~/Desktop&quot;) #デスクトップにあるsample_dataフォルダの中の「0_sample.csv」を読み込む dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) .（ピリオド）は，ワーキングディレクトリを表現している。/（スラッシュ）でフォルダの階層を区切り，ファイルを指定する。 2.9.3 サンプルデータ Rには予めサンプルデータがいくつか用意されている。このテキストでもところどころで，Rに入っているサンプルデータを使って解析の練習を行う。 iris #有名なフィッシャーのあやめデータ cars #自動車の速度と停止距離との関係 data() #data()で，入っているデータを確認できる 2.10 Rを終わらせる そのまま閉じてよい。 「Save workspace image?（作業スペースを保存しますか？）」が表示されるが，これは「いいえ」で良い。 確認問題 Rでのプログラムの書き方，データフレームの使い方，外部データの読み込み方について復習しよう。 問１ Rを使って以下のa, bを計算し、aとbの式どちらの方が答えが大きいかを確認せよ。 a: 1 × 1 × 2 × 2 × 3 × 4 × 5 × 9 × 8 × 7 × 6 b: 9 × 8 × 7 × 6 × 1 × 7 × 5 × 1 × 3 × 2 × 1 問２ ある細菌が，1分後に2個に分裂して増殖するとする。つまり，1個の細菌が1分後には2個に，2分後にはその2個が分裂して合計4個に，3分後にはその4個が分裂して合計8個になる。 1個の細菌は，30分後には何個になっているだろうか？ ヒント：aのx乗は^を使って求める（a^x） 問３ サンプルデータ「1_sample.csv」をデータフレームとして読み込もう。更に，データフレーム上の変数X, Yを用いて，Z = X - 3Y の新しい変数Zをデータフレームに追加しよう。 "],
["02-summary.html", "Chapter 3 統計学の基礎の復習 3.1 尺度水準 3.2 基本統計量 3.3 相関(correlation) 確認問題", " Chapter 3 統計学の基礎の復習 平均値の計算など，心理統計学の基礎について復習する。また，Rで平均値などの基礎統計量を計算する方法についても解説する。 変数の区別 基本統計 代表値（平均，中央値） 散布度（分散，標準偏差，分位数） 共分散，相関 3.1 尺度水準 まず，統計学における変数やデータの種類の区別について確認する。データの種類は大きく分けて，数値である量的変数とカテゴリーを意味するカテゴリカル変数（質的変数）の2つに区別できる。 3.1.1 量的変数 数値として扱う変数。計算することができる。量的変数は更に，間隔尺度と比率尺度で区別ができる。 間隔尺度 データの間隔に意味があるもの。ゼロが何もない状態を意味するものでないもの。例えば，セ氏温度など（0℃以下も-1℃があるように，ゼロは何もない状態を意味しない）。差には意味があるが，比率については意味を持たない。例えば，「10℃と20℃の差は10℃である」とはいえるが，「20℃は10℃の2倍の熱さである」とは言えない。 比率尺度 データの間隔に意味があるもの。ゼロがなにもない状態を意味するもの。例えば，身長，体重，絶対温度など。間隔を比率で表現できる。例えば，「体重100キロの人は体重50キロの人より2倍重い」といえる。 また，量的変数は離散値か連続値かでも区別する。 離散値 離散値とは，小数の間隔を持たない数値のこと。例えば個数。1個, 2個，3個と数えるが，1.1個, 1.2個などは存在しない。 連続値 連続値とは，小数の間隔を持つ数値のこと。例えば身長。150cmから151cmの間には小数で表現できる数値が連続的に並んでいる。 3.1.2 カテゴリカル変数（質的変数） 分類や種類などを意味するデータ。数量化して計算することはできない。 カテゴリカル変数も更に，いくつかに分類される。ここでは，名義尺度と順序尺度の区別をする。 名義尺度 性別（男，女），血液型，出身地など。順序関係がないのが特徴である（男性&lt;女性といった関係はない）。 順序尺度 「優，良，可，不可」といった成績，「1. 賛成，2. どちらでもない，3. 反対」といった尺度など。心理学ではよくリッカート尺度（＊）で態度などを測定するが，これも順序尺度である。 ＊「1:そう思わない，2: 思わない，3: どちらでもない, 4: そう思う，5: 強くそう思う」といったように，自分の態度に当てはまる数字を選ばせて態度の強さを測定する方法。 順序尺度は順序関係があるが，間隔は定義されない。例えば，成績には優 &gt; 良 &gt; 可という順序関係があるが，優と良の間と良と可の間の幅は等しいといった定義はできない。また，良+可=優といった計算もできない。つまり，順序尺度は順序関係はあるがカテゴリー変数なのである。 3.2 基本統計量 以下では，Rにもともと入っているirisデータをサンプルデータとして使いながら，平均値や中央値などの代表値や分散や標準偏差などの散布度について復習する。 irisと入力して実行すると，データの中身を確認できるが，これだとデータ全てが出力されるて見やすいとはいえない。head()を使うとデータの変数を含めた上部数行を表示してくれる。また，見るときに，str()を使うと，簡略化したデータの構成を表示してくれる。これらの関数は今後もよく出てくるので覚えておこう。 head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 3.2.1 代表値 データの代表となる値のこと。平均や中央値などがよく用いられる。 平均値(mean) \\(n\\)個の数値\\(X_{1}, X_{2}, \\cdots, X_{n}\\)からなるデータの平均値\\(\\bar{X}\\)は，以下の数式で求める。 \\[ \\bar{X} = \\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\\\ \\] Rで平均値を求めるには，mean()関数が使える。カッコの中に，平均値を求めたいデータをベクトルの形式で入れる。 データフレームの場合は，「データ名$変数名」でベクトルを指定する。これはよく使う表記なので覚えておこう。 mean(iris$Sepal.Length) ## [1] 5.843333 データに欠損値が含まれている場合は，計算ができない。オプションとしてrm.na = TRUEを指定すれば，欠損値を除いて平均値を算出してくれる。 a = c(1, 2, 3, NA, 4, 5) mean(a) ## [1] NA mean(a, na.rm = TRUE) ## [1] 3 中央値(median) データを小さい順から並べた場合，つまり，\\(X_{1} \\le X_{2} \\le ... \\le X_{n}\\)と並べた場合に，中央に位置する値を中央値という。 データの個数を\\(n\\)とした場合，\\(n\\)が奇数の場合は\\(X_{(n+1)/2}\\)，\\(n\\)が偶数の場合は\\((X_{n/2}+X_{n/2+1})/2\\)が中央値となる。 Rで中央値を求めたい場合は，median()関数が使える。mean()と同じく，カッコ内にデータをベクトルの形式で入れる。 median(iris$Sepal.Length) ## [1] 5.8 3.2.2 散布度 データの散らばり具合を示す値のこと。分散，標準偏差などが知られる。 分散(variance) 分散（\\(\\sigma^2\\)）は，以下の式で定義される。すなわち，各変数が平均値から離れている程度を表現したものである。 \\[ \\sigma^2 = \\frac{1}{n-1}\\sum_{k=1}^{n}(X_{k}-\\bar{X})^2\\\\ \\] Rで分散を求める際には，var()関数を使う。 var(iris$Sepal.Length) ## [1] 0.6856935 標準偏差(standard deviation) 分散の平方根が標準偏差（\\(\\sigma\\)）である。 Rで標準偏差を求める際には，sd()関数を使う。 sd(iris$Sepal.Length) ## [1] 0.8280661 分位数(quantile) データを小さい順から大きい順に並べ替えたときに，データを分割する値を分位数という。一般的に，四分位数（下位25%，50%[中央値]，上位75%で分割した値）が報告によく使われる。 Rでは，quantile()で分位数を求められる。 quantile(iris$Sepal.Length) #デフォルトだと，0%, 25%, 50%, 75%, 100%が表示される） ## 0% 25% 50% 75% 100% ## 4.3 5.1 5.8 6.4 7.9 quantile(iris$Sepal.Length, probs = c(0.1, 0.3, 0.5, 0.8, 1.0)) #オプションのprobsで，分割する点を任意で指定することができる。 ## 10% 30% 50% 80% 100% ## 4.80 5.27 5.80 6.52 7.90 3.2.3 要約統計量 以上のように，Rには代表値や散布度を求めるための関数が標準で入っているが，これらをまとめて計算してくれるsummary()もある。 summary()に変数を入れると，最小値，最大値，平均，四分位数をまとめて算出してくれる。 summary(iris$Sepal.Length) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 3.2.4 表の作り方 カテゴリカル変数の場合は，データ全体を把握する上でそれぞれの頻度やパーセンテージを知りたいときが多い。 table()関数を使うと，頻度を集計して表にしてくれる。 table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 prop.table()を使うと，パーセンテージを求めてくれる。 tab_iris = table(iris$Species) #まず表を別の名前で保存する prop.table(tab_iris) #カッコの中に，表を入れる ## ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 3.3 相関(correlation) 2変数間の関連のことを相関という。相関の強さは相関係数で示される。 変数\\(x\\)と変数\\(y\\)の相関係数（\\(r\\)）は，以下の式で求められる。 \\[ r = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}} \\] \\(\\sigma_{xy}\\)はxとyの共分散(covariance)で，\\(\\sigma_{xy} = \\sum^n_{i=1}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\)である（\\(\\bar{x}\\)はxの平均）。\\(\\sigma_{x}\\)と\\(\\sigma_{y}\\)はxとyそれぞれの標準偏差である。 相関係数は\\(-1\\)から\\(1\\)までの範囲をとり得る。相関係数の絶対値が大きいほど，相関関係が強いことを意味する。相関係数rが \\(r &gt; 0\\)のときは「正の相関関係」，つまり一方の変数の量が増えればもう一方の変数も増える関係にあることを意味する。\\(r &lt; 0\\)のときは負の相関関係，つまり一方が増えればもう一方が減るという関係にあることを意味する。 Rで相関係数を求めるには，cor()が使える。カッコの中に，相関係数を求めたい2つのデータを入れれば良い。また，cor.test()を使うと，相関係数の検定などより詳細な結果を示してくれる。 cor(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.1175698 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 なお，共分散はcov()で求められる。 cov(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.042434 補足: 単に「相関係数」というと，一般的には「ピアソンの積率相関係数」のことをいう場合が多い。上で示した例も，ピアソンの積率相関係数を求めている。以降の章でも，特に断りがない限り「相関係数」は積率相関係数を指すものとする。 相関係数にもいくつか種類があり，他にも例えばスピアマンの順位相関係数などもある。順位相関係数は，変数を順位に変換した上で（大きい順から1, 2, …と順位を振る）その順位を使って相関係数を求めたものである。データに極端な値（外れ値）がある場合やデータ数が少ない場合は，外れ値を調整してくれる順位相関係数の方が望ましいと言われている。 cor()及びcor.test()のオプションで，method =で相関係数の算出法を指定することができる。 cor(iris$Sepal.Length, iris$Sepal.Width, method = &quot;spearman&quot;) ## [1] -0.1667777 何も指定しなければ，デフォルトでピアソンの積率相関係数を計算してくれる。データ数が十分ある場合は，特に気にせずピアソンの積率相関係数を使えば良い。 確認問題 irisデータを使って，代表値，散布度，相関係数の求め方を復習する。 問１ Petal.LengthとPetal.Widthそれぞれの平均値と標準偏差を求めよう。 問２ Petal.LengthとPetal.Widthの共分散を求めよう。 ヒント：cov()で2つの変数を入れると求まる。 問３ 問1で求めた標準偏差と，問2で求めた共分散を使って，2変数の間の相関係数を求めよう。 また，cor()でもPetal.LengthとPetal.Widthの相関係数を求め，値が一致することを確かめよう。 ヒント：相関係数の式を確認して，相関係数を求める。 "],
["03-data.html", "Chapter 4 データ・ハンドリング 4.1 tidyverseパッケージのロード 4.2 変数の作成 4.3 データの抽出 4.4 パイプ 4.5 グルーピング 4.6 データの変換 4.7 データの結合 4.8 データの読み込み 確認問題", " Chapter 4 データ・ハンドリング データに新しく変数を加えたり，データの形式を変えるなど，より高度で複雑なデータの操作について学んでいく。 この章では，tidyverseというパッケージに入っている関数を解説する。 変数の作成 データの抽出 パイプ グルーピング データの変換 データの結合 データの読み込み 4.1 tidyverseパッケージのロード まず，tidyverseパッケージを使うためにパッケージのロードをしよう。初めて使う場合は，マシンに予めパッケージをインストールする必要がある。パッケージのインストール及びロードについては，第2章で解説されている。 install.packages(&quot;tidyverse&quot;) library(tidyverse) tidyverseは様々なパッケージを含んだ，パッケージのセットである。Rを使いやすくするための便利なパッケージがまとめて入っている。例えば，以下のパッケージが含まれている。 ggplot2: グラフを作るのに特化したパッケージ readr: ファイルの読み込みに特化したパッケージ dplyr: データの整理に特化したパッケージ など このテキストでは，以降の章での解析に必要なパッケージ及び関数のみに絞って解説する。詳細はtidyverseの公式ページを参照のこと。 https://www.tidyverse.org 注意： 以降のプログラムでは，関数を「XXXX::YYYY」と表現しているが，これらは「XXXXパッケージに入っているYYYYという名前の関数を使う」ということを意味している。XXXX::の部分は基本的に省略しても問題ないが，例えばtidyverseパッケージ以外もロードしていて，同じ名前の関数が別のパッケージに含まれている場合には，思った通りの結果が表示されない場合もある。外部パッケージの関数を使う場合は，できる限りXXXX::を付けた方が無難である。 以降では，Rに標準で入っているirisデータを例として，ファイル操作の練習を行う。以下のプログラムを実行し，irisデータをdatという名前に置き換えて使っていこう。 dat = iris 4.2 変数の作成 dplyrパッケージに入っているmutate()を使うと，新たに変数を追加することができる。 mutate()に，データの名前，新しい変数の順番で入力すると，データの右端に新しい変数を追加してくれる。 dat2 = mutate(dat, new_var = Sepal.Length + Petal.Length, hoge = ifelse(Species == &quot;setosa&quot;, 1, 0)) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species new_var hoge ## 1 5.1 3.5 1.4 0.2 setosa 6.5 1 ## 2 4.9 3.0 1.4 0.2 setosa 6.3 1 ## 3 4.7 3.2 1.3 0.2 setosa 6.0 1 ## 4 4.6 3.1 1.5 0.2 setosa 6.1 1 ## 5 5.0 3.6 1.4 0.2 setosa 6.4 1 ## 6 5.4 3.9 1.7 0.4 setosa 7.1 1 上の例では，Sepal.LengthとPetal.Lengthを足したnew_varという名前の新しい変数を作っている。更に，「Sepeciesが“setosa”ならば1, そうでなければ0とする」という条件で新たにhogeという変数を作っている。 4.3 データの抽出 dplyrパッケージに入っているselectや filter関数を使うと，データの中から必要な部分のみを取り出すことができる。 4.3.1 必要な列のみを取り出す（select） select()で，データの名前，取り出したい変数名（複数選択可）の順番で入力すると，指定した変数の列のみを取り出してくれる。 以下には，irisデータからSepal.LengthとPetal.Lengthのみを取り出す場合のプログラム例を示す。 dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) head(dat2) ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 上の例では，データの中からSepal.LengthとPetal.Lengthの列を取り出している。 4.3.2 条件に合う行を取り出す（filter） ある条件に合う行のみを取り出したい場合（例えばデータの中から男性のみを取り出したいなど），filter()で，データの名前，条件式の順番で入力すると，データの中から条件に合う行のみを取り出してくれる。 以下には，irisデータから，あやめの種類（&quot;Species&quot;）のうち&quot;versicolor&quot;のみを取り出す場合のプログラム例を示す。 dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4.0 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor データから条件に合う行だけが取り出される。上の例では，「Speciesがversicolorである」行をdatから取り出している。 「イコール」は=ではなく，==と表記していることに注意。つまり，計算式と論理式ではイコールの表現が異なる。他の論理式の表現については，第2章で説明しているので確認しておこう。例えば，「Sepal.Lengthが7以上」という条件で取り出したいときは，dplyr::filter(iris, Sepal.Length &gt;= 7)とすれば良い。 4.4 パイプ 複数のプログラムをつなげることをパイプ処理という。purrrパッケージで，Rでパイプ処理をすることができる。 例えば，irisデータで「あやめの種類のうち&quot;setosa&quot;のみの行を取り出して，更にSpecies，Sepal.Length, Petal.Lengthのみの列を取り出したい」という複数の処理をする場合を例として考える。 先程まで学んだ内容を駆使すれば，以下のように複数のプラグラムを段階的に書けばできなくはないが，プログラムが非常に長くなる（プログラムを分けて書くと途中でミスも生じやすくなる）。 dat = iris #irisデータをdatという名前に置き換える dat2 = dplyr::filter(dat, Species == &quot;setosa&quot;) #まずSpeciesのうち，setosaのみを取り出す。dat2という名前で保存する。 dat3 = dplyr::select(dat2, Species, Sepal.Length, Petal.Length) #別の名前で保存し直したdat2から，Sepal.LengthとPetal.Lengthの列を取り出す。dat3という名前で保存する head(dat3) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 パイプ（%&gt;%）を使えば，このプログラムを1行で書くことができる。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::filter(., Species == &quot;setosa&quot;) %&gt;% dplyr::select(., Species, Sepal.Length, Petal.Length) head(dat2) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 %&gt;%はプログラムを渡していく関数であり，.はそのプログラム以前の結果を示している。irisデータをfilterに渡し，その結果をselectに渡している。 ドットは省いても良い（以降でも，.は省略して表記する）。 dat2 = dat %&gt;% dplyr::filter(Species == &quot;setosa&quot;) %&gt;% dply::select(Species, Sepal.Length, Petal.Length) 4.5 グルーピング パイプを利用することで，グループごとに統計量を算出することができる。 irisデータを例として，グループごとに平均や標準偏差を計算する方法を覚えよう。 あやめの種類ごとに，がくの長さの平均値と標準偏差を算出してみる。 先ほど学んだパイプ処理（%&gt;%）に加え，dplyrパッケージのgroup_byとsummarise関数を利用する。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::group_by(Species) %&gt;% dplyr::summarise(Mean = mean(Sepal.Width, na.rm = TRUE), SD = sd(Sepal.Width, na.rm = TRUE), N = length(Sepal.Width)) dat2 ## # A tibble: 3 x 4 ## Species Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 group_by()はグループ変数を作成する関数である。データの中でグループとして使いたい変数を括弧内に指定する。 summarise()は，複数の関数を実行させる関数である。この例では，mean()，sd(), length()の3つの関数を実行し，それぞれの結果をMean, SD, Nという別の名前で保存している。 4.6 データの変換 tidyrパッケージに入っているgather()とspread()を使うと，データの並び替えなどをすることができる。 4.6.1 wide型とlong型の区別 まず，データのレイアウトには，wide型とlong型の二種類があることを理解しよう。 以下のデータを例として説明する。A, B, Cの3人の参加者が，X, Y, Z条件の３つの条件で実験課題を行ったとする。 それぞれの条件での課題の成績（数値），参加者の性別，年齢をデータとして入力する。 まずは，以下のプログラムを実行してサンプルデータを作成しよう。 dat_wide = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_wide ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 このようにデータの入力方法として，１行につき１人の参加者の情報を入力するやり方がある（実験でデータを入力する際も，このレイアウトの方が入力しやすいだろう）。このようなデータのレイアウトをwide型という。 同じデータを，以下のようなレイアウトで表現することもできる。同じく，以下のプログラムを実行して，サンプルデータを作ろう。 dat_long = data.frame(Subject = sort(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 3)), Condition = rep(c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;), 3), Score = c(6,9,7,2,3,5,7,4,7), Gender = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;, &quot;F&quot;), Age = sort(rep(c(18,19,20), 3))) dat_long ## Subject Condition Score Gender Age ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 実験成績ごとに１行ずつでデータが作られている。すなわち，同じ参加者1人につき3行のデータがある。このようなデータの方をlong型と呼ぶ。 どのデータ型にすべきか？ Rでデータ解析に使う関数は，「1つの観測値（observation）につき1行」が原則，つまりlong型でデータが入っていることを前提として作られている。このテキストで学ぶデータ解析も，基本的に分析で使うデータはlong型を前提とする。 その一方，データを入力する際にはwide型で用意する方が楽な場合もある。データ入力は研究者の都合に応じてやりやすい方法で用意するとして，解析をする際に適切なデータ形式に変換するすべを身に着けておこう。 4.6.2 wide型からlong型に変換 tidyrパッケージのgather()を使う。 dat_long2 = dat_wide %&gt;% tidyr::gather(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, key = Condition, value = Score) dat_long2 ## Subject Gender Age Condition Score ## 1 A M 18 X 6 ## 2 B F 19 X 2 ## 3 C F 20 X 7 ## 4 A M 18 Y 9 ## 5 B F 19 Y 3 ## 6 C F 20 Y 4 ## 7 A M 18 Z 7 ## 8 B F 19 Z 5 ## 9 C F 20 Z 7 gather()で，並べ替える変数を指定する。そして，key=でkey変数，value=でvalue変数として新たにつけたい名前を指定する。 4.6.3 long型からwide型に変換 tidyrパッケージのspread()を使う。 dat_wide2 = dat_long %&gt;% tidyr::spread(key = Condition, value = Score) dat_wide2 ## Subject Gender Age X Y Z ## 1 A M 18 6 9 7 ## 2 B F 19 2 3 5 ## 3 C F 20 7 4 7 spread()で，key=でkey変数，value=でvalue変数とする変数を指定する。 4.7 データの結合 複数のデータを結合したい場合は，dplyrパッケージのjoin関数を使うとよい。join関数には，left_join, full_joinなど，いくつかの種類が用意されている。 サンプルデータを使いながら，手順について説明する。まず，以下のプログラムを実行して，サンプルデータを作ろう。 dat_sample = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) dat_sample ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 実験で3人の参加者A, B, Cについて，X, Y, Zのデータを取ったとする。 更に，2人の参加者（AとB）に追加で実験を行い，Wのデータを取ったとする。 dat_sample2 = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;), W = c(8,3)) dat_sample2 ## Subject W ## 1 A 8 ## 2 B 3 dat_sampleとdat_sample2のデータを結合して，一つのデータにしたい。 full_join()で結合したい2つのデータ，更に結合する際にキーとなる変数（2つのデータに共通して存在する変数）をby=で指定すると2つのデータを結合してくれる。 なお，by=を省くと，自動で2つのデータに共通する変数を見つけて，それを手がかりに結合してくれる。 dat_sample3 = dplyr::full_join(dat_sample, dat_sample2, by = &quot;Subject&quot;) dat_sample3 ## Subject X Y Z Gender Age W ## 1 A 6 9 7 M 18 8 ## 2 B 2 3 5 F 19 3 ## 3 C 7 4 7 F 20 NA full_join()だと，2つのデータをすべてつなげてくれる。データが含まれていない部分は，欠損になる（data_sample2に参加者Cのデータはないので，欠損になっている）。 left_join()だと，left_join()で左側に入力したデータを含む部分のみをつなげてくれる。 dat_sample3 = dplyr::left_join(dat_sample2, dat_sample, by = &quot;Subject&quot;) dat_sample3 ## Subject W X Y Z Gender Age ## 1 A 8 6 9 7 M 18 ## 2 B 3 2 3 5 F 19 4.8 データの読み込み Rにもともと入っているread.csv()を使えばcsvファイルを読み込むことができるが，readrパッケージの関数を使うと大量のデータが含まれるファイルも高速で読み込んでくれる。また，readxlパッケージの関数を使えば，Excelファイルも読み込んでくれる。 4.8.1 readr 様々な形式のファイルを高速で読み書きことを目標としたパッケージ。 dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) dat ## X Y Gender ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F dat_2 = readr::read_csv(&quot;./sample_data/0_sample.csv&quot;) dat_2 ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F #データの書き出し readr::write_excel_csv(dat_2, &quot;./sample_data/0_sample_2.csv&quot;) read.csv()ではなく，read_csv()なので注意（ドットではなく，アンダースコア）。 ファイルを書き出すための関数も用意されている。 ここではwrite_excel_csv() を使っているが，単にwrite_csv()でも可。 また，read_csv()で読み込んだファイルは，データフレームではなく，tibbleという形式になる。 tibble tibbleとは，Rのデータ形式の一つである。 tibbleは，データフレームよりも可読性を向上させているのが特徴である。コンソールにはデータ全てではなく，最初の10行程度のみ，列も画面に入る範囲のみが表示される。 as_tibble()でデータフレームをtibble形式にすることもできる。 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows tibbleだとデータをすべて閲覧することはできないが，すべて閲覧したい場合はView()を使えばよい。 View(dat) 4.8.2 readxl readxlパッケージで，エクセル形式（xlsx）のファイルを読み込むことができる。 dat = readxl::read_excel(&quot;./sample/0_sample.xlsx&quot;) dat 特にオプションを指定しなければ，1番目に保存されているシートの中身をtibble形式で読み込んでくれる。読み込みたいシートや読み込む範囲を指定したい場合など，細かい点についてはread_excelのヘルプを参照してほしい。 確認問題 問１ irisデータから，1)Speciesがversicolorである行を選び，2) Species, Petal.Length及びPetal.Widthの列を取り出し，3) Petal.LengthとPetal.Widthを足し合わせた変数hogeを作るという一連の処理を，パイプ処理を使ってプログラム1行でやってみよう。 問２ irisデータから，1)Speciesがvirginica以外の行を選び，2) Species, Petal.Length及びPetal.Widthの列を取り出す処理を，パイプ処理を使ってプログラム1行でやってみよう。 ヒント：Rでは，!=が「○○ではない」を意味する論理式である（第2章参照）。 問３ irisデータで，Species別にPetal.Lengthの平均値，標準偏差を求めよう。 ヒント：group_by()とsummarise()の使い方をおさらいする。 "],
["04-graph.html", "Chapter 5 グラフ 5.1 なぜ可視化が重要か？ 5.2 Rのグラフィック 5.3 ggplot2の基本 5.4 オプション 5.5 図の保存 5.6 その他の機能 確認問題", " Chapter 5 グラフ データをグラフで表現する方法について学ぶ。 データの傾向をグラフによって表現することを可視化という。この章では，ggplot2パッケージを使って，データを可視化する方法について学んでいく。 ggplot2もtidyverseパッケージに含まれている。まずは，tidyverseパッケージをロードしよう。 library(tidyverse) 5.1 なぜ可視化が重要か？ データを解析する際には，単に平均値など数値を確認するだけではなく，必ずデータの分布を確認する習慣を身に着けよう。 例えばデータに極端に高い値があったりする場合は，代表値として平均値を使うのは適切でないことがわかったりする。データが正規分布と違うかたちをしているか，データの中に外れ値はないかなど，分布の形状をデータ解析の前に確認しておくことは重要である。 5.2 Rのグラフィック Rの利点は，データのグラフィックに優れていることである。 Rには標準でグラフを作成するための関数がいくつか用意されている。 hist()はヒストグラム，plot()は散布図，boxplot()は箱ひげ図を作成するための関数である。 hist(iris$Sepal.Length) #ヒストグラムを作成する plot(iris$Sepal.Length, iris$Sepal.Width) #散布図を作成する boxplot(iris$Sepal.Length) #箱ひげ図を作成する 手っ取り早くデータの分布や相関を確認したい場合は，これらの関数を使えば十分である。 しかし，標準で入っている関数は出力されるグラフがきれいではなかったり（論文やレポートなどで報告するには不十分），自分の好みに合わせてグラフの体裁を変えることもできるがオプションの指定方法が複雑といったデメリットもある。作成できるグラフの種類も限られている。 これに対し，ggplot2はグラフを作る機能に特化したパッケージである。 ggplot2の使い方も初学者には難しいが，以降で基本的な使い方について絞って確認していこう。 5.3 ggplot2の基本 早速，ggplot2を使ってグラフを作ってみよう。以下のプログラムを実行してみよう。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) p プログラムの解説： ggplot()：初期設定。「ggplot2を使ってグラフを書きます」という意味。必ず書く。カッコの中には何も入れなくて良い。 geom_xxxx()：グラフの種類の指定。必ず書く。xxxxには，グラフの種類を入力する。この例では，散布図を書くのでgeom_pointを指定した。更に，カッコの中に必要な設定を記す。 data =でグラフを描画するデータを指定する。 更に，aes()のカッコの中に描画に必要な要素を指定する。x=とy=でそれぞれ， x軸とy軸に指定したい変数を指定する。点の大きさ，色，線の種類など，グラフの種類によって指定できる要素がある。 オプション：上での例では，labs()でx軸やy軸のラベルを指定している。他にも，軸の値の範囲，軸のラベル，グラフの色の設定などをオプションで指定することができる。オプションを加えなくてもグラフは出力される。 とりあえず，これらだけ知っておけばggplot2でグラフが作れる。 5.3.1 散布図 geom_pointで作成できる。 p = ggplot2::ggplot() + geom_point(data=mpg, aes(x=cty, y=hwy, shape = fl)) p この例では，shape =でグループを指定して，グループごとに点のかたちを変えた散布図を作成した。 点が重なって見えにくい場合は，geom_jitterを使うとランダムのズレを生成して表示してくれる。 p = ggplot2::ggplot() + geom_jitter(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 5.3.2 ヒストグラム geom_histogramで作成する。 p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length)) #xに，横軸にしたい変数を入れる。 p p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, fill = Species)) #種類ごとに色の塗りつぶしを変えたい場合は，fillに指定する。 p p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, color = Species)) #colorだと周りの線の色を変える。 p 5.3.3 箱ひげ図 geom_boxplotで作成する。 最小値，第一分位点，中央値，第三分位点，最大値を示す（外れ値は点で示される）。 p = ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 5.3.4 バイオリンプロット データの分布を表現したグラフ。 geom_violinで作成する。 p = ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 5.3.5 折れ線グラフ geom_line()を使う。geom_line()だけだと線のみだが，geom_point()で作ったグラフを重ねることで，点もつけることができる。このように，複数のグラフを重ねて描画することもできる。 #サンプルデータをつくる: 10日間の気温の変化 temperature = data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) temperature ## Days Celsius ## 1 1 17.2 ## 2 2 17.5 ## 3 3 18.1 ## 4 4 18.8 ## 5 5 19.0 ## 6 6 19.2 ## 7 7 19.7 ## 8 8 20.2 ## 9 9 20.5 ## 10 10 20.1 p = ggplot() + geom_line(data = temperature, aes(x=Days, y=Celsius)) + geom_point(data = temperature, aes(x=Days, y=Celsius)) p 5.3.6 エラーバーつきのグラフ geom_errorbar()でエラーバーをつけることができる。 あるいは，geom_pointrange()でも作れる。 #サンプルデータをつくる sample_dat = data.frame(Condition=c(&quot;A&quot;, &quot;B&quot; ,&quot;C&quot;), mean=c(2, 5, 8), lower=c(1.1, 4.2, 7.5), upper=c(3.0, 6.8, 9.1)) #meanが平均，lowerとupperにそれぞれ下限値と上限値。 p = ggplot() + geom_point(data = sample_dat, aes(x = Condition, y = mean)) + geom_errorbar(data = sample_dat, aes(x = Condition, ymax = upper, ymin = lower), width = 0.1) #まず，geom_pointで平均を点で示したグラフを作成する。そのグラフに，ymaxとyminにそれぞれ上限値と下限値を指定したエラーバーのグラフを重ねる（widthでエラーバーの横の長さを指定できる）。 p p2 = ggplot() + geom_pointrange(data = sample_dat, aes(x = Condition, y = mean, ymax = upper, ymin = lower)) #geom_pointrangeならば，点とエラーバーの両方を一括して指定できる。 p2 5.4 オプション 5.4.1 ファセット（Facet） グループごとにグラフを分けたい場合は，ファセット（facet）を利用すると良い。facet_wrap()を使う。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + facet_wrap(vars(Species)) p 5.4.2 ラベル（labs） x軸やy軸のラベルを変えたいときは，labsを使うと良い。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) 5.4.3 テーマ（Theme） theme()で，フォントの大きさや色などグラフのテーマも細かく変えることができる。具体的にどの部分を変えられるかは，theme()のヘルプで確認してほしい。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme(axis.title.x = element_text(size = 20)) #x軸のフォントサイズを変える p + theme(panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) #背景や枠線の色を変える 手っ取り早く一括でテーマを変えたい場合は，予め用意されている既存のテーマを選ぶと良い。theme_bw(), theme_classic()などが用意されている。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme_bw() p + theme_gray() p + theme_classic() 5.5 図の保存 ggsave()を使う。plotに保存した図を，filenameにファイル名を指定すると，ワーキングディレクトリに作成した図が保存される。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Sepal Length&quot;, y = &quot;Petal Length&quot;) + theme_bw() ggsave(plot = p, filename = &quot;plot.png&quot;) #ファイル名を指定する。拡張子も忘れずにつける。 ggsave(plot = p, filename = &quot;plot_2.png&quot;, dpi = 300) #解像度（dpi）を指定可能。 ggsave(plot = p, filename = &quot;plot_3.png&quot;, width = 8, height = 5) #幅(width)や高さ(height)を指定可能（単位はインチ）。 5.6 その他の機能 ggplot2の機能はまだまだある。このテキストで説明しきれていないことについては，関数のヘルプやRStudioのサイトにあるggplot2のCheat sheetを見てみよう（日本語訳もある）。 https://rstudio.com/resources/cheatsheets/ 確認問題 Rに標準で入っているmtcarsデータを使って，グラフを作成しよう。 問１ Rで標準で入っているhist()を使って，変数mpgのヒストグラムを作ろう。 問２ Rで標準で入っているplot()を使って，変数mpgと変数wtの散布図を作ろう。 問３ ggplot()で，変数mpgと変数wtの散布図を作ろう。グラフには以下の点を反映させること。 x軸をmpg，y軸をwtとする。 x軸のラベルは「Miles/gallon」，y軸のラベルは「Weight」とする。 テーマはtheme_classic()にする。 "],
["05-probability_distribution.html", "Chapter 6 確率分布 6.1 確率変数と確率分布 6.2 一様分布 6.3 二項分布 6.4 正規分布 6.5 Rで使える確率分布関数 6.6 なぜ正規分布はよく使われるのか？ 6.7 確率分布の表現の仕方 6.8 その他の確率分布 確認問題 6.9 補足", " Chapter 6 確率分布 前章まではRの使い方について扱ってきたが，この章以降から本格的に統計解析を学んでいく。まずは，基礎統計学の復習であるが，「確率分布」を学んでいく。確率分布は統計学の基礎であると同時に，今後学んでいく線形モデルを理解する上でとても必要な知識である。 確率変数と確率分布 一様分布 二項分布 正規分布と中心極限定理 Rの確率分布関数 その他の確率分布 準備として，以下のプログラムを実行しよう。 この章でもtidyverseパッケージを使うので，ロードしておく。 library(tidyverse) 更に，乱数の種を指定する。set.seed(1)と入力して実行する。このプログラムを実行しておくと，このテキストに書かれているプログラムの結果と同じ結果が再現できる。 set.seed(1) 6.1 確率変数と確率分布 まず，サイコロを例として，確率分布とは何かについて理解する。 サイコロを1個投げるとする。それぞれの目が出る確率は1/6である。それぞれの目を\\(X\\)（1, 2, 3, 4, 5, 6），それぞれの目が出る確率を\\(P(X)\\)とする。\\(X\\)と\\(P(X)\\)を以下の表で示す。 このとき，Xを確率変数と呼ぶ。確率変数とは，その値と対応する確率が存在する変数のことをいう。表のように，確率変数とその変数が取り得る確率の分布を確率分布という。 確率分布には様々な種類がある。まず，一様分布，二項分布，ベルヌーイ分布について理解する。 6.2 一様分布 1個のサイコロをふったとき，それぞれの目が出る確率はどれも1/6で等しい。このように，どの確率変数Xについても常に一定の値の確率を取る確率分布は，一様分布(uniform distribution)と呼ばれる。 6.3 二項分布 コインを\\(n\\)回投げる。表が出る確率を\\(q\\)とすると，裏が出る確率は\\((1-q)\\)である。\\(n\\)回中，表が\\(x\\)回出る確率\\(P(x)\\)は，以下の式で求められる。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] \\(x\\)を確率変数とした場合，上記の式の確率に従う確率分布を二項分布(binomial distribution)という。 つまり二項分布は，2つのカテゴリで表現されるある事象が，何回生じるかの確率を表している。コイン（表か裏）を何回か投げたときの表が出る回数，学生の中から何人か選んだときの男の人数など。これらのような事象が生じる確率は，理論的には二項分布に従う。 例えば，コインの表が出る確率qを0.5とする。10回投げたときに表が6回出る確率を計算してみよう。Rならば，dbinom()関数を使えば計算できる（この関数の意味については，また後で説明する）。 #xは確率変数（コインの例でいうと表が出た回数），sizeは試行回数（コインの例でいうとコインを投げた回数）， dbinom(x=6, size=10, prob=0.5) ## [1] 0.2050781 #上の式n, x, pに実際に値を入れて計算する。dbinom()関数を使った場合と結果が一致することを確認しよう。 choose(10, 6) * 0.5^6 * (1 - 0.5) ^4 ## [1] 0.2050781 6.3.1 二項分布の期待値と分散 表が出る回数xが0〜10回全てについて，それぞれが生じる確率を計算すると以下のようになる。 dbinom(x=0:10, size=10, prob=0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 グラフにすると以下のようになる。横軸をx, 縦軸をP(x)とする。 d_plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) p = ggplot() + geom_bar(data = d_plot, aes(x = factor(x), y = p), stat=&quot;identity&quot;) + labs(y= &quot;P(x)&quot;, x= &quot;x&quot;) p グラフからもわかるように，表が出る確率が0.5のコインを10回投げたときに，最も出やすいのは10回中5回であることがわかる。10回中8回以上はほとんどまれであることがわかる。 この図からも，確率分布には最も出やすい変数（平均値。確率分布の場合は「期待値」と呼ぶ）と分散が存在することがわかる。 二項分布の期待値\\(E(x)\\)と分散\\(Var(x)\\)は，以下の式から計算できる。 \\[ E(x) = nq\\\\ Var(x) = nq(1-q) \\] この式から，表が出る確率が0.5（つまり，\\(q=0.5\\)）として，10回投げた場合(つまり, \\(n=10\\))における，表が出る回数の期待値と分散を計算してみよう。 #E(x) = nq 10*0.5 ## [1] 5 #Var(x) = nq(1-q) 10*0.5*(1-0.5) ## [1] 2.5 6.3.2 ベルヌーイ分布 二項分布で\\(n=1\\)のときは，ベルヌーイ分布(Bernoulli distribution)と呼ぶ。\\(x\\)は1か0を取る変数で，\\(x=1\\)のときの確率を\\(q\\)とすると，\\(x=1\\)及び\\(x=0\\)となる確率は以下の式で表される。 \\[ P(x=k) = q^k(1-q)^{(1-k)} \\] 例： コインを一回だけ投げたときに，表が出るあるいは裏が出る確率。 サイコロを一回だけ振ったときに，1が出るあるいは1以外が出る確率。 dbinom(x=0:1, size=1, prob=0.5) #表が出る確率0.5のコインを投げた時に，表が出ない時の確率と表が出る時の確率が出力される。 ## [1] 0.5 0.5 dbinom(x=0:1, size=1, prob=1/6)#ある目が出る確率1/6のサイコロを振った時に，その目以外が出る確率とその目が出る確率が出力される。 ## [1] 0.8333333 0.1666667 6.4 正規分布 6.4.1 正規分布の基礎 統計学で用いられる確率分布の中でも有名なのは，正規分布(normal distribution)である。正規分布は，平均\\(\\mu\\)，標準偏差\\(\\sigma\\)を持つ確率分布で，釣鐘型（ベル・カーブ）の分布を描く。 平均\\(\\mu\\)，標準偏差\\(\\sigma\\)とする正規分布の確率密度関数\\(f(x)\\)は，以下の式から計算される（「確率密度関数」とは何かは，後で説明する）。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] 正規分布の式を覚える必要はない。式の中に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)が含まれていることだけ記憶しておこう。また，正規分布はガウス分布（gaussian distribution）と呼ばれることもある。 試しに，平均0，標準偏差1の正規分布のグラフを作ってみよう。以下のプログラムではdnorm()という新しく出てきた関数もあるが，使い方については後で解説する。 x = seq(-3, 3, 0.05) # -3から3まで0.05刻みで数字の連続を作る y = dnorm(x=x, mean=0, sd=1) #平均0，標準偏差1の正規分布の確率を算出する dat_norm = data.frame(x = x, y = y) p = ggplot() + geom_line(data = dat_norm, aes(x = x, y = y)) p 6.4.2 確率密度関数 正規分布のグラフは確率分布を表しているのではなく，確率密度関数であることにも注意してほしい。先程の二項分布では，縦軸は，横軸の値（確率変数）が生じる「確率」を意味していた。しかし，正規分布のグラフの縦軸は，横軸の値が生じる確率そのものを意味しない。確率密度関数は，グラフの面積が確率を表す。 例えば，x = 0のときのyの値を確認しよう。 dnorm(x = 0, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 ## [1] 0.3989423 dnorm()は，確率密度関数の縦軸の値を出力する。x = 0のときの縦軸の値は0.4であり，先程のグラフからもわかるように，縦軸の値と一致している。しかし，x = 0が生じる確率は0.4というわけではない。確率密度関数の場合，縦軸の値そのものは何の意味も持たない。 二項分布のような離散型の変数である場合に対して，正規分布のように変数が連続型（小数点を含む値）であるものは，特定の値の確率を定義することができない。例えば，身長が170.5cmである確率を求めるにしても，170cmから171cmまでの間に,170.001cm, 170.002cmと無限の値が広がっている。170.5cmぴったりである確率を求めることはできない（特定の値が生じる確率はゼロということになる）。したがって，連続型の変数の確率分布は，個々の確率を求めるのではなく，確率密度関数によって区間の確率を求める。 x &lt; 0の範囲の面積を求めよう。正規分布の左半分なので，確率は0.5である。 pnorm(q = 0, mean = 0, sd = 1) #-∞からqの値までの範囲を求めてくれる。 ## [1] 0.5 6.4.3 正規分布の平均値と標準偏差 以下が平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の値をそれぞれ変えた場合の正規分布である。赤が平均0で標準偏差1，青が平均1で標準偏差1, 黒が平均0で標準偏差2である。 x = seq(-3, 3, 0.05) y_1 = dnorm(x=x, mean=0, sd=1) y_2 = dnorm(x=x, mean=1, sd=1) y_3 = dnorm(x=x, mean=0, sd=2) dat_norm_1 = data.frame(x = x, y = y_1) dat_norm_2 = data.frame(x = x, y = y_2) dat_norm_3 = data.frame(x = x, y = y_3) p = ggplot() + geom_line(data = dat_norm_1, aes(x = x, y = y), color=&quot;red&quot;)+ geom_line(data = dat_norm_2, aes(x = x, y = y), color= &quot;blue&quot;)+ geom_line(data = dat_norm_3, aes(x = x, y = y), color=&quot;black&quot;) p 6.5 Rで使える確率分布関数 ここまで，dbinom，dnorm, pnormなど，確率分布を扱う関数がいくつか出てきた。これらは，Rに標準で入っている関数である。 Rには確率分布から乱数を生成したり，確率変数の確率を求めることができる関数が実装されている。関数は，確率分布それぞれにrXXXX, qXXXX, pXXXX, dXXXXといった4種類の関数が用意されている（ XXXXには確率分布の種類が入る）。 rXXXXは，乱数(random number)を出力する。 rnorm(n = 10, mean = 0, sd = 1) #平均0，標準偏差1に従う正規分布から乱数を10個生成する ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 dXXXXは，確率変数xが生じる確率密度（確率密度関数の縦軸の値）を出力する。 dnorm(x = 0.5, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，x=0.5のときの確率密度を求める（確率密度の値であって，確率そのものではないので注意） ## [1] 0.3520653 qXXXXは，確率点(quantile)を出力する。ある確率を取る時のx軸の値を出力してくれる。 qnorm(p = 0.5, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，p ≤ 0.5の確率となるときの確率変数の値を求める ## [1] 0 pXXXXは，累積確率を出力する。 pnorm(q = 1, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，x ≤ 1の確率を求める ## [1] 0.8413447 他の確率分布についても同様に，4種類の関数が用意されている。 #乱数を作る関数 runif(n=100, min = 0, max = 1) #一様分布からの乱数 rbinom(n=100, size=10, prob=0.5) #二項分布からの乱数 6.6 なぜ正規分布はよく使われるのか？ なぜ正規分布は「正規（normal）」な分布なのか？よく言われるのは，「現実世界の様々な分布が正規分布に従う」からという説明である。 しかし，現実には正規分布に従わないデータの方が多い。体重や身長なども，実際には正規分布を描くことは少ない。年収なども釣鐘型の分布にならない。また，正規分布は確率変数が連続量の場合の確率分布である。心理学ではアンケートへの回答得点などを分析したりするが，離散値や順序尺度（すなわちカテゴリカル変数）が正規分布に従うという前提を置くのはそもそも適切ではない。 では，正規分布がよく使われる理由は何なのか。もっとらしい理由は，数学的な扱いやすさである。なぜならば，元の変数がどのような確率分布に従っていたとしても，変数を足し合わせた結果は正規分布に従うという都合の良い性質があるからである。この性質は，中心極限定理と呼ばれるものである。中心極限定理により，正規分布は身近に現れる「正規な分布」となり得るのである。 6.6.1 中心極限定理 中心極限定理とは，「母集団が平均及び標準偏差を持つ確率分布であるならば，たとえ母集団が正規分布でなくても，母集団から標本を無作為抽出して平均値を計算することを何回も繰り返すとその分布は正規分布に近づく」という定理である。 シミュレーションで中心極限定理を実感してみよう。6面のサイコロを100回振る実験を行うとする。 以下のプログラムを実行してみよう。runif()は一様分布から乱数を生成する関数である。round()は値を丸める関数で，以下では小数点以下の値を丸めて整数の値を出力するようにしている。 X = round(runif(n = 100, min = 1, max = 6),0) mean(X) ## [1] 3.5 それぞれの目が出る確率は1/6で一定である。すなわち，サイコロが出る目は一様分布に従う（つまり，元の分布は正規分布ではない）。一様分布の平均値は，最大値をa, 最小値をbとすると，(a+b)/2。つまり，サイコロの例の場合の平均値は理論的には(1+6)/2=3.5となる。 サイコロを100回振って平均値を求める。この平均値を求めるのを，1,000回繰り返す。求めた平均値1,000個の分布を見てみよう。 以下のプログラムで，シミュレーションとグラフの作成を行う。sapplyは，ある処理を繰り返し行う関数である。詳細な説明は省くが，以下のプログラムでは，100回サイコロを振って平均値を求める処理mean(round(runif(n = 100,min=1,max=6),0))を1,000回行ってその結果を保存して，sample.meansという名前のベクトルで保存している。 sample.means = sapply(c(1:1000), function(x) {mean(round(runif(n = 100,min=1,max=6),0))} ) hist(sample.means) ヒストグラムは正規分布に似ている。もっと回数を増やすと，より正規分布っぽいかたちになる。 #サイコロを7回振ってその合計を求める。これを10,000回行ったときの出目の合計値の分布 sample.sum = sapply(c(1:10000), function(x) {sum(round(runif(n = 7,min=1,max=6),0))} )#sum()はカッコ内のベクトルの要素を足し合わせる関数 hist(sample.sum) このように，元の母集団の分布がたとえ正規分布でなくても（この例の場合は一様分布），その標本平均は正規分布に近似する。平均値を計算しなくても，単に値を足し合わせるだけでも同じことである。 中心極限定理により，たとえ元の変数が正規分布に従っていなくても，変数を加算したものは正規分布に近似する。なので，心理学で行われる様々なデータ分析も，以下のような処理を行うことで変数が正規分布に従うという前提に基づいて分析がなされる。 複数の質問項目（順序尺度）をまとめて平均化した心理尺度を分析に使う。 選挙への投票（「した」もしくは「しなかった」の二値）者の割合を県ごとに算出して，県を単位として分析に使う。 中心極限定理は簡単に言うと，「どのような確率分布に従う変数でも，足し合わせると正規分布に近くなる」ということである（ただし，この章の補足でも述べているように例外もある）。 6.7 確率分布の表現の仕方 確率変数と確率分布の関係は，以下のように表現することも多い。 二項分布の場合 \\[ x \\sim Binomial(n, q) \\] Binomialのカッコ内のn, qのように，確率分布を構成する変数のことをパラメータ(parameter)と呼ぶ。 Binomialは二項分布，\\(\\sim\\)は「従う」という意味である。つまりこの式は，「\\(x\\)は，\\(n\\)と\\(q\\)をパラメータとする二項分布に従う」ということを示している。 正規分布の場合 \\[ x \\sim Normal(\\mu, \\sigma) \\] 正規分布のパラメータは，平均\\(\\mu\\)と標準偏差\\(\\sigma\\)である。この2つが決まれば，正規分布の形状が決まる。 6.8 その他の確率分布 ここまで，確率分布として一様分布，二項分布，ベルヌーイ分布，正規分布があることを学んできた。他にも，確率分布はたくさんある。 確率分布は大きく分けて，確率変数が離散値である離散型分布と連続値である連続型分布に分かれる 6.8.1 離散型分布 確率変数が離散値（小数の値を取らないもの）である場合の確率分布である。二項分布，ベルヌーイ分布に加え，ポアソン分布や幾何分布などがる。ここでは，ポアソン分布についてのみ触れる。 ポアソン分布 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ x \\sim Poisson(\\lambda)\\\\ \\] xは0以上の整数（0, 1, 2, 3, …）とする。 ポアソン分布 (poisson distribution)のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，ポアソン分布は平均と分散が等しい分布である。 以下に，パラメータ\\(\\lambda\\)をそれぞれ変えた場合のポアソン分布を示す。 pois_1 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=1), lambda=1) pois_2 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=2), lambda=2) pois_3 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=3), lambda=3) pois = rbind(pois_1, pois_2, pois_3) ggplot() + geom_bar(data = pois, aes(x=factor(x), y=p, fill=factor(lambda)), stat=&quot;identity&quot;, color = &quot;black&quot;, position = &quot;dodge&quot;) + labs(y= &quot;P(x)&quot;, x = &quot;x&quot;, fill = &quot;lambda&quot;) 一定の期間中にランダムで生じる事象はポアソン分布に従う。具体的な例としては，1日の間に届くメールの件数，営業時間中に来る客の数など。 二項分布\\(Binomial(n, q)\\)の\\(n\\)が十分大きく，かつ\\(q\\)が小さい場合は平均を\\(np\\)とするポアソン分布に近似する。つまり，めったに起こらない事象はポアソン分布に従う。例えば，1年間の間に生じる交通事故の件数など（365日それぞれで0.1%で生じる場合など）。歴史的に有名な例として，「ドイツ軍で1年間で馬に蹴られて死亡した兵士の数」がポアソン分布に従うといったものがある。 二項分布と同じく，ポアソン分布の確率変数は離散値である。離散値は，1個，2個，3個と数える個数のような整数の値をいう（1.1個といった小数の値が存在しないもの）。それに対し，正規分布の確率変数は連続量である。 6.8.2 連続型分布 確率変数が連続値（小数の値が存在するもの）である確率分布である。すでに見てきた正規分布に加え，他にも指数分布，対数正規分布，ガンマ分布などがある。 指数分布 指数分布(exponential distribution)のパラメータは，\\(\\lambda\\)である。指数分布の確率変数は，ゼロ以上の正の値のみを取りうる。 \\[ f(x) = \\lambda \\exp(-\\lambda x)\\\\ x \\sim Exponential(\\lambda)\\\\ \\] ある一定期間の間に平均して\\(\\lambda\\)回生じるイベントの時間間隔は，指数分布に従う。災害が生じてから次の災害が生じるまでの期間，店に客が来てから次の客が来るまでの期間などは指数分布に従う。 対数正規分布 対数正規分布(log-normal distribution)は，正規分布と同様に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の２つのパラメータを持つ分布で，名前の通り正規分布とは深い関係にある。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}x}\\exp\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right)\\\\ x \\sim logNorma(\\mu, \\sigma)\\\\ \\] 対数正規分布は，右の方に長い裾を描く分布である。対数正規分布に従うものとしては，年収の分布などが知られている。 ランダムな変数を足し合わせた物が正規分布に従うのに対し，ランダムな変数をかけ合わせたものは対数正規分布に従う。 #サイコロを7回振って，それぞれの値で掛け算をする。これを10,000回行ったときの出目の合計値の分布 sample.prod = sapply(c(1:10000), function(x) {prod(round(runif(n = 7,min=1,max=6),0))} ) #prod()はカッコ内のベクトルの要素をかけ合わせる関数 hist(sample.prod) 「ランダムな変数をかけ合わせたもの」の対数を取ってその分布を確認すると，正規分布のかたちになることがわかる。 sample.prod_2 = log(sample.prod) hist(sample.prod_2) 掛け算の対数を取るということは，足し算に直すことと同じである。つまり，対数を取ることで「ランダムな変数をかけ合わせたもの」を「ランダムな変数を足し合わせたもの」に変換される。「ランダムな変数を足し合わせたもの」は，中心極限定理により正規分布に従う。 したがって，対数正規分布の対数を取ったものは正規分布になる。 確認問題 問１ あなたは野球部の監督で，自分のチームの勝率はこれまでの練習の経験から32%だとわかっている。 これから遠征で，全部で10試合を行う予定である。 勝つ試合の回数をnとし，nとそれぞれのnに対応する確率を求めよ。 ヒント:dbinom()関数を使おう。 平均して何試合勝つことができるかを求めよ。 ヒント:二項分布の平均値の求め方を復習する。 問２ ある学校で小学6年生の身長を測ったところ，平均は150.2 cmで標準偏差が3.5 cmであった。 身長152 cmから155 cmの児童の割合はいくらか。 ヒント：pnorm()を使って求めよう。 身長158 cmを超える児童は何割いるか。 ヒント：同じく，pnorm()を使う。なお，全ての範囲の確率の合計は1である。 6.9 補足 6.9.1 中心極限定理の例外 中心極限定理より，元がどのような分布でも足し合わせると正規分布に近似するが，例外もある。例えば，コーシー分布（Cauchy distribution）と呼ばれる確率分布は，中心極限定理を適用できない。 コーシー分布は形状が正規分布に似ているが，裾が広いことが特徴である。つまり，極端な値が出る確率が正規分布よりも大きい。パラメータは，locationとscaleの2つである。以下に，locationが0，scaleパラメータが1と3の分布の例を示す。 dat_cauchy_1 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 1), scale = &quot;1&quot;) dat_cauchy2 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 3), scale = &quot;3&quot;) dat_cauchy = rbind(dat_cauchy_1, dat_cauchy2) ggplot() + geom_line(data = dat_cauchy, aes(x = x, y = y, linetype = scale)) + xlim(-6,6) + labs(x = &quot;y&quot;, y = &quot;density&quot;) コーシー分布は，極端な値が存在することにより，その分布の特徴を平均や標準偏差などで捉えることができない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) #location = 0, scale = 1のコーシー分布から乱数を100個生成する mean(d_cauchy) ## [1] -1.812771 median(d_cauchy) ## [1] -0.01332628 sd(d_cauchy) ## [1] 13.17875 コーシー分布から乱数を100個作って足し合わせることを5回やり，分布を確認する。足し合わせても，正規分布に近似しない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) hist(d_cauchy) "],
["06-NHT.html", "Chapter 7 統計的仮説検定 7.1 準備 7.2 統計的仮説検定の考え方 7.3 統計的仮説検定の種類 7.4 統計的仮説検定で重要な概念 7.5 統計的検定が抱える問題 確認問題", " Chapter 7 統計的仮説検定 統計的仮説検定の考え方とそれが抱える問題について理解する。 統計的仮説検定の考え方（p値とは何か？） 第1種の過誤と第2種の過誤 p値とサンプル数との関係 7.1 準備 この章でも，tidyverseパッケージを使う。予めロードしておく。 library(tidyverse) 7.2 統計的仮説検定の考え方 前の章で学んだ二項分布を用いて，統計的仮説検定の考え方について学ぶ。p値とは何なのかを理解する。 7.2.1 二項分布の復習 コインを10回投げて表が出た回数\\(x\\)をカウントしていく。”理論的”には，表が\\(x\\)回出る確率\\(P(x)\\)は，コインを投げる回数\\(n\\)と表が出る確率\\(q\\)をパラメータとする二項分布に従う。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)}\\\\ x \\sim Binomial(n, q) \\] plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) ggplot() + geom_bar(data=plot, aes(x=factor(x), y=p), stat=&quot;identity&quot;) + labs(y = &quot;P(x)&quot;, x =&quot;x&quot;) 7.2.2 統計的仮説検定 ”理論的には”，表が出る回数\\(x\\)が生じる確率は上の図のようになる（平均は\\(nq = 10*0.5 = 5\\)）。 では，実際にコインを10回投げてみて表が出た回数を数えてみたところ，表が2回しか出なかったとする。この結果から，「このコインには歪みがあって，片一方の面だけが出やすい」と言ってもよいのか？ これを検討するために，表と裏それぞれが出る確率の等しいコインを投げる場合（すなわち，\\(q=0.5\\)の場合）との比較を行い，今回の実験結果がどれくらいまれな事象と言えるのかを比較する。 このとき，研究者が検証したい仮説を対立仮説（alternative hypothesis），対立仮説を検証するために比較の対象とする「偏りを仮定しない」仮説のことを帰無仮説(null hypothesis)と呼ぶ。 では，今回の帰無仮説となる二項分布（2つのパラメータが，\\(n=10, q=0.5\\)の場合)の分布を見てみよう。理論的には，表が\\(x\\)回出る確率\\(P(x)\\)は，\\(x\\)それぞれについて以下のようになる。 d = data.frame(x=0:10, p_x =dbinom(x=0:10, size=10, prob=0.5)) d ## x p_x ## 1 0 0.0009765625 ## 2 1 0.0097656250 ## 3 2 0.0439453125 ## 4 3 0.1171875000 ## 5 4 0.2050781250 ## 6 5 0.2460937500 ## 7 6 0.2050781250 ## 8 7 0.1171875000 ## 9 8 0.0439453125 ## 10 9 0.0097656250 ## 11 10 0.0009765625 表もしくは裏が出る回数が2回以下の場合の確率を計算すると， d$p_x[1] + d$p_x[2] + d$p_x[3] + d$p_x[9] + d$p_x[10] + d$p_x[11] ## [1] 0.109375 となる。つまり，もし歪みのないコインならば，片一方の面だけが出る回数が2回以下の確率はおおよそ0.11ということになる。 この例で求めた確率0.11のように，「帰無仮説の前提のもとで，特定の実験結果が得られる確率」をp値と呼ぶ。 p = 0.11 は小さい確率のように思える。なので，「歪みのないコインならば，一方の面が2回出る確率は本来0.11である。本来だったらあまり起こり得ない実験結果が得られたので，このコインは歪みのないコインであると結論づけるのは自然ではない。ゆえに，このコインには歪みがあって片一方の面が出やすい」という結論を出すのが妥当なように思える。 しかし，人によって0.11を小さいと評価しても良いのか，基準が分かれる。そこで，研究者の間でどこまでの数値を小さいと評価するかの基準が決まっている。この基準となる確率が，有意水準である。 一般的に有意水準には0.05（5%）とされることが多い。ただし，なぜ5％を判断基準とするのかについては特に明確な理由はない（みんなから合意されているからという以上の理由はない）。 つまり，「帰無仮説（フェアなコインを投げる）の前提のもとでは，表が出る回数が2回以下の確率は0.11 であった。これは小さい確率のように思えるが，判断基準の5％よりかは大きい。すなわち，このコインはゆがんでいると結論付ける訳にはいかない」ことになる。 以上が，統計的仮説検定の考え方である。まとめると， 1)ある特定の理論分布（帰無仮説）のもとで今回の実験結果が生じる確率（p値）を求め， 2)その確率が小さいかを評価し， 3)小さい場合は帰無仮説を棄却する というのが，統計的仮説検定のプロセスである。 今回のように「コインが表か裏かに関わらず，一方の面だけが出やすい」という対立仮説を検討する場合の検定は，両側検定という。仮に，今回の仮説で表と裏を区別するとして「表が出にくい」つまり「表が出る回数が2回以下の確率」を対象とする場合，このような検定を片側検定という。二項分布は左右対称の分布なので，両側p値は片側p値の2倍の値である(厳密には左右対称ではないのであくまで近似値)。多くの場合，両側検定を使うのが一般的である。 7.3 統計的仮説検定の種類 7.3.1 二項検定 コイン投げの例は，二項分布に従う事象である。二項分布に従う事象の統計的仮説検定は，二項検定と呼ばれる。 Rにも，二項検定を行うための関数binom.test()が用意されている。 binom.test()に二項分布のパラメータ（\\(n\\)と\\(q\\)にあたる数値）と実験結果を入れると，p値を求めてくれる。 上の例について，binom.test()でp値を求めてみよう。 binom.test(x = 2, n = 10, p = 0.5) #出てくる結果はデフォルトで両側検定になる。 ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 7.3.2 t検定 心理統計では，2つのグループの間で平均値に差があるかどうかの統計的仮説検定として，t検定を使うことが多い。連続量の変数を扱う検定の場合は，t検定がよく使われる。 t検定の考え方も，基本的に上と同じである。2つの集団の間で平均値に差がないと仮定したときの理論分布（t分布）と比べて，実際に得られた値がどれくらい珍しいのかを検討する。 Rにも，t検定を行うための関数t.test()が標準で入っている。 まず，以下のプログラムを実行して，サンプルデータを作る。 set.seed(1) Value = c(rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 10, mean = 1, sd = 1)) Treatment = c(rep(&quot;X&quot;, 10), rep(&quot;Y&quot;, 10)) sample_data = data.frame(Treatment = Treatment, Value = Value) str(sample_data) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Treatment: chr &quot;X&quot; &quot;X&quot; &quot;X&quot; &quot;X&quot; ... ## $ Value : num -0.626 0.184 -0.836 1.595 0.33 ... 実験でXとYの２つの条件(Treatment)を設定し，ある値（Value）を測定したとする。 まず，2つの条件別にValueの平均値や標準偏差を求める。 sample_data %&gt;% group_by(Treatment) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), N = length(Value)) ## # A tibble: 2 x 4 ## Treatment Mean SD N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 X 0.132 0.781 10 ## 2 Y 1.25 1.07 10 条件Yの方が条件Xよりも平均値が大きいよう見えるが，そう結論づけて良いのか。これをt検定で検討しよう。 まず，2つの集団間の平均値の差を元に，以下の式から「t値」を求める。 \\[ t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\sigma^2_{X}/n_{X}+\\sigma^2_{Y}/n_{Y}))}} \\] \\(\\bar{X}\\)と\\(\\bar{Y}\\)はそれぞれ条件Xと条件Yの平均値，\\(\\sigma^2_{X}\\)と\\(\\sigma^2_{Y}\\)はそれぞれ条件Xと条件Yの分散，\\(n_{X}\\)と\\(n_{Y}\\)はそれぞれ条件Xと条件Yのサンプル数である。 XとYが同じ正規分布\\(Normal(\\mu, \\sigma^2)\\)から抽出される場合，t値は自由度\\(n_{X}+n_{Y}-2\\)のt分布に従う。 つまり，同じ平均値を持つ母集団からXとYが抽出された（つまり，\\(\\bar{X} = \\bar{Y}\\)のとき）と仮定した上でp値を求め，今回のデータが得られる確率がどのくらいまれであるかを検討する。 Rのt.test()関数を使って実験データの情報を入力すれば，p値を求めてくれる。 t.test(data = sample_data, Value ~ Treatment) #dataにデータの名前，比較の対象となる変数~グループを意味する変数とうかたちで入力すると結果が出力される。 ## ## Welch Two Sample t-test ## ## data: Value by Treatment ## t = -2.6669, df = 16.469, p-value = 0.01658 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.0022169 -0.2310675 ## sample estimates: ## mean in group X mean in group Y ## 0.1322028 1.2488450 p値は0.02であった。これは5%よりも小さいので，「今回の結果が生じる確率はまれである。XとYの母集団の平均値は等しいちう帰無仮説を棄却し，XとYは平均値が異なる集団を母集団とする」と結論づけることになる。 t検定には，2つの標本の母集団の分散が等しいと仮定するかしないかで二種類の検定がある。母集団の分散が等しいと仮定しない場合の検定はウェルチの検定(Welch’s t-test)と呼ばれ，Rのt.test()関数でデフォルトで出る検定結果はこのウェルチの検定による結果である。一般的に2つの標本の母分散は不明であるので，それらが等しいかどうかも不明である。なので，等分散を仮定しないt検定をしておくほうが保守的である。 7.4 統計的仮説検定で重要な概念 7.4.1 第1種の過誤と第2種の過誤 「帰無仮説が真なのに，帰無仮説を棄却してしまう誤り」のことを，第1種の過誤（type Ⅰ error）という。つまり，「本当は差がないのに，”差がある”と判断してしまう誤り」のことである。 これに対し，「帰無仮説が偽なのに，帰無仮説を採択してしまう」誤りのことを，第2種の過誤（type Ⅱ error）という。つまり，「本当は差があるのに，”差がない”と判断してしまう誤り」のことである。 第1種の過誤を犯す確率\\(\\alpha\\)は，要は有意水準の値そのものである（\\(\\alpha=0.05\\)）。有意水準を高くする，すなわち「差があると判断する上での基準をゆるく」してしまうと，誤った仮説を採用してしまう恐れが増えてしまう。 第2種の過誤を犯す確率を\\(\\beta\\)と表現する。\\(1 - \\beta\\)は検定力と呼ばれ，検定力とは「帰無仮説が偽であるときに，正しく帰無仮説を棄却する確率」のことをいう。つまり，差があるときに，”差がある”と正しく判断できる確率のことである。統計的仮説検定では，この検定力をいかに高く保つかが重要となる。 第1種の過誤と第2種の過誤はトレード・オフの関係にある。第1種の過誤を避けようとして有意水準を小さくすれば（例えば\\(\\alpha=0.001\\)とする）帰無仮説の棄却が厳しくなり，逆に第2種のエラーを犯してしまう確率も高くなる（帰無仮説が偽であるにもかかわらず，棄却しない）。 7.5 統計的検定が抱える問題 7.5.1 p値と標本数の関係 p値は標本数に依存する。標本数が多くなるほどp値は小さくなる。 再び，コイン投げの例に戻る。先程の例では，コインを10回投げて2回表が出たケースについて二項検定で評価をしたが，今度は100回コインを投げて20回表が出た場合についても二項検定をしてみよう。 どちらのケースも，表が出る割合は0.2で等しい。 binom.test()関数で，「フェアなコインの場合と比べてこのコインはゆがんでいるか」を検討してみよう。 binom.test(x = 2, n = 10, p = 0.5) #表が出る確率0.5のコインを10回(n=10)なげて，2回(x=2)表が出た ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 binom.test(x = 20, n = 100, p = 0.5)#表が出る確率0.5のコインを100回(n=100)なげて，20回(x=20)表が出た ## ## Exact binomial test ## ## data: 20 and 100 ## number of successes = 20, number of trials = 100, p-value = 1.116e-09 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.1266556 0.2918427 ## sample estimates: ## probability of success ## 0.2 100回コインを投げたケースについては，p値が1.115908910^{-9}と非常に小さい値となった。 二項分布\\(Binomial(n = 100, q = 0.5)\\)のグラフも確認してみよう。\\(x &lt; 20\\)が非常に小さい確率であることがグラフからもわかる。 plot = data.frame(x=0:100, p=dbinom(x=0:100, size=100, prob=0.5)) ggplot() + geom_bar(data=plot, aes(x=factor(x), y=p), stat=&quot;identity&quot;) + scale_x_discrete(breaks = seq(0,100,10)) + labs(y = &quot;P(x)&quot;, x =&quot;x&quot;) 「有意ではない（\\(p &gt; 0.05\\)）」というのは，「差がない」ということを意味しない。差自体は常に存在する（今回の実験結果0.2とフェアなコインの結果0.5の間には，0.3という差が存在する）。統計的仮説検定で評価するのは，その差が意味のある差かどうかである。 少ない標本数では，珍しい結果が生じることもありえる。10回投げた程度では，その差が意味のある差かどうかが，標本数が少なすぎて判断できない。 逆に，実質意味のない差であっても，標本数が多ければ統計的に有意な差（\\(p &lt; .05\\)）が得られてしまう。もっと極端に，10,000回コインを投げて，表が出た回数が4,900回だった場合（つまり表が出る割合は0.49）を考えてみよう。このコインがフェアなコインよりも歪んでいるかを検定してみると，有意（\\(p &lt; .05\\)）な結果が得られる。 n = 10000 x = 0.49 * n binom.test(x = x, n = n, p = 0.5) ## ## Exact binomial test ## ## data: x and n ## number of successes = 4900, number of trials = 10000, p-value = 0.04659 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4801563 0.4998496 ## sample estimates: ## probability of success ## 0.49 しかし，0.49と0.50の差，すなわち1%の違いを意味のある差と判断してよいのだろうか？ このように，統計的仮説検定には差の有無の評価が標本数に依存する欠点がある。実質意味のない大きさの差でも，場合によっては「有意な差がある」と結論付けられてしまうときもある。 統計的仮説検定で検討しているのは，差の大きさ（効果の大きさ）ではないということには注意が必要である。 7.5.2 多重比較の問題 統計的仮説検定を繰り返すほど，差がなくても差があると評価してしまう確率（つまり第1種の過誤を犯す確率）は増える。 例えば，5%水準で10回検定を行えば，少なくとも1回は帰無仮説を誤って棄却してしまう確率が0.4 になる。 1 - (1 - 0.05)^10 #全ての確率から，「10回検定を行って全て正しい判断を行う」確率を差し引いたものが，「少なくとも1回は誤った判断をしてしまう確率」 ## [1] 0.4012631 このように複数回検定を行うことを多重比較という。 これにより，例えば心理学の研究ならば以下のように「有意な結果」を恣意的に導くことも可能である。 たくさんの質問項目について個別に検定を行い，有意な結果だけを選んで議論する。 たくさん実験を行って，有意な結果だけを選んで報告する。 このように，「有意な結果」を導くトリックはp-haking（ピー・ハッキング）と表現されることがある。 確認問題 問１ 以下のプログラムを読み込む。 ある教授法に児童の学力向上の効果があるかを検討した。学校Bにはその教授法を実施し，学校Aには何もしなかった。その後，学校Aと学校Bそれぞれ10人の生徒に学力テストを行った。A，Bそれぞれが学校A，Bそれぞれの生徒の成績である（架空のデータである）。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(48, 40, 43, 56, 69, 53, 47, 41, 42, 91) Value = c(A, B) Treatment = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample = data.frame(Treatment = Treatment, Value = Value) str(sample) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Treatment: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Value : num 38 53 61 27 54 55 44 45 44 41 ... 学校Aと学校Bそれぞれについて，テストの得点の平均値及び標準偏差を求めて報告せよ。 この教授法に成績向上があったかどうかについてt検定（等分散を仮定しない）で検討し，結果について報告するとともに結論を述べよ。 ※t.test()関数を使う。等分散を仮定しない検定の場合は，特にオプションをしていしないでもよい。 "],
["07-analysis.html", "Chapter 8 様々な解析法 8.1 t検定 8.2 分散分析 8.3 ノンパラメトリック検定 8.4 まとめ", " Chapter 8 様々な解析法 この章では，t検定，χ二乗検定，分散分析，ノンパラメトリック検定など，心理学における基礎統計学で学んできた手法について，Rでの解析法をみながら内容についておさらいしていく。 データ分析をするときは，「データが量的変数で2つのグループ間での比較ならばt検定」，「データが量的変数で3つ以上のグループ間での比較ならば分散分析」といったように，更には「変数が正規分布に従わないときにはノンパラメトリック検定」といったように，様々な条件に応じて解析方法を使い分けてきたように思う。 このテキストでは，t検定や分散分析などの手法を，「統計モデル」という一つの枠組みで捉えることが目的である。 まず，準備としてtidyverseパッケージをロードしよう。 library(tidyverse) 8.1 t検定 「分析の対象が量的変数で，２つのグループの間でその変数の平均値を比較する」ときにはt検定を使うと学んだと思う。更に，t検定には2つのグループに対応があるかないかで，「対応のあるt検定」と「対応のないt検定」で区別される。 前の章でもみてきたように，Rにはt検定を行うためのt.test()関数が用意されている。 8.1.1 対応のないt検定 Rに標準で入っているsleepデータを使って，Rでt検定をやってみよう。 以下のプログラムを読み込み，サンプルデータを作る。 ttest_sample = sleep #ttest_sampleという名前保存する。 ttest_sample$ID = 1:nrow(ttest_sample) ttest_sample ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 11 ## 12 0.8 2 12 ## 13 1.1 2 13 ## 14 0.1 2 14 ## 15 -0.1 2 15 ## 16 4.4 2 16 ## 17 5.5 2 17 ## 18 1.6 2 18 ## 19 4.6 2 19 ## 20 3.4 2 20 IDは参加者を意味する番号で，1から20までの人がグループ1かグループ2のどれかに属し，変数extraを測定したとする。グループの間でextraに違いがあるかどうかを検討したい。 このように参加者が２つのグループのうちどれか一つに属しているケースが「対応のない場合」で，この場合は対応のないt検定で検討する。 前の章で見たように，t.test()関数で以下のように入力すれば結果が出力される。 t.test(data = ttest_sample, extra~group) ## ## Welch Two Sample t-test ## ## data: extra by group ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.75 2.33 8.1.2 対応のあるt検定 同じく，sleepデータを使って対応のある場合について解析をしてみる。 以下のプログラムを読み込み，サンプルデータを作る。 ttest_sample2 = sleep #ttest_sampleという名前保存する。 ttest_sample2 ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 今度は20名の参加者が，グループ1とグループ2の両方に属して，それぞれで変数extraを測定したとする。 このように同じ参加者が２つのグループのr兵法に属しているケースが「対応のある場合」である。 t.test()関数でオプションpaired = TRUEを入れれば，対応のあるt検定を実施できる。 t.test(data = ttest_sample2, extra~group, paired = TRUE) ## ## Paired t-test ## ## data: extra by group ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 8.2 分散分析 t検定で比較できるのは2つのグループの間の平均値である。3グループ以上の間で平均値の比較を行いたい場合は，分散分析（ANOVA）を行う。 ここでは，一要因３水準の分散分析（単純に3つのグループの間で平均値を比較する）を例として，Rでの解析法について説明する。 Rでは，一要因の分散分析をするための関数aov()が標準で入っている。同じくRで標準で入っているPlantGrowthデータを使って解析をしてみよう。 anova_sample = PlantGrowth #PlantGrowthをanova_sampleという名前で保存する anova_sample ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl ## 7 5.17 ctrl ## 8 4.53 ctrl ## 9 5.33 ctrl ## 10 5.14 ctrl ## 11 4.81 trt1 ## 12 4.17 trt1 ## 13 4.41 trt1 ## 14 3.59 trt1 ## 15 5.87 trt1 ## 16 3.83 trt1 ## 17 6.03 trt1 ## 18 4.89 trt1 ## 19 4.32 trt1 ## 20 4.69 trt1 ## 21 6.31 trt2 ## 22 5.12 trt2 ## 23 5.54 trt2 ## 24 5.50 trt2 ## 25 5.37 trt2 ## 26 5.29 trt2 ## 27 4.92 trt2 ## 28 6.15 trt2 ## 29 5.80 trt2 ## 30 5.26 trt2 植物の生長を3つの条件で調べたデータである。 まず，３つのグループごとに平均値や標準を確認しよう。 anova_sample %&gt;% dplyr::group_by(group) %&gt;% dplyr::summarise(Mean = mean(weight), SD = sd(weight), N = length(weight)) ## # A tibble: 3 x 4 ## group Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.03 0.583 10 ## 2 trt1 4.66 0.794 10 ## 3 trt2 5.53 0.443 10 ctrl，trt1，trt2の間で平均値に下がるかを一要因の分散分析で検討する。以下のようにプログラムを書く。 result = aov(data = anova_sample, weight ~ group) summary(result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary()を使うと分散分析表が出力され，変数の効果が有意か否かを検討できる。 一要因の分散分析では「グループの間で平均値に差がない」という帰無仮説を検討する。この例の分析についてはp値は0.02であり，5%水準で帰無仮説は棄却されることとなる。これは「グループの間で平均値に差がない」という可能性は棄却されたが，どのグループの間に有意な差があるかはわからない。そこで，グループ１とグループ２，グループ２とグループ３，グループ１とグループ３との間，計３つの組み合わせで平均値の比較を行う。つまり，t検定を3回行って条件間の比較をする。 前の章でも触れたように，検定を繰り返すことは第１種の過誤を犯す確率を高めてしまう。そこで，検定を行う回数に応じてp値を厳し目に見積もるペナルティを課すことで，第１種の過誤を犯す確率を低くする工夫がなされる。この工夫が，「多重比較補正」と呼ばれるものである。 多重比較の補正を行うときは，pairwise.t.test関数を使う。各群間の比較について，補正後のp値が出力される。 Rでは，多重比較補正を行うための関数として，pairwise.t.test()がある。 pairwise.t.test(anova_sample$weight, g = anova_sample$group) #gにグループを意味する変数，p.adjust.methodに補正方法を指定する。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: anova_sample$weight and anova_sample$group ## ## ctrl trt1 ## trt1 0.194 - ## trt2 0.175 0.013 ## ## P value adjustment method: holm グループの組み合わせごとに，補正後のp値が表示される。これを見ると，trt1とtrt2との間で5%水準で有意な差があることがわかる。 分散分析は選択肢も多く，非常に複雑な解析方法である。上に述べたように，条件が複数ある場合は要因の効果が有意だったら，多重比較補正をして条件間で差の検定がなされる。多重比較補正にもボンフェローニ（Bonferroni)，チューキー(Tukey)，ホルム(Holm)の方法と，様々な方法が提案されている。 二要因の分散分析，三要因の分散分析，更には二要因対応あり一要因対応なしの分散分析と，要因の数やそれぞれの要因の対応ありなしで分散分析のやり方も非常に複雑になる！ 更には，平方和の値の計算方法もタイプ1, タイプ2，タイプ3と様々な選択肢がある。 このように複雑な分散分析であるが，統計モデルという考え方を導入すれば，簡単に解析のデザインを組みやすくなる。詳しくは次章以降でみていく。 8.3 ノンパラメトリック検定 以上のt検定や分散分析は，分析の対象の変数が「正規分布に従う」を前提とする解析手法である。変数が正規分布に従わない変数，例えば質的変数（人数の比率，順次尺度など）の場合には，t検定や分散分析を用いるのは適切でなく，「ノンパラメトリック検定」を使うべきだと指摘されている。 ノンパラメトリック検定とは「変数が正規分布に従うという前提を置かない解析手法」の総称であり，様々な種類が提案されている。 以下では，ウィルコクソンの順位和検定，クラスカル・ウォリスの検定，カイ二乗検定について触れる。 8.3.1 ウィルコクソンの順位和検定 2群間で値を比較するノンパラメトリック検定である。データを順位データに変換し，2群間でデータの大きさを比較する検定である。 「マン・ホイットニーのU検定」という名前でも知られる。 Rではwilcox.test()が用意されている。 wilcox_sample = airquality %&gt;% filter(Month &gt;= 8) #Rに入っているサンプルデータairqualityから2群だけ取り出したデータで試してみる wilcox.test(data = wilcox_sample, Ozone ~ Month) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Ozone by Month ## W = 552, p-value = 0.003248 ## alternative hypothesis: true location shift is not equal to 0 8.3.2 クラスカル・ウォリスの検定 3群以上で値を比較するノンパラメトリック検定である。同じく，データを順位データに変換して比較を行う。 Rではkruskal.test()が用意されている。 kruskal_sample = airquality #Rに入っているサンプルデータairqualityで試してみる kruskal.test(data = kruskal_sample, Ozone ~ Month) ## ## Kruskal-Wallis rank sum test ## ## data: Ozone by Month ## Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06 8.3.3 カイ二乗検定 解析の対象の変数が質的変数で，頻度に偏りがあるかを比較したい場合は，カイ二乗検定が使われる。 Rには，カイ二乗検定を行うためのchisq.test()がある。 tab = matrix(c(12, 30, 25, 16), ncol=2) #表を作成する chisq.test(tab) #chisq.testの中に表を入れる ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 7.5549, df = 1, p-value = 0.005985 カイ二乗検定は比率に偏りがあるかを検定してくれるが，どこに偏りがあるかは研究者自身が表を見て判断するしかない。 8.4 まとめ 心理統計学で学んできた解析をRで行う方法について解説してきた。 これまで，心理統計学では「変数が連続型でグループが２つならばt検定」，「変数が連続型でグループが3つ以上ならば分散分析」，「変数がカテゴリカル変数ならばカイ二乗検定」と変数のタイプや研究デザインに応じて行う解析を選ぶということがなされてきた。 しかし，上に挙げた条件に当てはまらないデータの場合，当てはまる解析手法はあるだろうか？ 以降の章では，これまで学んできた統計手法を統計モデルという一つの枠組みで捉え直していく。 "],
["08-linear_model.html", "Chapter 9 線形モデル 9.1 準備 9.2 線形モデルの概要 9.3 線形モデルによる解析 9.4 最尤法 9.5 信頼区間と予測区間 9.6 まとめ 確認問題", " Chapter 9 線形モデル これまで学んできた様々な統計手法（t検定，分散分析など）を，「線形モデル」という一つの枠組みで捉えていく。 9.1 準備 いつもどおり，tidyverseパッケージをロードしよう。 library(tidyverse) 9.2 線形モデルの概要 まず，線形モデルの表現の仕方を理解する。以下の式は，変数\\(x\\)から，変数\\(y\\)を予測するプロセスを記述したものである。変数\\(x\\)は予測変数（predictor variable），変数\\(y\\)は応答変数（response variable）と呼ばれる。このように，応答変数と予測変数との関係を式で表現したものをモデルと呼ぶ。 予測変数は，「独立変数」や「説明変数」とも呼ばれる。応答変数は，「従属変数」や「被説明変数」とも呼ばれる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\tag{1}\\\\ y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] 1番目の式の右側に\\(\\alpha + \\beta x\\)という線形の式がある。この式は，線形予測子(linear predictor)と呼ばれる。変数\\(x\\)に係る\\(\\beta\\)は予測変数に係る傾き(slope)，\\(\\alpha\\)は切片(intercept)である。1番目の式は，変数\\(x\\)の持つ効果（傾き）及びそれ以外の効果（切片）と変数\\(y\\)の予測値（\\(\\hat{y}\\)）との関係を示している。 予測変数は2個以上でも構わない。予測変数の個数を\\(K\\)とすると，(1)の1番目の式は以下のように表現できる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x \\\\ \\tag{2} \\end{equation} \\] \\(y \\sim Normal(\\hat{y}, \\sigma)\\)は，「応答変数\\(y\\)が，予測値\\(\\hat{y}\\)を平均，\\(\\sigma\\)を標準偏差とする正規分布に従う」ことを示している。つまり，線形予測子から予測された値\\(\\hat{y}\\)と誤差\\(\\sigma\\)から，実際の値\\(y\\)が推定されるプロセスを表現している。 応答変数が正規分布に従うという前提をおいたモデルのことを，一般的に線形モデル(linear model)と呼ぶ。 線形モデルは，基本的に心理統計でも学んだ「回帰分析(regression analysis)」と同じである。 応答変数\\(y\\)を決定づける変数，\\(\\alpha\\), \\(\\beta\\)，及び \\(\\sigma\\)はパラメータ(parameter)と呼ばれる。このパラメータを，既知の変数である\\(x\\)と\\(y\\)から推定する。 9.2.1 まとめ まずは，「線形予測子」，「傾き」，「切片」といったキーワードを覚えておく。 線形モデルは，応答変数と予測変数の関係を線形の式で表したモデルである。 線形予測子の傾き，切片及び誤差（正規分布の分散パラメータ）を推定する。 予測変数が応答変数に及ぼす効果を推定することが，線形モデルの目的である。 9.3 線形モデルによる解析 実際に，Rで線形モデルの解析をしてみよう。 Rには線形モデルを扱える関数lm()がある。irisデータを使って解析をしてみよう。 Sepal.LengthとPetal.Lengthの関係を散布図で確認する。 p = ggplot() + geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) p Sepal.Lengthが大きいほどPetal.Lengthが大きいという関係（正の相関）がありそうである。そこで，Sepal.Lengthの大きさから，Petal.Lengthの大きさを予測することを試みる。 lm()関数に，「応答変数~予測変数」のかたちで入力する。以下のプログラムを実行してみよう。 result = lm(data = iris, Petal.Length ~ Sepal.Length) 結果をresultという名前でいったん保存した（名前はresult以外でも構わない）。summary()関数の中に，resultを入れて実行すると詳細な結果が出力される。 summary(result) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 色んな情報が出力されるが，まずは係数（Coefficients）の部分を見てみよう。ここでは，データから推定された切片や予測変数の傾きの結果が出力されている。 Interceptの部分が切片の推定結果である。各変数の名前の部分（ここではSepal.Length）が予測変数の傾きの推定結果を示している。 Estimateが推定された切片または傾きの値である。 Std.Errorは推定された係数の標準誤差である。 予測変数が応答変数に対して影響力を持っているか？ それは傾きの係数の推定結果からわかる。係数の値は，予測変数が1単位増えたら応答変数がどう変化するかを意味している。 係数がプラスならば，予測変数の値が増えると応答変数の値が増加する関係にあることを意味する。 係数がマイナスならば，予測変数の値が増えると応答変数が減少する関係にあることを意味する。 t value及びPrは係数の有意性検定の結果を示している（それぞれt値，p値）。ここでは，「係数がゼロである」という帰無仮説を検定している。p値が極端に低い場合は，「求めた係数の値は有意にゼロから離れている」と結論付けることができる。 Sepal.LenghtとPetal.Lengthの散布図に，線形モデルから推定された切片と傾きの値を持つ以下の直線を引いてみよう。 \\[ \\begin{equation} Petal.Length = -7.10 + 1.86 Sepal.Length\\\\ \\tag{3} \\end{equation} \\] p = ggplot() + geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + geom_smooth(data = iris, aes(x = Sepal.Length, y = Petal.Length), formula = y~x, method = &quot;lm&quot;, se = FALSE) p 線形モデルでは，直線の式で予測変数と応答変数の関係を表現する。実際のデータ（散布図の点）とのズレが最小になるような，直線の式を推定する（予測値と実測値とのズレが最小になるのが，最も良い予測である）。 9.4 最尤法 では，どうやって実測値と予測値とのずれが最小になるように傾きと切片の値を求めるのか？ 線形モデル（及び一般化線形モデル）では，パラメータの推定に最尤法（maximum likelihood method）という最適化手法が用いられる。 9.4.1 最尤法によるパラメータ推定 ここにコインが1枚ある。コインの表が出るかを決定づけるパラメータ（つまりコインを投げて表が出る確率）を\\(\\theta\\)（シータ）とする。この\\(\\theta\\)の値を何回かコインを投げる実験を通して推定する。 1回目は，表が出た。この時点で，この実験結果が生じる確率は\\(\\theta\\)である。 2回目は，裏が出た。1回目と2回目までの実験結果が生じる確率は\\(\\theta(1-\\theta)\\)である。 3回目は，表が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)\\theta\\)である。 その後，4回目は裏，5回目は裏だったとする。5回目で実験をストップすることにする。この実験結果が生じる確率\\(L\\)は以下のように表すことができる。 \\[ L = (1-\\theta)^3 \\theta^2 \\tag{4} \\] \\(L\\)のことを尤度(likelihood)と呼ぶ（”ゆうど”と読む）。 尤度とは「もっともらしさ」を示す概念である。イメージとしては，「今回の観測結果が得られる確率」である。今回の観測データに対して最も当てはまりが良くなる，すなわち尤度が最も高くなるときのパラメータを求めるのが，最尤法 (maximum likelihood method)と呼ばれる手法である。 掛け算を扱う尤度は計算が困難なので，実際のパラメータ推定の際には対数化して足し算を扱う。対数化した尤度を対数尤度(log-likelihood)と呼ぶ。対数尤度が最大となるパラメータ\\(\\theta\\)を求める。 \\[ \\log L = \\log(1-\\theta)+\\log(1-\\theta)+\\log(1-\\theta)+\\log(\\theta)+\\log(\\theta) \\tag{5} \\] 以下のプログラムで，上のコイン投げの例で最も対数尤度が高くなるときの\\(\\theta\\)を求めている。maximumが対数尤度が最も高くなるパラメータの値で，objectiveがそのときの対数尤度である。（プログラムの意味については理解しなくて良い） D = c(1, 0, 1, 0, 0) #観測データのベクトル：1=表，0=裏とする #対数尤度を求める関数 LogLikelihood = function(x){ return(function(theta){ L = 1 for(i in 1:length(x)){ L = L * theta^(x[i]) * (1-theta)^(1-x[i]) } return(log(L)) }) } #optimize関数で，対数尤度が最も高くなるパラメータthetaを推定する result_mlm = optimize(f = LogLikelihood(D), c(0, 1), maximum=TRUE) result_mlm ## $maximum ## [1] 0.4000015 ## ## $objective ## [1] -3.365058 パラメータ\\(\\theta\\)と対数尤度\\(\\log L\\)との関係を以下に示す。 #図示する theta = seq(0.01,0.99,0.01) logL = log(theta)+log(1-theta)+log(theta)+log(1-theta) + log(1-theta) data_mlm = data.frame(x=theta, y=logL) ggplot() + geom_line(data=data_mlm, aes(x=x, y= y), size=1) + geom_vline(xintercept = result_mlm$maximum, linetype=&quot;dashed&quot;, colour=&quot;red&quot;, size=1) + labs(x =&quot;theta&quot;, y= &quot;log L&quot;) 対数尤度が最も大きくなるのは，\\(\\theta=\\) 0.4のときである（表が出た割合である2/5と一致）。 ここではわかりやすくパラメータを1つのみ使って説明しているが，線形モデルの傾きや切片の推定も同じである。上の例の\\(\\theta\\)を線形予測子に置き換えて同様の計算をする。 線形モデルのときのパラメータ推定には，最小二乗法と呼ばれる別の推定方法もある。ただし，最小二乗法を使っても最尤法を使っても，線形モデルの傾きや切片の推定結果は全く同じになる。 9.5 信頼区間と予測区間 モデルでパラメータの推定を行ったあとは，そのモデルがデータを上手く予測できているかを確認することも重要である。 具体的には，パラメータの信頼区間(confidence interval)とデータの予測区間 (predictive interval)をチェックする。 信頼区間とは，パラメータが分布する区間のことをいう。今回得られた標本を用いて係数を推定したが，標本の元となる母集団の係数の値はどのくらいか？その母集団の係数の予想の範囲が，信頼区間である。 予測区間とは，標本がどの範囲に分布するかを予測する範囲のことをいう。新たな標本を取ったときに，そのデータがどの範囲に分布するか。その予想の範囲が予測区間である。 Rには，線形モデルの推定結果から信頼区間と予測区間を算出してくれるpredict()関数が用意されている。先ほどの線形モデルの解析結果を使って，信頼区間と予測区間を求めてみよう。 9.5.1 信頼区間 result = lm(data = iris, Petal.Length ~ Sepal.Length) result_conf = predict(result, interval = &quot;confidence&quot;, level = 0.95) interval = &quot;confidence&quot;とすると，信頼区間を求めてくれる。 level = に信頼区間の幅を入力する（デフォルトで0.95だが，幅を変えたい場合は指定する）。 head(result_conf) ## fit lwr upr ## 1 2.376565 2.188121 2.565009 ## 2 2.004878 1.792226 2.217531 ## 3 1.633192 1.393955 1.872428 ## 4 1.447348 1.194160 1.700536 ## 5 2.190722 1.990526 2.390917 ## 6 2.934095 2.775149 3.093040 uprが95%信頼区間の上限，lwrが95%信頼区間の下限に当たる。 求めた信頼区間を図示してみよう plot_conf = cbind(iris, result_conf) #実測値のデータと予測値のデータを結合する。 ggplot() + geom_point(data = plot_conf, aes(x = Sepal.Length, y = Petal.Length)) + geom_line(data = plot_conf, aes(x = Sepal.Length, y = fit)) + geom_ribbon(data = plot_conf, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 9.5.2 予測区間 result = lm(data = iris, Petal.Length ~ Sepal.Length) new = data.frame(Sepal.Length = seq(4, 8, 0.1)) #0.1刻みで4から8まで範囲の数値ベクトルを入れたデータを仮に作る result_pred = predict(result, newdata = new, interval = &quot;prediction&quot;, level = 0.95) #newdataに先ほど作成した仮のデータを入れる。 head(result_pred) #仮データの数値に対応する予測区間が求められる ## fit lwr upr ## 1 0.3322885 -1.4165179 2.081095 ## 2 0.5181318 -1.2277203 2.263984 ## 3 0.7039751 -1.0390829 2.447033 ## 4 0.8898184 -0.8506063 2.630243 ## 5 1.0756617 -0.6622915 2.813615 ## 6 1.2615050 -0.4741389 2.997149 interval = &quot;prediction&quot;と入力する。 予測区間を図示してみよう。 plot_pred = data.frame(Sepal.Length = seq(4, 8, 0.1), result_pred) #予測区間のデータを作成する ggplot() + geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + geom_line(data = plot_pred, aes(x = Sepal.Length, y = fit)) + geom_ribbon(data = plot_pred, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 実際のデータが予測区間の範囲に収まっているならば，そのモデルは概ねよくデータを予測できていることを示している。 9.6 まとめ この章では，線形モデルの概念について学んできた。次章では，線形モデルを扱う上で注意すべき点について見ていく。 確認問題 Rに入っているサンプルデータtreesを使って，線形モデルの結果の解釈の仕方とlm()関数の扱い方を復習をする。 head(trees) ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 問１ Heightを応答変数，Girthを予測変数として，切片と傾きの推定値を求めよ。 問２ Heightを応答変数，Volumeを予測変数として，切片と傾きの推定値を求めよ。 問３ 問2の推定結果から，Volumeが1単位増えるとHeightがどう変化するかを説明せよ。 "],
["09-linear_model2.html", "Chapter 10 線形モデルの注意点 10.1 準備 10.2 予測変数がカテゴリカル変数の場合 10.3 グループが複数ある場合 10.4 予測変数が複数ある場合 10.5 線形モデルが抱える問題 10.6 モデルの予測力の評価 確認問題", " Chapter 10 線形モデルの注意点 前の章で，線形モデルの全体像を見てきた。 次に，線形モデルを扱う上で注意すべき点について見ていく。 10.1 準備 いつもどおり，tidyverseパッケージをロードしよう。 library(tidyverse) 10.2 予測変数がカテゴリカル変数の場合 前の章では，予測変数が量的変数の場合を例として扱ったが，予測変数はカテゴリカル変数（質的変数）でも構わない。ただし，予測変数がカテゴリカル変数の場合は，予測変数を0か1のどちらかの値を取るダミー変数(dummy variable)に変換する必要がある。 Rに入っているsleepデータを少し変えたもの使って，カテゴリカル変数を使って線形モデルの解析をしてみよう。 dat = sleep #データを別の名前datに保存し直す #変数の名前を変える dat$x = ifelse(dat$group == 1, &quot;control&quot;, &quot;treatment&quot;) dat$y = dat$extra dat = dat %&gt;% dplyr::select(y, x) str(dat) #datの構成を確認する ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ y: num 0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0 2 ... ## $ x: chr &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ... xはグループを意味する変数で，カテゴリカル変数である（統制群controlもしくは実験群treatment）。まずこれを，「treatmentなら1，controlなら0」とする新たな変数x_1を作る。 dat$x_1 = ifelse(dat$x == &quot;treatment&quot;, 1, 0) str(dat) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ y : num 0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0 2 ... ## $ x : chr &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ... ## $ x_1: num 0 0 0 0 0 0 0 0 0 0 ... ifelse()関数は，ifelse(XXX, A, B)と表記することで，「XXXの条件に当てはまればA，当てはまらなければB」という処理をしてくれる。ここでは，予測変数のベクトルxについて，treatmentならば1, それ以外なら0に変換し，0か1を取る変数\\(x_{1}\\)を新たに作った。 この\\(x_{1}\\)がダミー変数である。 解析に用いるモデルを確認すると，以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x_{1} \\\\ \\tag{1}\\\\ y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] \\(x_{1}\\)は0か1のどちらかを取る変数で，\\(x_{1} = 0\\)のとき，つまり統制群のとき，応答変数の予測値は\\(\\hat{y} = \\alpha\\)となる。\\(x_{1} = 1\\)のとき，つまり実験群のとき，応答変数の予測値は\\(\\hat{y} = \\alpha + \\beta\\)となる。すなわち，切片\\(\\alpha\\)は統制群のときの効果，傾き\\(\\beta\\)は実験群の時に加わる実験群特有の効果を意味する。 lm()を使って，上のモデル式のパラメータの推定をしよう。 result = lm(data = dat, y ~ x_1) summary(result) ## ## Call: ## lm(formula = y ~ x_1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.430 -1.305 -0.580 1.455 3.170 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7500 0.6004 1.249 0.2276 ## x_1 1.5800 0.8491 1.861 0.0792 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.899 on 18 degrees of freedom ## Multiple R-squared: 0.1613, Adjusted R-squared: 0.1147 ## F-statistic: 3.463 on 1 and 18 DF, p-value: 0.07919 2つの群間で平均値を比較するときにはt検定がよく使われる。t.test()関数を使って\\(x_{1}=0\\)と\\(x_{1}=1\\)との間で\\(y\\)の値の平均値を比較したときのt値及びp値の結果が，lm()の傾きのt値及びp値と一致することを確認しよう。 t.test(data = dat, y ~ x_1) ## ## Welch Two Sample t-test ## ## data: y by x_1 ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.75 2.33 lm()の傾きの検定は，「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは，予測変数\\(x_{1}\\)が1単位増えたときの応答変数\\(y\\)の変化量であった。傾きの検定は，「\\(x_{1}=0\\) から \\(x_{1}=1\\) に変化することによって， \\(y\\) が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は，「\\(x_{1}=0\\)と\\(x_{1}=1\\)の間で\\(y\\)の値に差があるか」を検定しているのと論理的に同じである。 このように，予測変数が1つで，予測変数が二値（0もしくは1）であるときの線形モデルは，t検定に対応する。 10.3 グループが複数ある場合 先ほどの例は，統制群と実験群の二つのグループの場合であった。例えば実験で統制群，実験群1，実験群2といったように三つ以上のグループを設定した場合は，どうダミー変数を作成すればよいのか？ Rに入っているPlantGrowthを例として見ていこう。例えばやり方としては，以下の方法がある。 dat = PlantGrowth dat$y = dat$weight #名前をyに変える dat$x_c = ifelse(dat$group == &quot;ctrl&quot;, 1, 0) dat$x_t1 = ifelse(dat$group == &quot;trt1&quot;, 1, 0) dat$x_t2 = ifelse(dat$group == &quot;trt2&quot;, 1, 0) str(dat) ## &#39;data.frame&#39;: 30 obs. of 6 variables: ## $ weight: num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... ## $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ y : num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... ## $ x_c : num 1 1 1 1 1 1 1 1 1 1 ... ## $ x_t1 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ x_t2 : num 0 0 0 0 0 0 0 0 0 0 ... head(dat) ## weight group y x_c x_t1 x_t2 ## 1 4.17 ctrl 4.17 1 0 0 ## 2 5.58 ctrl 5.58 1 0 0 ## 3 5.18 ctrl 5.18 1 0 0 ## 4 6.11 ctrl 6.11 1 0 0 ## 5 4.50 ctrl 4.50 1 0 0 ## 6 4.61 ctrl 4.61 1 0 0 3種類のダミー変数を作った。それぞれ，x_cは「ctrlならば1，それ以外なら0」，x_t1は「trt1ならば1，それ以外なら0」，x_t2は「trt2ならば1，それ以外なら0」となっている。これら3つのダミー変数を使ってモデルを作り，パラメータを推定する。 \\[ \\begin{equation} \\hat{y} = \\beta_{c} x_{c} + \\beta_{t1} x_{t1} + \\beta_{t2} x_{t2} \\\\ \\tag{2} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] ここで注意が必要なのは，今回のモデルでは切片\\(\\alpha\\)が省かれていることである。その理由は後ほど説明する。 モデルをlm()で記述して，推定してみよう。以下のプログラムを実行する。lm(data = dat, y ~ x_c + x_t1 + x_t2 - 1)の中に-1が加わっている点に注意。これは「モデルから切片を除け」という命令である。 result = lm(data = dat, y ~ x_c + x_t1 + x_t2 - 1) summary(result) ## ## Call: ## lm(formula = y ~ x_c + x_t1 + x_t2 - 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_c 5.0320 0.1971 25.53 &lt;2e-16 *** ## x_t1 4.6610 0.1971 23.64 &lt;2e-16 *** ## x_t2 5.5260 0.1971 28.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.9867, Adjusted R-squared: 0.9852 ## F-statistic: 665.5 on 3 and 27 DF, p-value: &lt; 2.2e-16 それぞれのダミー変数に係る傾きの係数，すなわち式(2)における\\(\\beta_{c}\\)，\\(\\beta_{t1}\\)，\\(\\beta_{t2}\\)の推定結果が出力される。それぞれ，ctrl，trt1, trt2における応答変数(y)の推定値を意味している。 10.3.1 変数の中心化 上記の例で出力される係数の推定値\\(\\beta_{c}\\)，\\(\\beta_{t1}\\)，\\(\\beta_{t2}\\)は，各条件の平均値と一致している。 dat %&gt;% group_by(group) %&gt;% summarise(M = mean(y)) ## # A tibble: 3 x 2 ## group M ## &lt;fct&gt; &lt;dbl&gt; ## 1 ctrl 5.03 ## 2 trt1 4.66 ## 3 trt2 5.53 つまり，このモデルの場合，係数が意味することは各条件での応答変数の推定値（平均）であって，その条件の効果の強さを反映しているわけではない。各係数の有意性検定の結果を見るとp値が非常に低く「有意」であるが，これらの結果は何の意味も持たない。係数の有意性検定は係数がゼロから有意に離れているかを検定していて，今回のデータならば応答変数（植物の重量）は正の値を取りうるので，「0より有意に大きい」という結果はある意味当然である。 その条件の効果の強さ（例えば実験条件は植物の生長を促す効果があるのかなど）を係数から直接解釈したいのならば，モデルを組み直す必要がある。 例えば，応答変数から応答変数の平均値を引く中心化(centering)という処理を事前に行う。 dat$y_2 = dat$y - mean(dat$y) #yからyの平均値を引いた新たな変数y_2を作る summary(lm(data = dat, y_2 ~ x_c + x_t1 + x_t2 - 1)) #y_2を応答変数として解析する ## ## Call: ## lm(formula = y_2 ~ x_c + x_t1 + x_t2 - 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_c -0.0410 0.1971 -0.208 0.8368 ## x_t1 -0.4120 0.1971 -2.090 0.0462 * ## x_t2 0.4530 0.1971 2.298 0.0295 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.1824 ## F-statistic: 3.231 on 3 and 27 DF, p-value: 0.03793 係数の推定結果及び有意性検定の結果が変わった。今度は，x_t1の係数が負でp値も\\(p&lt;.05\\)に，x_t2の係数は正でp値も\\(p&lt;.05\\)となった。これらが意味していることは，「x_t1 = 1のときに，y_2の値は有意に-0.41下がる」と「x_t2 = 1のときに，y_2の値は有意に0.45上がる」ということである。言い換えれば，「実験条件1では平均よりも植物の重量の値が低く」，「実験条件2では平均よりも植物の重量の値が高い」傾向にあることを示している。 図でも条件別にy_2の分布を確認してみよう。分布を見ても同様の傾向があるが，線形モデルの解析の結果その効果が有意であることが確認できた。 ggplot() + geom_boxplot(data = dat, aes(x = group, y = y_2)) このように，係数が持つ意味を直感的に理解しやすくするために，ここでは応答変数を変換した。モデルによっては，応答変数だけではなく，予測変数も中心化する必要がある。 10.3.2 基準となるグループと比較する もう一つの方法としては，グループの数が\\(K\\)個あるのならば，基準となるグループを定めてダミー変数を\\(K-1\\)個作る方法である。 以下のプログラムを実行して，データを作り直そう。 dat = PlantGrowth dat$y = dat$weight #名前をyに変える dat$x_t1 = ifelse(dat$group == &quot;trt1&quot;, 1, 0) dat$x_t2 = ifelse(dat$group == &quot;trt2&quot;, 1, 0) str(dat) ## &#39;data.frame&#39;: 30 obs. of 5 variables: ## $ weight: num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... ## $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ y : num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... ## $ x_t1 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ x_t2 : num 0 0 0 0 0 0 0 0 0 0 ... head(dat) ## weight group y x_t1 x_t2 ## 1 4.17 ctrl 4.17 0 0 ## 2 5.58 ctrl 5.58 0 0 ## 3 5.18 ctrl 5.18 0 0 ## 4 6.11 ctrl 6.11 0 0 ## 5 4.50 ctrl 4.50 0 0 ## 6 4.61 ctrl 4.61 0 0 今度は，ダミー変数は2つで各条件を表している。ctrlのときは「x_t1 = 0, x_t2 = 0」,trt1のときは「x_t1 = 1, x_t2 = 0」,trt2のときは「x_t1 = 0, x_t2 = 1」となる。 これら2つのダミー変数を予測変数として，lm()でyを推定しよう。ただし，今度は切片\\(\\alpha\\)を入れたモデルで推定する。モデルは以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{t1} x_{t1} + \\beta_{t2} x_{t2} \\\\ \\tag{3} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] summary(lm(data = dat, y ~ x_t1 + x_t2 + 1)) #1は省略しても可（デフォルトで切片を加えた結果を出力してくれる） ## ## Call: ## lm(formula = y ~ x_t1 + x_t2 + 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0710 -0.4180 -0.0060 0.2627 1.3690 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.0320 0.1971 25.527 &lt;2e-16 *** ## x_t1 -0.3710 0.2788 -1.331 0.1944 ## x_t2 0.4940 0.2788 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6234 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 式（3）より，切片の推定値は\\(x_{t1}=0\\)かつ\\(x_{t2}=0\\)のときの\\(\\hat{y}\\)，つまり統制群(ctrl)のときの応答変数\\(y\\)の推定値を意味している。各ダミー変数の係数（傾き）は，切片に加わる各条件の効果を意味している。例えば，x_t2の係数は0.49であるが，これは\\(x_{t2}=1\\)のとき（つまりtrt2のとき）の応答変数の予測値は， 5.03 + 0.49 = 5.52となることを示している。 このように，グループが\\(K\\)個ある場合（\\(K &gt; 2\\)），\\(K-1\\)個のダミー変数を作って推定する方法もある。係数の意味することは，基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。 このように，モデルを組み直すことにより，係数が意味することも変化してくる。モデル（式）を意識しながら，係数の意味を解釈することを意識しよう。 10.4 予測変数が複数ある場合 前の章でも述べたように，予測変数は2つ以上入れても良い。予測変数が複数ある場合の注意点を見ていく。 10.4.1 変数の効果の統制 予測変数を複数加えた線形モデルの解析のメリットは，ある予測変数について他の予測変数の効果を統制(control)したときの効果を検討できることにある。 Rで標準で入っているattitudeデータを使って，予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。 head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 以下のように，complaints, privileges, learning, raisesの4つを予測変数として，ratingの値の推定を行ってみよう。 result = lm(data = attitude, rating ~ complaints + privileges + learning + raises) summary(result) ## ## Call: ## lm(formula = rating ~ complaints + privileges + learning + raises, ## data = attitude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.2663 -5.3960 0.5988 5.8000 11.2370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.83354 8.53511 1.386 0.178 ## complaints 0.69115 0.14565 4.745 7.21e-05 *** ## privileges -0.10289 0.13189 -0.780 0.443 ## learning 0.24633 0.15435 1.596 0.123 ## raises -0.02551 0.18388 -0.139 0.891 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.996 on 25 degrees of freedom ## Multiple R-squared: 0.7152, Adjusted R-squared: 0.6697 ## F-statistic: 15.7 on 4 and 25 DF, p-value: 1.509e-06 切片（Intercept）は全ての予測変数の値がゼロのときの応答変数の予測値であり，各予測変数の係数は予測変数が1単位増えた場合の応答変数の変化量を意味している。例えば，complaintsの係数は0.69であるが，これは「complaintsが1増えるとratingは0.69増える傾向にある」ことを意味している。 各係数の値は「他の変数の値がゼロであるときの効果」を意味している。先程のcomplaintsの係数0.69は，その他の予測変数privileges, learning, raisesがゼロのときの，complaintsがratingに与えるそのものの効果を示している。 このように複数の予測変数を入れたモデルで推定される係数は，他の予測変数の効果を統制した上での予測変数が応答変数に及ぼす効果を意味する。 10.4.2 交互作用 以下のプログラムを実行して，サンプルデータdを作ろう。 set.seed(1) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.1 + 0.4 * x y = rnorm(n = 20, mean = mu, sd = 1) d_M = data.frame(x = x, y = y, gender = &quot;M&quot;) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.3 + -0.6 * x y = rnorm(n = 20, mean = mu, sd = 1) d_F = data.frame(x = x, y = y, gender = &quot;F&quot;) d = rbind(d_M, d_F) str(d) ## &#39;data.frame&#39;: 40 obs. of 3 variables: ## $ x : num 3 4 6 9 3 9 10 7 7 2 ... ## $ y : num 2.81 2.09 1.88 1.49 2.42 ... ## $ gender: chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... このデータdには，x, y, genderの3つの変数が含まれている。genderは性別を意味する変数としよう。M（男性）かF（女性）のいずれかである。男女別に，実験で2つの変数を測定したとしよう。 応答変数をy，予測変数をxとして線形モデルで切片及びxの傾きのパラメータを推定する。モデルは以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\\\ \\tag{4} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] lm()関数を使って推定しよう（\\(x\\)と\\(y\\)の散布図及び係数の信頼区間も図示する）。 result = lm(data = d, y ~ x) summary(result) ## ## Call: ## lm(formula = y ~ x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8209 -2.5577 -0.7021 2.4363 5.1560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7389 1.3472 0.549 0.587 ## x -0.1811 0.2060 -0.879 0.385 ## ## Residual standard error: 3.231 on 38 degrees of freedom ## Multiple R-squared: 0.01993, Adjusted R-squared: -0.005863 ## F-statistic: 0.7727 on 1 and 38 DF, p-value: 0.3849 newdat = data.frame(x = seq(1,10,0.1)) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(x = seq(1,10,0.1), result_conf) ggplot() + geom_point(data = d, aes(x = x, y = y), size = 3) + geom_line(data = plot_conf, aes(x = x, y = fit)) + geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 予測変数xの傾きはほぼフラットで，yに対してあまり効果がないようにみえる。 しかし，このデータdにはもう一つ性別を意味するgenderという変数が含まれていた。genderを区別して，またxとyの散布図を見てみよう。 ggplot() + geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 性別が女性（F）か男性（M）かで，xとyの関係が違うようである。 このように，別の変数との組み合わせにより，変数間の関係が変化することを交互作用(interaction)という。このデータでも，応答変数yに対して性別genderとxの交互作用がありそうである。 交互作用のあるモデルは，以下のように表現する。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{1} x + \\beta_{2} M + \\beta_{3} xM \\\\ \\tag{5} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] \\(M\\)は性別genderのダミー変数で，M（男性）ならば1，F（女性）ならば0の変数とする。 線形モデルでは，交互作用は予測変数同士の積で扱う。男性（M=1）の場合のyの推定値は，\\(\\alpha +(\\beta_{1} + \\beta_{3}) x +\\beta_{2}\\)となる。一方，女性（M=0）の場合は，$+{1} x \\(となる。\\){3}\\(は，男性のときの\\)x$に係る傾きの変化量を意味することになる。このように，交互作用を考慮する予測変数の積をモデルに加えることで，男性か女性かで切片及び傾きが変化することを表現できる。 d$M = ifelse(d$gender == &quot;M&quot;, 1, 0) #genderがMならば1, Fならば1のダミー変数を作る result = lm(data = d, y ~ x*M) summary(result) ## ## Call: ## lm(formula = y ~ x * M, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3555 -0.6534 0.2205 0.5636 1.6618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05079 0.53625 0.095 0.925 ## x -0.53691 0.08107 -6.622 1.03e-07 *** ## M 1.04827 0.73745 1.421 0.164 ## x:M 0.77868 0.11274 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8827 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 2つの予測変数の積の傾き（\\(\\beta_{3}\\)）は，x:Mである。p値も小さく，有意な効果を持っているようである。 ここで注意が必要なのは，交互作用を含む線形モデルの係数は解釈が複雑になることである。 男性(M = 1)の予測値は，線形モデルの式に推定された傾きと切片及び\\(M=1\\)を代入して，(0.05 + 1.05) + (-0.54 + 0.78) \\(x\\) となる。女性(M = 0)の場合は，0.05 -0.54 \\(x\\) となる。 xとMの傾きの推定値は，xやMそのものの効果（いわゆる主効果）を反映しなくなる。 交互作用効果が見られた場合は，解釈は慎重に行う必要がある。 サンプルデータについて，推定されたパラメータを元に，男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。 new_x = seq(1,10,0.1) newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x)))) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(newdat, result_conf) plot_conf$gender = ifelse(plot_conf$M == 1, &quot;M&quot;, &quot;F&quot;) ggplot() + geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 10.5 線形モデルが抱える問題 予測変数を増やすと，他の予測変数を統制することによって，その予測変数が応答変数に及ぼすそのものの効果を検討することができる。ただし，予測変数を加えることで生じる問題もある。以降では，多重共線性と過学習の問題について触れる。 10.5.1 多重共線性 予測変数同士が非常に強く相関しあっている場合，予測変数の係数の推定結果が信頼できなくなる恐れがある。この問題は，多重共線性(multicollinearity)と呼ばれる。 サンプルデータを使って確認してみよう。Rには多重共線性の例としてlongleyというサンプルデータがある。 head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 まず，このデータに入っている変数間の相関を確認してみよう。 cor(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population ## GNP.deflator 1.0000000 0.9915892 0.6206334 0.4647442 0.9791634 ## GNP 0.9915892 1.0000000 0.6042609 0.4464368 0.9910901 ## Unemployed 0.6206334 0.6042609 1.0000000 -0.1774206 0.6865515 ## Armed.Forces 0.4647442 0.4464368 -0.1774206 1.0000000 0.3644163 ## Population 0.9791634 0.9910901 0.6865515 0.3644163 1.0000000 ## Year 0.9911492 0.9952735 0.6682566 0.4172451 0.9939528 ## Employed 0.9708985 0.9835516 0.5024981 0.4573074 0.9603906 ## Year Employed ## GNP.deflator 0.9911492 0.9708985 ## GNP 0.9952735 0.9835516 ## Unemployed 0.6682566 0.5024981 ## Armed.Forces 0.4172451 0.4573074 ## Population 0.9939528 0.9603906 ## Year 1.0000000 0.9713295 ## Employed 0.9713295 1.0000000 Employed，GNP.deflatorを予測変数としたモデル（model01）と，Employedを応答変数，GNPを予測変数としたモデル（model02）でそれぞれ解析してみよう。 model01 = lm(data = longley, Employed ~ GNP.deflator) summary(model01) ## ## Call: ## lm(formula = Employed ~ GNP.deflator, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.68522 -0.44820 -0.07106 0.57166 1.61777 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.18917 2.12919 15.59 3.06e-10 *** ## GNP.deflator 0.31597 0.02083 15.17 4.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8706 on 14 degrees of freedom ## Multiple R-squared: 0.9426, Adjusted R-squared: 0.9385 ## F-statistic: 230.1 on 1 and 14 DF, p-value: 4.389e-10 model02 = lm(data = longley, Employed ~ GNP) summary(model02) ## ## Call: ## lm(formula = Employed ~ GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77958 -0.55440 -0.00944 0.34361 1.44594 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.843590 0.681372 76.09 &lt; 2e-16 *** ## GNP 0.034752 0.001706 20.37 8.36e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6566 on 14 degrees of freedom ## Multiple R-squared: 0.9674, Adjusted R-squared: 0.965 ## F-statistic: 415.1 on 1 and 14 DF, p-value: 8.363e-12 次に，Employedを応答変数，GNPとGNP.deflatorの両方を予測変数として入れて解析をしてみよう。 model03 = lm(data = longley, Employed ~ GNP.deflator + GNP) summary(model03) ## ## Call: ## lm(formula = Employed ~ GNP.deflator + GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.81315 -0.54330 0.05572 0.27894 1.40590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.94504 7.44945 7.644 3.67e-06 *** ## GNP.deflator -0.08511 0.12374 -0.688 0.5037 ## GNP 0.04391 0.01343 3.269 0.0061 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6693 on 13 degrees of freedom ## Multiple R-squared: 0.9685, Adjusted R-squared: 0.9637 ## F-statistic: 200 on 2 and 13 DF, p-value: 1.727e-10 それぞれの予測変数の係数を見てみると，一つずつ予測変数として入れたときと比べて値が変わっており，p値も低くなっている。 GNPとGNP.deflator同士は相関係数0.99とかなり強く相関している。このように，強く相関し合う変数を入れると係数の効果について信頼できる結果が得られなくなってしまう。 なぜ強く相関しあっている変数を入れるとまずいのか？モデルから考えてみよう。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{1} x_{1} + \\beta_{2} x_{2} \\\\ \\tag{6} \\end{equation} \\] 2つの予測変数\\(x_{1}\\)と\\(x_{2}\\)が強く相関している場合，つまり\\(x_{1}=x_{2}\\)だとすると，式(6)は以下のように置き換えることができる。 \\[ \\begin{equation} \\hat{y} = \\alpha + (\\beta_{1} + \\beta_{2}) x_{1} \\\\ \\tag{7} \\end{equation} \\] \\((\\beta_{1} + \\beta_{2})\\)について，パラメータ\\(\\beta_{1}\\)と\\(\\beta_{2}\\)の組み合わせは無限に考えられる。このように，強く相関する予測変数を入れると２つの予測変数のパラメータについて推定することが難しくなってしまう（パラメータの信頼区間が大きくなってしまう）。 多重共線性への対処 多重共線性の対策として，VIF(variance inflation factor)という指標がよく用いられる。一般的に，\\(VIF &gt; 10\\)の場合は，多重共線性を疑った方が良いといわれている。VIFの高い変数同士のうちどちらか一方を予測変数から除くといった対処をして，解析し直してみるのが良い。 carパッケージのvif()関数を使えば，VIFを算出することができる。 library(car) vif(model03) ## GNP.deflator GNP ## 59.69828 59.69828 10.5.2 過学習 以下のプログラムを実行して，サンプルデータdを作成しよう。 set.seed(10) N = 10 x = seq(1,N,1) y = runif(N, min = 1, max = 5) d = data.frame(x = x, y = y) str(d) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ x: num 1 2 3 4 5 6 7 8 9 10 ## $ y: num 3.03 2.23 2.71 3.77 1.34 ... ggplot() + geom_point(data =d, aes(x=x, y = y)) このデータについて，以下の線形モデルを当てはめ，パラメータを推定しよう。図に線形モデルの直線及び信頼区間を図示するところまでやってみる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{1} x\\\\ \\tag{8} \\end{equation} \\] result_1 = lm(data = d, y ~ x) newdat = data.frame(x = x) conf.int_1 = predict(result_1, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_1) ggplot() + geom_point(data = conf_dat, aes(x = x, y = y))+ geom_line(data = conf_dat, aes(x = x, y = fit)) + geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 直線はほとんど観測値から外れており，当てはまりが悪いようである。 そこで，予測変数を増やして検討してみる。lm()では，予測変数\\(x\\)のn乗を含む多項式のモデルを考慮することも可能である。例えば，以下は3次の多項式の例である。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}\\\\ \\tag{9} \\end{equation} \\] n次式のモデルは多項式回帰(polynomial regression)と呼ばれる。 lm()では，I()の中に書くかたちでn次の予測変数を入れることができる。 result_3 = lm(data = d, y ~ x + I(x^2) + I(x^3)) 同じく，3次の多項式による予測の結果を図で確認しよう。 newdat = data.frame(x = x) conf.int_3 = predict(result_3, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_3) ggplot() + geom_point(data = conf_dat, aes(x = x, y = y))+ geom_line(data = conf_dat, aes(x = x, y = fit)) + geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 9次の式でも推定してみよう。 result_9 = lm(data = d, y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9)) newdat = data.frame(x = x) conf.int_9 = predict(result_9, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_9) ggplot() + geom_point(data = conf_dat, aes(x = x, y = y))+ geom_line(data = conf_dat, aes(x = x, y = fit)) + geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 線は全てのデータ点を完全に通っている。当然ながら，データの観測値の分だけパラメータがあれば，そのモデルはデータ点を全て通る線を引くことができる。現在のデータ点全てを予測することができる。 しかし，そのモデルは現在のデータを全て当てられても，将来得られる未知のデータを当てられるとは限らない。予測変数を多くすると現在のデータには当てはまるが，当てはまりすぎて未知のデータの予測力が低下してしまうことを，過学習(overfitting)という。 複雑なモデルが現在のデータによく当てはまるのは，ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし，複雑なモデルは現在のデータに当てはまっても，未知のデータにもうまく当てはまるとは限らない。 理想的なモデルは，「予測力が高く，かつ予測変数ができるだけ少なくてシンプルなモデル」となる。 10.6 モデルの予測力の評価 モデルの予測力を評価するための指標について説明する。 10.6.1 決定係数 線形モデルでは，データに対する回帰分析のモデルの予測力を表す指標として，決定係数（R-squared）がある。 サンプルデータattitudeを例に見てみよう。 model_01 = lm(data=attitude, rating ~ complaints + learning) summary(model_01)$r.squared #r.squaredで決定係数のみを取り出すことができる。 ## [1] 0.7080152 これは，モデルから求めた予測値と実測値の分散が，実際のデータの分散に占める割合を意味する指標である。つまり，そのモデルでどれだけ全データの分散を説明できているかを意味する。 \\[ R^2 = \\sum_{i=1}^{n} \\frac {(y_{i}-\\hat{y}_{i})^2}{(y_{i}-\\bar{y})^2} \\tag{10} \\] ただし，決定係数は単純に，予測変数が増えるほど大きくなる（説明できる分散の量が増える）。 例えばattitudeデータ内の全ての変数を予測変数に使ってみる model_full = lm(data=attitude, rating ~ .) #線形予測子を入力するところにドットを入力すると，そのデータに含まれる全ての変数を予測変数として扱う summary(model_full)$r.squared ## [1] 0.732602 予測変数に影響を及ぼさない変数を含めても，決定係数は上昇してしまう。 決定係数は，「予測力が高く，シンプルなモデル」を探すには常に適切な指標であるとは言えない。 10.6.2 赤池情報量基準（AIC） 予測変数の少なさとモデルの予測力の高さのバランスを取った指標の一つとして，赤池情報量基準(Akaike inoformation criteria: AIC)がよく知られている。AICは以下の式で計算される。 \\[ AIC = -2 \\log L + 2k \\tag{11}\\\\ \\] \\(\\log L\\)は最大対数尤度，\\(k\\)はモデルのパラメータ数である。 第9章で，モデルのパラメータを推定する方法として「最尤法」を紹介した。最尤法は，モデルのもっともらしさ（データが生じる確率）を意味する「対数尤度」が最大となるときのパラメータを求める方法であった。最大対数尤度は，現在のモデルに対する当てはまりの良さを反映している。その最大対数尤度に対し，パラメータ数\\(k\\)に応じてペナルティ(penalty term)を加える。 AICの値が低いほど，モデルの予測力が高いと評価する。AICは余計なパラメータが多くなる（\\(k\\)が大きくなる）ほど大きい値を取る。つまり，データをうまく予測しつつ，かつパラメータ数を抑えてシンプルなモデルを探る目的にかなっている。 AIC()関数でモデルをカッコ内に入れると，AICを算出してくれる。さきほどのattitudeに当てはめた2つのモデルのAICを見てみよう。 AIC(model_full) ## [1] 210.4998 AIC(model_01) ## [1] 205.1387 model_fullよりもmodel_01のAICが低く，model_01の予想力の方が高いことを示している。 確認問題 問1 Rで標準で入っているデータwarpbreaksを使って練習をする。 prac_dat_1 = warpbreaks #別の名前で保存する head(prac_dat_1) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot() + geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks)) ggplot() + geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks)) 1-1 変数woolについて, 「Aを1, それ以外を0」としたダミー変数を作成し，そのダミー変数を予測変数，breaksを応答変数として線形モデルを行い，切片及びダミー変数に係る傾きの推定値を報告せよ。 また，ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。 1-2 変数tensionについて, 「Lを1, それ以外を0」，「Mを1, それ以外を0」とした2種類のダミー変数を作成し，それら2つのダミー変数を予測変数，breaksを応答変数として線形モデルを行い，切片及び各ダミー変数に係る傾きの推定値を報告せよ。 更に，そのときの切片及び各ダミー変数の係数が意味することを説明せよ。 1-3 1-2で作ったダミー変数に加え，更に「Hを1, それ以外を0」としたダミー変数を追加で作成する。 更に，breaksから全体のbreaksの平均を引いた変数breaks_2を作成する。 それら3つのダミー変数を予測変数，breaks_2を応答変数として線形モデルを行い，各ダミー変数に係る傾きの推定値を報告せよ。ただし，モデルには切片の項は加えないものとする。 更に，そのときの各ダミー変数の係数が意味することを説明せよ。 問2 問1に引き続き，Rで標準で入っているデータwarpbreaksを使って練習をする。ただし，tensionがHの部分を除いたデータを用いる。 library(tidyverse) prac_dat_2 = warpbreaks %&gt;% filter(tension != &quot;H&quot;) #tension == Hは除き，別の名前で保存する head(prac_dat_2) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot() + geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension)) breaksを応答変数，wool, tension, woolとtensionの交互作用項を予測変数とした線形モデルを行い，切片，woolの傾き，tensionの傾き，交互作用項の推定値を報告せよ。 問3 Rで標準で入っているairqualityを使う。 prac_dat_3 = airquality %&gt;% na.omit() #欠損値を除き，別の名前で保存する head(prac_dat_3) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 3-1 Ozoneを応答変数，Solar.R, Wind, Tempを予測変数とした線形モデルを行う。そして，切片及び傾きの推定値を報告せよ。 3-2 3-1で行った線形モデルについて，決定係数を報告せよ（Multiple R-squared）。 3-3 以下の3種類の線形モデルの解析を行い， モデル1: Ozoneを応答変数，Solar.R, Wind, Tempを予測変数とした線形モデル モデル2: Ozoneを応答変数，Solar.R, Tempを予測変数とした線形モデル モデル3: Ozoneを応答変数，Tempを予測変数とした線形モデル それぞれのモデルのAICを報告するとともに，3つのモデルのうち予測力が高いと考えられるものはどれかを報告せよ。 "],
["10-glm.html", "Chapter 11 一般化線形モデル 11.1 準備 11.2 一般化線形モデル 11.3 応答変数が二値の場合 11.4 応答変数がカウントデータの場合 確認問題", " Chapter 11 一般化線形モデル 誤差分布が正規分布以外の場合の「一般化線形モデル」について学ぶ。 ロジスティック回帰 ポアソン回帰 その他の一般化線形モデル 11.1 準備 tidyverseパッケージを読み込む。また，新たにMASSパッケージを使うので，インストールとロードを行う。 library(tidyverse) install.packages(&quot;MASS&quot;) library(MASS) 11.2 一般化線形モデル 線形モデルは，以下の式で表されるモデルであった。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x \\\\ \\tag{1} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] 線形モデルでは，応答変数が正規分布に従うという前提で，応答変数\\(y\\)を予測するパラメータ（線形予測子の切片と係数，及び正規分布の分散）を求めた。 今回は，応答変数が正規分布以外の確率分布に従うモデルを扱う。 線形モデルを正規分布以外の確率分布に拡張したモデルを，一般化線形モデルという。 まず，応答変数が二値のカテゴリカル変数の場合に例として用いながら見ていく。 11.3 応答変数が二値の場合 前の章までは応答変数が量的変数の例を扱ってきた。では，応答変数がカテゴリカル変数である場合は，どのような解析をすればよいのだろうか。 MASSパッケージに入っているサンプルデータbiopsyを使いながら検討していこう。まず，以下のプログラムを実行して，練習用のデータdatを作成する。 library(MASS) dat = biopsy dat$y = ifelse(dat$class == &quot;malignant&quot;, 1, 0) #classがbenignならばゼロ，それ以外なら1という変数yを作る dat$x = dat$V1 #V1という変数をxという名前に変える head(dat) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class y x ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 5 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 5 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 3 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 6 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 4 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 8 xは整数の変数，yは1ならば癌，0ならば癌ではないことを意味する変数とする。 xが変化すると癌である確率が変化するかを検討したい。 まず，xとyとの関係を図で確認してみる。 ggplot2パッケージで，xをx軸，yをy軸にしてプロットしてみよう。 普通にgeom_pointで散布図を作っても点が重なって見にくいので，geom_jitterを使って描画する。geom_jitterは点をランダムでずらして描画してくれる。 ggplot() + geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + scale_y_continuous(breaks = seq(0,1,0.1)) では，前章までで学んだとおりに，xを予測変数，yを応答変数とした線形モデルでxの効果を検討しよう。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\\\ \\tag{2} y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] result_lm = lm(data = dat, y ~ x) summary(result_lm) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77804 -0.17331 -0.01994 0.06859 1.06859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.189535 0.023395 -8.102 2.43e-15 *** ## x 0.120947 0.004467 27.078 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3323 on 697 degrees of freedom ## Multiple R-squared: 0.5127, Adjusted R-squared: 0.512 ## F-statistic: 733.2 on 1 and 697 DF, p-value: &lt; 2.2e-16 xに係る傾きの推定値を数値通りに解釈すると，「xが1単位増えると，yが0.12増える」ことを示している。 では，求まった傾きと切片から直線を先程のxとyとの関係の図に引いてみよう。 predict_lm = predict(result_lm, interval = &quot;confidence&quot;, level = 0.95) #直線の95%信頼区間を求める dat_predict = cbind(dat, predict_lm) ggplot() + geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + geom_line(data = dat_predict, aes(x = x, y = fit)) + geom_ribbon(data = dat_predict, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.5) + scale_y_continuous(breaks = seq(0,1,0.1)) 線形モデルから推定された直線は，「xが増えるほどyが増える」関係を表しているように見える。 しかし，この線形モデルの結果は，yを予測する上で問題がある。 解析の目的は，\\(y = 1\\)の確率，つまりがんにかかる確率を推定することであるが，例えば\\(x\\)が10を超えると，応答変数の予測値は1以上の値を取る。また，\\(x\\)が2.5を下回ったときも，0未満の数値が推定されてしまう。応答変数は0か1しか取らないのに，それぞれを超える値が予測されてしまう。これは確率の推定としては不都合である。 応答変数\\(y\\)は連続量ではなく，0か1の値を取るカテゴリカル変数である。連続量の確率分布である正規分布に応答変数が従うという前提を置くのは理論的に正しくない。 11.3.1 ロジスティック回帰 ではどうすれば良いのか？ 解決策として，モデルを以下のように変更する。 \\[ \\begin{equation} q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\\\ \\tag{3} y \\sim Bernouli(q) \\end{equation} \\] \\(\\exp(\\alpha + \\beta x)\\)は，\\(e^{(\\alpha + \\beta x)}\\)とも表記できる。 \\(y = 1\\)である確率（がんである確率）を\\(q\\)とする。式(3)の1つ目の式は，線形予測子\\(\\alpha + \\beta x\\)から\\(q\\)を導くように，式の変形を行っている（詳細は後で述べる。今はわけがわからないかもしれないが，気にせず読み進めてほしい）。 2つ目の式は，応答変数\\(y\\)は成功確率\\(q\\)のベルヌーイ分布に従うということを示している。 復習すると，ベルヌーイ分布は，試行回数が1の時の二項分布である。ベルヌーイ分布のパラメータは成功確率\\(q\\)で，0（失敗）か1（成功）の値を生成する。 rand = rbinom(n =20, size = 1, prob = 0.5) #size = 1にする。コインを１回投げて，表だったら1，裏だったら0。これをn=20回行った結果 rand #0か1の値が20個出るが，それぞれの0か1が裏あるいは表を意味しているとイメージしてほしい ## [1] 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 式(3)の1つ目は何を意味しているのか？以下の式について，\\(z = \\alpha + \\beta x\\)として，\\(z\\)を変化させると\\(q\\)がどう変化するか図で見てみよう。 z = seq(-10, 10, 0.1) #-10から10まで0.1刻みのベクトルzを作成 q = exp(z)/(1+exp(z)) #上の式にzを代入して，qを求める d = data.frame(z = z, q = q) #グラフを作るために，データフレームを作る ggplot()+ geom_line(data = d, aes(x=z, y=q)) \\(z\\)は\\(-\\infty\\)から\\(\\infty\\)の範囲を取るが，\\(z\\)がどのような値をとっても，\\(0&lt;q&lt;1\\)となる（限りなく0もしくは1に近づく）。\\(q\\)は確率なので，この0から1の範囲に収まるようになる変換は都合が良い。 この\\(\\exp(z)/(1 + \\exp(z))\\)の変換は，ロジスティック(logistic)または逆ロジット(invers-logit)と呼ばれる。 つまり，線形予測子に逆ロジットの変換を施すことで，パラメータの値にかかわらず確率の値を予測することが可能となる。 以下の式(4)は， \\[ \\begin{equation} q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\\\ \\tag{4}\\\\ \\end{equation} \\] 右辺を線形予測子の式に揃えると，以下の式(5)になる。 \\[ \\begin{equation} \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\tag{5}\\\\ \\end{equation} \\] この変換は，ロジット関数（logit function）と呼ばれる。 ロジットとは確率の対数オッズのことをいう。オッズとは成功確率と失敗確率の比（\\(q/(1-q)\\)），対数オッズとはオッズの対数を取ったものである。 線形予測子を変換する関数は，「リンク関数」と呼ばれる。上の例のように，応答変数が二値の場合は，推定値を0から1に収めるためにロジット関数をリンク関数として使うのが適切である。 このように，「応答変数が従う確率分布」と「線形予測子に変換をほどこすリンク関数」を選ぶことにより，線形モデルを様々なデータ解析に一般化させたものが一般化線形モデル(generalized linear model)である。 「確率分布」と「リンク関数」を応答変数のタイプに応じてカスタマイズするというイメージで捉えると良い。 上の例で見た「応答変数が従う確率分布をベルヌーイ分布（または二項分布）」，「リンク関数をロジスティック関数（ロジット関数）」とした一般化線形モデルは，ロジスティック回帰と呼ばれる。 11.3.2 Rでの一般化線形モデル Rには，一般化線形モデルを行うための関数glm()がある。線形モデルを扱うlm()と同じ要領でプログラムを書けばよいが，確率分布とリンク関数のオプションを自分で指定する必要がある。先程のサンプルデータdatで，glm()関数を使ってロジスティック回帰をやってみよう。 result_glm = glm(data = dat, y ~ x, family = binomial(link=&quot;logit&quot;)) glmで設定すること： 「線形予測子」，「応答変数が従う確率分布」，「リンク関数」を指定する。 familyで，応答変数が従う確率分布を指定する。 family = binomial，すなわち二項分布（binomial distribution）に従うとする。（正確にはベルヌーイ分布であるが，binomialで構わない） (link=)で，リンク関数を指定する。ロジット関数(logit)を指定しよう。 ちなみに，(link=&quot;logit&quot;)は省略してもかまわない。family=binomialとすると，デフォルトでリンク関数をlogitとしてくれる。 では，出力結果を見てみよう。 summary(result_glm) ## ## Call: ## glm(formula = y ~ x, family = binomial(link = &quot;logit&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## x 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 出力はlm()と似ている。 Coefficientsの部分を見る。Estimateがパラメータの推定結果である。Prがp値である。パラメータの推定値は，プラスならば応答変数が1の値，マイナスならば応答変数が0の値を取りやすいことを意味する。 xに係る傾の値き0.94は何を意味しているのか？ 線形モデルでは傾きの推定値は，「予測変数が1単位増えたときの応答変数の変化量」を意味していた。今回の例も，xが1増えると確率が0.94上がるということを示しているのか？ 一般化線形モデルの場合，係数の値が意味することの解釈には注意が必要である。 式(5)を思い出してほしい。 \\[ \\begin{equation} \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\tag{5}\\\\ \\end{equation} \\] 右辺を線形の式とすると，左辺は対数オッズとなる。つまり，\\(x\\)に係る傾き\\(\\beta\\)は，\\(x\\)が1増えた時の\\(q\\)の対数オッズの変化量を意味する。確率\\(q\\)そのものの変化量ではない。 対数オッズと確率\\(q\\)との関係を図で見てみよう。x軸を\\(\\log(q/[1-q])\\)，y軸を\\(q\\)とした図を示す。 q = seq(0, 1, 0.01) logit = log(q/(1-q)) sample_dat = data.frame(q = q, logit = logit) ggplot() + geom_line(data = sample_dat, aes(x = logit, y = q)) つまり，対数オッズがプラスだと確率\\(q\\)は0.5より大きくなり，対数オッズがマイナスだと確率\\(q\\)は0.5より小さくなる関係にある。要は，対数オッズがプラスだと\\(y = 1\\)が起こりやすくなり，マイナスだと起こりにくくなることを意味している。 求まった係数の推定値を元に，確率を予測する線を引いてみよう。 new = data.frame(x = seq(0, 11, 0.1)) predict_glm = predict(result_glm, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot() + geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + geom_line(data = dat_predict, aes(x = x, y = y)) + scale_y_continuous(breaks = seq(0,1,0.1)) 予測線は0から1の範囲に収まっており，線形予測子から確率の予測ができている。 11.4 応答変数がカウントデータの場合 応答変数が正規分布以外に従う場合の例として，先程は応答変数が0か1の二値の値の場合を扱った。同じく応答変数に制約がある場合の例として，次は応答変数が正の値の整数しか取らない場合（0を含む）の例を扱う。 具体的には，応答変数がカウントデータの場合である（非負の整数。0個，1個，2個,3個といった個数など）。この場合は，ポアソン回帰と呼ばれる一般化線形モデルによる解析をするのが適切だと言われている。 サンプルデータを扱いながら，ポアソン回帰について学んでいこう。 11.4.1 ポアソン回帰 以下のプログラムを実行して，サンプルデータdatを作成しよう。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd=1) lambda = exp(0.01+ 0.6*x) y = rpois(n=N, lambda = lambda) dat = data.frame(y=y, x=x) 分布を確認してみる。 ggplot()+ geom_histogram(data = dat, aes(x=y)) ggplot()+ geom_point(data = dat, aes(x=x, y=y)) xが大きいほど，yが大きいという関係がありそうである。\\(x\\)から，\\(y\\)を予測する。 ただし，ここで注意が必要な点がある。推定される\\(y\\)は正の値を取る離散値（整数）として推定されなければならない。 このように，応答変数が正の整数であるカウントデータである場合は，確率分布としてポアソン分布が用いられる。 モデルは以下のようになる。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\\\ y \\sim Poisson(\\lambda) \\end{equation} \\] 1つ目の式では，線形予測子から\\(\\lambda\\)を求めている。 2つ目の式では，1つ目で求めた\\(\\lambda\\)をパラメータとするポアソン分布から，応答変数\\(y\\)が生成されるということを示している。 11.4.2 ポアソン分布の復習 ポアソン分布は，パラメータ\\(\\lambda\\)を持つ確率分布である。 \\[ P(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!} \\tag{7}\\\\ \\] \\(y\\)は0以上の整数（0, 1, 2, 3, …），\\(P(y)\\)は\\(y\\)が生じる確率とする。 ポアソン分布のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，平均と分散が等しい分布である。 ポアソン分布は，二項分布とも関連している。 二項分布のパラメータは，試行回数\\(n\\)と成功確率\\(p\\)であった。二項分布の期待値（平均）は\\(np\\)，分散はnp(1-p)である。 $$ y Binomial(n, p)\\ E(y) = np\\ Var(y) = np(1-p)\\ $$ 二項分布の試行回数\\(n\\)が大きく，成功確率\\(p\\)が小さい場合，二項分布の平均と分散はほとんど等しくなり，ポアソン分布に近似する。 つまり，めったに起こらないイベントが生じる回数は，ポアソン分布に従うといわれている。 11.4.3 ポアソン回帰のリンク関数 線形予測子がリンク関数でどう変換されているかを確認すると， \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\tag{3} \\\\ \\end{equation} \\] なぜ指数関数を用いるのか？\\(z=\\alpha + \\beta x\\)として，\\(\\lambda=\\exp(z)\\)との関係を図で見てみよう。 z = seq(-5, 5, 0.1) #-10から10まで0.1刻みのベクトルzを作成 lambda = exp(z) #上の式にzを代入して，lambdaを求める d = data.frame(z = z, q = lambda) #グラフを作るために，データフレームを作る ggplot()+ geom_line(data = d, aes(x=z, y=lambda)) 図からもわかるように，\\(z\\)の値に関わらず，\\(\\lambda\\)は正の値を取る。ポアソン分布のパラメータ\\(\\lambda\\)は\\(\\lambda&gt;0\\)という制約であるため，このように変換することでちょうどよくなる。 また，式(3)の右辺を線形予測子に直すと，以下になる。 \\[ \\log\\lambda_{i}=\\alpha+\\beta x \\tag{4}\\\\ \\] このように，ポアソン回帰では，応答変数が従う確率分布をポアソン分布，線形予測子と応答変数をリンクさせるリンク関数は対数関数（log）を設定する。 11.4.4 Rでのポアソン回帰 Rでポアソン回帰をやってみよう。 一般化線形モデルを扱う関数glm()で，確率分布にポアソン分布，リンク関数に対数を指定して，ポアソン回帰をやってみる。 (link = &quot;log&quot;)は省略しても構わない。family = poissonで確率分布をポアソン分布にすれば，リンク関数は自動で対数になる。 result_poisson = glm(data = dat, y ~ x, family = poisson(link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = y ~ x, family = poisson(link = &quot;log&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4112 -0.7542 0.1362 0.5628 1.7629 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6829 0.2807 -2.433 0.015 * ## x 0.8951 0.1052 8.506 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 130.32 on 49 degrees of freedom ## Residual deviance: 46.52 on 48 degrees of freedom ## AIC: 197.96 ## ## Number of Fisher Scoring iterations: 5 推定された式が実測値をうまく予測できているか，図で確認してみよう。 new = data.frame(x = seq(0, 4, 0.1)) predict_glm = predict(result_poisson, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot() + geom_point(data = dat, aes(x=x, y=y)) + geom_line(data = dat_predict, aes(x = x, y = y)) 11.4.5 一般化線形モデルのまとめ 線形モデルを正規分布以外の別の確率分布に拡張したモデルのことを，一般化線形モデルという。 二値のデータや割合である場合→ロジスティック回帰（確率分布はベルヌーイ分布もしくは二項分布，リンク関数はロジット） カウントデータである場合→ポアソン回帰（確率分布はポアソン分布，リンク関数は対数） 次の章では，ロジスティック回帰やポアソン回帰以外の一般化線形モデルを扱う。 確認問題 問１ 以下のプログラムを実行し，サンプルデータを作成する。 変数の意味は以下の通りである。 Disease: ある病気にかかっているか（1=かかっている，0=かかっていない） BMI: BMI（肥満度を表す指標） Exercise: 1週間あたりの運動時間（単位：時間） Sleep: 1日の睡眠時間（単位：時間） Disease = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) BMI = c(15, 16, 16, 18, 19, 20, 21, 22, 22, 23, 23, 23, 24, 24, 24, 30, 31, 31, 33, 34, 34, 34, 35, 36, 40, 40, 40, 41, 43, 43) Exercise = c(2, 1, 1, 2, 0, 3, 1, 1, 4, 4, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 0, 2, 2, 3, 0, 4) Sleep = c(7, 4, 5, 4, 4, 6, 5, 6, 4, 6, 4, 7, 4, 7, 4, 6, 5, 4, 5, 6, 7, 5, 4, 6, 4, 7, 5, 5, 4, 7) data_q01 &lt;- data.frame(Disease = Disease, BMI = BMI, Exercise = Exercise, Sleep = Sleep) Diseaseを応答変数，BMI，Exercise，Sleepを予測変数としたロジスティック回帰を行い，5%水準で有意な効果を持つ予測変数について報告せよ。 ヒント：係数の値が正か負かにも注意して，予測変数が応答変数にどのような影響を持っていたかについて報告する。 "],
["11-glmm.html", "Chapter 12 マルチレベルモデル 12.1 準備 12.2 個人差や集団差の問題 12.3 マルチレベルモデルの概要 12.4 Rでのマルチレベルモデル 12.5 正規分布以外を扱う例 確認問題", " Chapter 12 マルチレベルモデル 一般化線形モデルを拡張し，個人差や集団差を扱うモデルについて学ぶ。 12.1 準備 tidyverseパッケージに加え，新たにlme4及びlmerTestというパッケージを使う。lme4とlmerTestは初めて使うので，インストールした上でロードしよう。 library(tidyverse) install.packages(&quot;lme4&quot;, &quot;lmerTest&quot;) library(lme4, lmerTest) 12.2 個人差や集団差の問題 以下では，Rにデフォルトで入っている iris データを例として使う。 head(iris) #irisデータの上数行を表示 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa まず，がくの長さ（Sepal.Length）とがくの幅（Sepal.Width）の関係を散布図で示してみよう。 graph_1 = ggplot() + geom_point(data=iris, aes(x=Sepal.Length, y=Sepal.Width),size = 3) graph_1 まず，lm()を使って，がくの幅を応答変数，がくの長さを予測変数とした線形モデルで係数を推定する。 iris_lm = lm(data = iris, Sepal.Width ~ Sepal.Length) summary(iris_lm) ## ## Call: ## lm(formula = Sepal.Width ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1095 -0.2454 -0.0167 0.2763 1.3338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.41895 0.25356 13.48 &lt;2e-16 *** ## Sepal.Length -0.06188 0.04297 -1.44 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4343 on 148 degrees of freedom ## Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 ## F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 推定された切片及び傾きの値から予測直線を引くと，以下のようになる。 graph_lm = ggplot()+ geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width), size = 3) + geom_smooth(data = iris, aes(x = Sepal.Length, y = Sepal.Width), formula = y~x, method = &quot;lm&quot;, se = FALSE) graph_lm がくの長さ（Sepal.Length）は，がくの幅に対してあまり影響を持っていない可能性にあることがうかがえる。 では，この散布図を種（Species）ごとに色わけして示してみる。 graph_2 = ggplot() + geom_point(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species, shape = Species), size = 3) graph_2 種を無視して検討したところ，がくの長さとがくの幅の間には関係がないようにみえたが，種ごとに分けてみると「がくの長さが大きくなるほど，がくの幅が大きくなる」関係にあるように見える。 このあやめのデータのように，いくつかのデータが同じグループに属している構造の場合，グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には，統計的独立性が保証されていないためである。つまり，同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。 独立(independence)とは，各データが他のデータに影響されないという意味である。これまで学んできた確率分布では，独立同分布(independent and identically distributed: i.i.d.)が前提とされている。例えば，コインを数回投げて投げて表が出る回数は二項分布に従うが，表が出るかどうかは前の試行に影響されることはない（前回表が出たら，次も表が出やすいということはありえないという前提を置く）。 しかし，現実のデータでは，データ間の相関などにより，事象の独立性が保たれていないケースもありえる。その場合，統計的独立性を前提とした解析を行うと，上の例のように誤った結論を導いてしまう恐れがある。 この例に限らず，階層構造を持つデータや繰り返し測定データにも，同じことがいえる。例えば，学校ごとに学力テストを行った場合，同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合，その参加者のデータは似たような傾向になる可能性も考えられる。 このようなデータに対して，個人や集団の影響を考慮した統計モデルとして，マルチレベルモデル(multilevel model)が提案されている。 マルチレベルモデルは，「階層モデル(hierarchical model)」，「混合モデル(mixied model)」など，色々な呼ばれ方がされている。 12.3 マルチレベルモデルの概要 マルチレベルモデルでは，予測変数が応答変数に及ぼす効果だけではなく，個人や集団の効果を扱う。予測変数そのものの効果は固定効果（fixed effect）と呼ばれ，個人や集団ごとの効果はランダム効果（random effect）と呼ばれて区別される。 前章まで扱ってきた，一般化線形モデルは固定効果のみを含むモデルである。 例として， 繰り返し測定されたデータを扱う。以下のプログラムを実行して，サンプルデータexampleを作ろう。 example = data.frame(i = 1:6, j = c(1, 1, 2, 2, 3, 3), y = round(rnorm(6), 2), x = rep(c(0, 1),3) ) example ## i j y x ## 1 1 1 0.29 0 ## 2 2 1 -0.44 1 ## 3 3 2 0.00 0 ## 4 4 2 0.07 1 ## 5 5 3 -0.59 0 ## 6 6 3 -0.57 1 \\(i\\)がデータを意味する番号（何行目か），\\(j\\)を個人もしくはグループを意味する番号とする。例えば，個人\\(j\\)が\\(x=0\\)の場合と\\(x=1\\)の場合の2回\\(y\\)を測定している，あるいは同じ集団\\(j\\)から2人が選ばれてそれぞれの人について\\(y\\)が測定された，といったケースが当てはまる。 一般化線形モデルの線形予測子は，以下のような数式で表現できた。 $$ = + x_{i} \\ y_{j} Normal(, ) $$ \\(\\alpha\\)が切片，\\(\\beta\\)が予測変数\\(x\\)に係る傾きであった。 これに対し，マルチレベルモデルでは，以下のように線形予測子に\\(\\alpha_{j}\\)が加わる。 $$ y_{i} = {0} + x{i} + {j} \\ {j} Normal(0, {})\\ y{j} Normal(, ) $$ すべての個人に共通して影響する切片\\(\\alpha_{0}\\)に加え，個人ないしはグループごとに異なる値を取る切片\\(\\alpha_{j}\\)を考慮している。 更に，\\(y_{j} \\sim Normal(\\hat{y_{i}}, \\sigma)\\)にあるように，個人ごとの切片\\(\\alpha_{j}\\)が「平均をゼロ，\\(\\sigma_{\\alpha}\\)を標準偏差とする正規分布から生成される」という過程を置いている。 これにより，同じグループに属するデータ（例えば\\(j=1\\)）には同じ効果（\\(\\alpha_{1}\\)）が共通して係ることを表現できる。 ランダム効果は切片に限らない。例えば傾きを\\(\\beta_{ j}\\)にする，すなわち個人ごとに予測変数に係る効果が異なるという前提を置くこともできる。 しかし，実際に傾きをランダム効果としたモデルは複雑で推定するのは困難であるため（最尤推定法では解が求まらない場合がある），多くの場合，個人差の影響（ランダム効果）は切片のみを考慮したモデルで表現されることが多い。 ランダム傾きを含むマルチレベルモデルを扱う際には，ベイズ統計の手法が必要になる。これについては，次章以降で扱う。 12.4 Rでのマルチレベルモデル Rでマルチレベルモデルで解析を行うためには，外部パッケージが必要になる。様々なパッケージがあるが，lme4パッケージが扱いやすい（lmerTestも必要）。以下では，lme4パッケージに含まれるlmer()を使った解析の例を示す。 基本的に，lm()関数と似た表記で使うことができる。ランダム切片は，(1|グループを意味する変数名)というかたちで線形予測子に入れる。 model_lmm = lmer(data= iris, Sepal.Length ~ Sepal.Width + (1|Species)) #(1|Species)を加える summary(model_lmm) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.4062 0.6683 3.4050 5.097 0.0107 * ## Sepal.Width 0.7972 0.1062 146.6648 7.506 5.45e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 出力結果を見てみると，Fixed effectsという部分がある。ここに，固定効果の結果が表示されている。見方は，一般化線形モデルのときと同じである。切片(intercept)と予測変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず，すべての個体共通に係る予測変数の効果）。 ランダム効果の推定結果は，Random effectsという部分に表示されている。分散パラメータの推定結果で，さきほどの式の\\(\\sigma_{\\alpha}\\)の推定結果を意味している。 がくの幅（Sepal.Width)の回帰係数（Estimate）を見ると，lm()での推定結果とは逆に，プラスになっている。やはり，グループの違いを統制すると，実際にはがくの幅が大きくなるほど，がくの長さも大きくなる関係にあることが，lmer()による推定結果からわかる。 lmer()では，デフォルトで係数のp値は表示されない。p値も見たいのならば，lmerTest()パッケージをインストールしておく必要がある。 *** 12.5 正規分布以外を扱う例 応答変数が正規分布以外に従う場合のマルチレベルモデルについても見ていこう。 lme4のglmer()で，正規分布以外の確率分布を指定したマルチレベルモデルの解析を行うことができる。以下では，ランダム効果を加えたロジスティック回帰分析の例を示す。 まず，以下のプログラムを実行してサンプルデータdata_sampleを作ろう。 x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0) y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) ID = c(rep(&quot;a&quot;,5),rep(&quot;b&quot;,5),rep(&quot;c&quot;,5)) data_sample = data.frame(ID, x1, y1) str(data_sample) ## &#39;data.frame&#39;: 15 obs. of 3 variables: ## $ ID: chr &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; ... ## $ x1: num 1 2 3 4.2 5.1 3.1 4.2 5 6.1 7 ... ## $ y1: num 0 0 1 1 1 0 0 0 0 1 ... x1を予測変数（量的変数），y1を応答変数（0か1のいずれかを取る），IDが個体を示す変数とする。1つの個体からx1を変えて5回，y1が計測がされた実験をイメージしてほしい。 予測変数と応答変数の関係に，個体特有の効果を加えたモデルは以下となる。 $$ q_{i} = \\ {j} Normal(0, {})\\ y_{j} Bernoulli(q_{i}) $$ 線形予測子をロジット（逆ロジット）変換して，\\(y=1\\)が生じる確率\\(q\\)を求める。応答変数\\(y\\)は，\\(q\\)をパラメータとするベルヌーイ分布から生成される。これらの点は，一般化線形モデルで学んだ。 更に，線形予測子に，ランダム切片\\(\\alpha_{j}\\)を加えた。\\(\\alpha_{j}\\)は，平均ゼロ，標準偏差\\(\\sigma_{\\alpha}\\)の正規分布に従って生成されるとする。 正規分布以外の確率分布を扱うマルチレベルは，Rではlme4パッケージのglmer()で扱うことができる。 さっきのlmer()と同じ要領で，線形予測子に個体を識別する変数（ID）を加える。以下のように，(1|ID)というかたちで入れる。 あとは，確率分布とリンク関数を指定する。指定の仕方は，glm()のときと同じ要領である。確率分布はbinomial，リンク関数はlogitを指定する。 model_logistic_glmm = glmer(data = data_sample, y1 ~ x1 + (1|ID), family = binomial(link=&quot;logit&quot;)) summary(model_logistic_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: y1 ~ x1 + (1 | ID) ## Data: data_sample ## ## AIC BIC logLik deviance df.resid ## 14.3 16.4 -4.2 8.3 12 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.01928 0.00000 0.00000 0.00000 0.04031 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 67795 260.4 ## Number of obs: 15, groups: ID, 3 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -220.575 27.154 -8.123 4.54e-16 *** ## x1 38.996 4.804 8.117 4.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x1 -0.987 確認問題 問１ carパッケージに入っているカナダにおける職業の威信度に関する調査データPrestigeを使う。102業種に関する調査結果が入っている。 library(car) head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof prestigeを応答変数，education, income及び womenを予測変数，typeをランダム効果（切片）としたマルチレベルモデルで解析せよ。確率分布は正規分布を用いるものとする。 解析の結果，有意な効果を持った予測変数を挙げ，結論を述べよ（その予測変数が大きくなるほど，応答変数がどう変化するか）。 なお，変数の意味は以下の通りである。 prestige：職業威信度（値が高いほど威信度が高い） education：在職者の平均教育年数 income：平均所得（単位はドル） women：女性の割合 type：職業のカテゴリ（bc=ブルーカラー，wc=ホワイトカラー，prof=専門職） ヒント：正規分布を扱うマルチレベルの場合は，lme4パッケージのlmer()を使えば良い。p値を出力したい場合は，lmerTest()パッケージも必要になる。 なお，出力時にメッセージが出ても無視して良い（中心化せよという命令だが，無視して良い）。 "],
["12-Bayese.html", "Chapter 13 ベイズ統計 13.1 ベイズの定理 13.2 MCMC 13.3 準備", " Chapter 13 ベイズ統計 13.1 ベイズの定理 13.2 MCMC 13.3 準備 13.3.1 rtanとbrmsパッケージのインストール rstanパッケージのインストールについては，「Rstan Getting Started (Japanese)」https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese)のページを参照のこと。 「RStan」と「C++コンパイラ」のインストールを行う。 rstanをインストールできたら，brmsパッケージもインストールする。 13.3.2 パッケージのロード rstanとbrmsをロードする。 library(rstan) library(brms) また，計算の高速化のために，以下のプログラムも実行しておく。 #計算を高速化するオプション rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) 13.3.3 brmsでベイズ統計 13.3.3.1 一般化線形モデル brmsパッケージのbrm()関数を使う。 formulaに線形予測子，familyに確率分布とリンク関数を指定する。 lme4パッケージのglmer()関数と同様の形式でプログラムを書ける。 dat = iris result = brms::brm(data = dat, #データを指定 formula = Sepal.Length ~ Sepal.Width, #線形予測子 family = gaussian(link = &quot;identity&quot;), #確率分布とリンク関数 seed = 1 #乱数の種 ) result 13.3.3.2 マルチレベルモデル dat = iris library(lme4) result_lmer = lme4::lmer(data = dat, Sepal.Length ~ Sepal.Width + (1|Species)) summary(result_lmer) result_brm = brm(data = dat, formula = Sepal.Length ~ Sepal.Width + (1|Species), family = gaussian(link = &quot;identity&quot;), seed = 1) result_brm 13.3.4 事前分布の設定 13.3.4.1 無情報事前分布 result_brm = brm(data = dat, formula = Sepal.Length ~ Sepal.Width + (1|Species), family = gaussian(link = &quot;identity&quot;), seed = 1, prior = c(set_prior(&quot;&quot;, class = &quot;intercept&quot;), set_prior(&quot;&quot;, class = &quot;sigma&quot;)) ) result_brm 13.3.4.2 弱情報事前分布 result_brm = brm(data = dat, formula = Sepal.Length ~ Sepal.Width + (1|Species), family = gaussian(link = &quot;identity&quot;), seed = 1, prior = c(set_prior(normal(0, 10), class = &quot;intercept&quot;), set_prior(&quot;&quot;, class = &quot;sigma&quot;)) ) result_brm "]
]
