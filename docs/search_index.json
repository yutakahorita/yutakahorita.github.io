[["index.html", "心理データ応用解析法 Chapter 1 はじめに 1.1 このテキストについて 1.2 テキストの構成", " 心理データ応用解析法 帝京大学文学部心理学科 堀田結孝 2023-04-12 Chapter 1 はじめに これは、帝京大学文学部心理学科の科目「心理データ応用解析法」のために作成されたテキストである。 1.1 このテキストについて このテキストでは、Rを使いながら心理学における応用的なデータ解析を学んでいく。 心理学を専攻していて基礎統計学を学修済みの学生を対象としている。また、Rを全く使ったことがない人も想定して、Rのインストールの仕方から基本的な使い方も解説する。 このテキストでは、t検定や分散分析など心理統計で学んだ手法を統計モデルという一つの枠組みで包括的に理解することを目的とする。具体的には線形モデル、一般化線形モデル、マルチレベルモデルを順に学んでいき、様々な種類のデータに対して柔軟に解析を行う技術を身に着けていく。 1.2 テキストの構成 このテキストでは、R及びRStudioを用いてデータ解析の練習をしていく。 第2章から第5章にかけては、Rの使い方について扱っている。 第2章ではRのインストール、Rでのプログラムの書き方、パッケージのインストールの仕方など、Rを使ってデータ解析をする上で基本的なことについて、Rを使うのが初めての人向けに解説している。第3章では、平均値や標準偏差、相関などの基礎統計量をRで計算する方法について解説する。第4章と第5章では、Rでデータを加工する方法やグラフの作成などについて解説する。 第6章以降で、データ解析について学んでいく。 まずは、心理統計でも学んだ内容の復習も交え、統計モデルを用いる上で必要な知識を順に学んでいく。第6章では、確率分布について学ぶ。第7章では、統計的仮説検定とそれが抱える問題を学ぶ。第8章では、t検定、χ二乗検定など、心理統計で学んだ解析をRで行う方法について解説する。 第9章からは、統計モデルについて学んでいく。第9章では線形モデルを学ぶ。この章は心理統計における回帰分析の復習を通して、t検定、分散分析などの解析法が線形モデルという一つの枠組みで扱えることを理解することを目指す。第10章では、多重共線性や過学習の問題など、線形モデルで解析を行う上での注意点を扱う。 第11章と第12章では、線形モデルの拡張である一般化線形モデルを学ぶ。質的変数など、正規分布に従わないデータを解析する手法を学んでいく。第11章では代表的な解析法としてロジスティック回帰とポアソン回帰について、第12章ではその他の一般化線形モデルについて学ぶ。一般化線形モデルを用いることで、様々な種類のデータに対して柔軟な解析が可能となることを理解する。 第13章では、更に一般化線形モデルを拡張させたマルチレベルモデルを学ぶ。同じ集団や個人から得られたデータなど、個人差や集団差を含むデータを解析する手法について学ぶ。 第14章と第15章にかけては、ベイズ統計など、より応用的な解析について学ぶ。 1.2.1 テキストの読み方 各章の最後に、確認問題を設ける。 "],["01-intro.html", "Chapter 2 Rの使い方 2.1 Rのインストール 2.2 RStudioのインストール 2.3 プログラムの書き方 2.4 単純な演算 2.5 変数 2.6 データ構造 2.7 欠損値 2.8 パッケージのインストールとロード 2.9 データの読み込み 2.10 ヘルプ 2.11 Rを終わらせる 確認問題", " Chapter 2 Rの使い方 Rのインストールから使い方までを解説する。 R及びRStudioのインストール （このテキストの手順でうまくいかない場合は、「R インストール」などで検索してみよう） プログラムの書き方 変数 データ構造 欠損値の扱い パッケージ データの読み込み 2.1 Rのインストール インストールは、https://cran.r-project.orgから可能。自分のOSにあったインストーラを選ぶ。 インストーラを実行したら、あとは指示に従ってインストールをすすめる。 2.2 RStudioのインストール RStudioとは、Rの使いやすさを向上させる目的で開発されているアプリケーションである。Rをインストールしたら、RStudioもインストールしておくこともすすめる。このテキストでも、RStudioを使って解析することを前提に説明する。 インストールは、https://rstudio.com/products/rstudio/#rstudio-desktopからできる。 「DOWNLOAD RSTUDIO DESKTOP」を選択（無料版で良い）。 RとRstudioの両方をインストールできたら、RStudioの方を開く。 以下のような画面が表示される。 注意: ここまでの手順で「RもしくはRStudioをインストールできない」あるいは「R及びRStudioはインストールできたが、起動できない」という人は、以下の可能性を考えてみてほしい。 OSがWindowsの場合、管理者権限のあるアカウントでR及びRStudioをインストールする必要がある。インストールする際は、管理者権限として実行しよう。 同じくOSがWindowsの場合、アカウント名にマルチバイト文字（全角文字）を含んでいるとRが正常に起動しない。つまり、マシンにログインする時の名前を「ほげ」など全角文字（日本語）にしてしまっていると、うまくいかない。この場合は面倒ではあるが、「既にあるアカウントの名前を半角英数に変更する（[hoge]など）」、あるいは「もう一つ別の半角のアカウントを作る」といった方法で対処してみよう。 Rに限らず、ファイルやフォルダ名に全角文字が含まれていると障害になる場合がある。ファイル名やフォルダ名には、なるべく日本語（全角文字）は使わない習慣を身に着けよう。 2.3 プログラムの書き方 Rの画面構成について確認する。 2.3.1 コンソール（Console） コンソール（Console）という部分にプログラムを入力すると、結果が出力される。 ためしに、コンソールの&gt;の部分に、以下のプログラムを入力して、Enterを押してみよう。 このテキストでは以下のように、背景が灰色の箇所にプログラムとその出力結果（行頭に##が付いている部分）が示されている。 1 + 1 ## [1] 2 同じコンソールに、答えである2が出力されたはずである。 このように、コンソールに直接プログラムを入力すると、結果を返してくれる。 2.3.2 スクリプト コンソールに入力したプログラムや出力結果は、Rを閉じると消えてしまう。これでは復習できないので、プログラムは別のファイルに残しておいた方が良い。 プログラムを書き込んだテキストのことを「スクリプト」と呼ぶ。プログラムはなるべく、スクリプトに残しておく習慣をつけよう。 「File」から「New Script」を選ぶ。何も書かれていないファイル（R Editor）が開かれる。 名前をつけて保存する。「File」から「Save as..」を選び、名前をつけて保存する。拡張子が「.R」のファイルとして保存される。 スクリプトに、試しに以下のプログラムを入力してみよう。 1 + 1 プログラムを選択し、CtrlとEnterを同時に押して実行する（「Run line or selection」を選んでも可）。すると、「R Console」にプログラムの結果が出力される。 スクリプトファイルを開きたいときは、RStudioを立ち上げて、「File」から「Open File」を選び、スクリプトのファイルを選ぶ。 初心者が戸惑いやすい点について説明する。 ためしに、コンソールに以下のプログラムを入力してEnterを押してみよう。 1 + 何も表示されないし、冒頭が&gt;ではなく+が表示される。Enterを押しても元に戻らない。 プログラムが不完全なことが原因である。1 +と中途半端な状態で入力したので、Rはプログラムの続きがあるものと思って入力を待っている状態なのである。プログラムの続きを入力すれば、結果が出力される。例えばこの例ならば、1を入力してEnterを押せば、答えである2が出力される。 他にもカッコの閉じ忘れなどでも、同じようなことが生じる。 なお、Esc（エスケープ）キーを押せば、プログラムを中止することができる。困ったときには、Escキーを押そう。 他にも、エラーが生じた場合は、エラーメッセージを読んで、プログラムの書き方に間違いがないかを確認しよう。たいてい、入力間違いなど大したことのないミスが原因である。ちょっとプログラムを間違えたくらいでRが壊れるということは決してないので、冷静に対処しよう。 2.4 単純な演算 四則演算（足し算・引き算・割り算・掛け算）をしてみよう。それぞれ、+、-、*、/を使う。 1 + 1 1 - 1 2 * 3 10 / 2 また、カッコ()内の演算が優先される。 (1 + 3)/2 累乗は^で計算できる。 2^3 2.4.1 コメント文 行頭に#を挿入すると、#から改行まではコメント文として理解され、プログラムが実行されない。スクリプトにメモを残しておきたいときに便利なので覚えておこう。 #1 + 1 #この部分は実行されない 2.5 変数 数値を変数に代入して扱うことができる。 x = 5 + 8 x ## [1] 13 y = x - 2 y ## [1] 11 なお、=の代わりに&lt;-を使っても良い。 x &lt;- 5 + 8 x ## [1] 13 2.5.1 変数の使い方の注意 Rは小文字と大文字を区別する。たとえば、a（小文字）と入力して実行すると結果が出力されるが、A（大文字）では出力されない。 a = 2 #aに2を代入する。 a - 2 #ゼロが出力されるはず。 A - 2 #Aでは答えが表示されない。Aという変数は作られていないので。 また、数値を全角で入力していないかにも注意すること。全角文字は数値ではなく、文字として認識される。数値は常に半角で入力すること。 x = 2 #半角の2 x = ２ #全角の２ 2.5.2 変数の型 Rでは変数の種類として、数値型、文字列、日付、論理型の区別をする。 数値型(numeric) 数値型として格納した変数は、数値として扱うことができる。数値型の変数同士で、演算（足し算・引き算・掛け算・割り算）を行うことができる。 x_num = 5 class(x_num) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; y_num = 1.2 class(y_num) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; x_num + y_num #数値型同士は演算することができる ## [1] 6.2 文字列(character) 文字として扱われる。文字列同士は演算をすることができない。 文字を変数として代入したい場合は、文字をクオテーションマーク(\"\")で囲む。 x_char = &quot;hello&quot; class(x_char) #class()でその変数の型を確認することができる ## [1] &quot;character&quot; y_char = &quot;1&quot; class(y_char) #数値でもクオテーションで囲めば文字列として扱われる。 ## [1] &quot;character&quot; 日付(date) Dateは日付のみを保存し、POSIXctは日付と時間を保存する。 日付型同士で日数や秒数などの演算をすることができる。 x_date = as.Date(&quot;2020-06-15&quot;) class(x_date) #class()でその変数の型を確認することができる ## [1] &quot;Date&quot; x_date_1 = as.POSIXct(&quot;2020-06-14 12:00&quot;) class(x_date_1) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; x_date_2 = as.POSIXct(&quot;2020-06-15 12:00&quot;) x_date_2- x_date_1 ## Time difference of 1 days 論理型(logical) TRUEかFALSEの2つの値のどちらかを取る変数の型である。 a = TRUE class(a) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; b = FALSE class(b) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; 論理式 数値の大小関係などを扱うときに用いる。 2 == 1 #2 と 1 は同じか？ ## [1] FALSE 2 != 1 #2 と 1 は同じではないか？ ## [1] TRUE 2 &lt; 1 #2 は 1 よりも小さいか？ ## [1] FALSE 2 &lt;= 1 #2 は 1 以下か？ ## [1] FALSE 2 &gt; 1 #2 は 1 より大きいか？ ## [1] TRUE 2 &gt;= 1 #2 は 1 以上か？ ## [1] TRUE 2.6 データ構造 複数の数値などをまとめたものをデータと呼ぶ。Rには、データを扱うための形式がいくつか用意されている。 2.6.1 ベクトル 同じ型の要素を集めたもの。Rでは、c()関数でベクトルを作成することができる。 x = c(1, 2, 3, 4, 5) x ## [1] 1 2 3 4 5 y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) y ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; ベクトル[x]の表記でカッコの中に数値を入れると、そのベクトルのx番目の要素を取り出せる。 x[2] ## [1] 2 y[3] ## [1] &quot;c&quot; カッコの中に条件式を入れると、その条件に当てはまる部分を取り出せる。 x[x &gt;= 3] #3以上の部分を取り出す ## [1] 3 4 5 y[y == &quot;c&quot;] #3cを取り出す ## [1] &quot;c&quot; y[y != &quot;c&quot;] #3c以外を取り出す ## [1] &quot;a&quot; &quot;b&quot; &quot;d&quot; &quot;e&quot; ベクトルが数値で構成されている場合は、演算をすることもできる。 x * 2 #ベクトル内の全ての要素に2を掛ける ## [1] 2 4 6 8 10 x_2 = c(6, 7, 8, 9, 10) x + x_2 #（ベクトルに格納されている変数の数が同じならば、ベクトル同士で演算ができる） ## [1] 7 9 11 13 15 2.6.2 データフレーム 複数のベクトルを行列でまとめたデータ構造を、Rではデータフレームと呼ぶ。データフレームは頻繁に使うので、構造を覚えよう。 まず、2つのベクトルを作成する。 x_vec = c(1, 2, 3, 4, 5) y_vec = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) 次に、以下のプログラムを実行して、データフレームを作る。data.frame()は、データフレームを作るための関数である。 以下では、ベクトルx_vecとy_vecをそれぞれ、xとyという名前にしてdatという名前の行列データを作成している。 dat = data.frame(x = x_vec, y = y_vec) dat ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e 2.6.2.1 変数の扱い 以下のように、データフレーム$変数名で、データフレームの変数をベクトルとして取り出すができる。 dat$x ## [1] 1 2 3 4 5 データフレームに新たに変数を加えることも出来る。 dat$x_2 = c(6, 7, 8, 9, 10) dat ## x y x_2 ## 1 1 a 6 ## 2 2 b 7 ## 3 3 c 8 ## 4 4 d 9 ## 5 5 e 10 dat$x_3 = dat$x + dat$x_2 dat ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 データフレーム$変数名でデータ内の変数にアクセスする方法は、今後もよく使うので覚えておこう。 2.6.2.2 データの抽出 データフレーム[行数,列数]のかたちで指定することで、データフレームの行列を取り出すことができる。 dat[1,2] #1行目, 2列目に該当する部分を抽出 ## [1] &quot;a&quot; dat[1,] #1行目を抽出（列を指定しなければ，データフレームの全ての列が抽出される） ## x y x_2 x_3 ## 1 1 a 6 7 dat[,1] #1列目を抽出（行を指定しなければ，データフレームの全ての行が抽出される） ## [1] 1 2 3 4 5 dat[1:3,] #1行目から3行目を抽出 ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 dat[c(1,3,5),] #1行目, 3行目, 5行目を抽出 ## x y x_2 x_3 ## 1 1 a 6 7 ## 3 3 c 8 11 ## 5 5 e 10 15 カッコ内に条件式を入れると、その条件と一致する部分を取り出せる。 dat[dat$x &gt; 2,] #xが2を超える行を抽出 ## x y x_2 x_3 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 dat[dat$x &gt; 2 &amp; dat$y != &quot;c&quot;,] #xが2を超え，かつyがc以外の行を抽出 ## x y x_2 x_3 ## 4 4 d 9 13 ## 5 5 e 10 15 2.7 欠損値 心理学ならば一部の質問への無回答など、データが取得できなかったケースも生じ得る。そのような場合には、データの一部を欠損値として扱う。 Rでは、欠損値はNAで扱う。 先程の例で作ったデータフレームdatに、欠損値を含むベクトルx_4を入れてみよう。 dat$x_4 &lt;- c(1, 2, NA, 4, 5) dat ## x y x_2 x_3 x_4 ## 1 1 a 6 7 1 ## 2 2 b 7 9 2 ## 3 3 c 8 11 NA ## 4 4 d 9 13 4 ## 5 5 e 10 15 5 dat$x + dat$x_4 ## [1] 2 4 NA 8 10 欠損値を含むベクトルは、計算に用いることができない。例えば、Rには平均値を計算するためのmean()という関数がある。しかし、欠損値を含むベクトルの場合は結果が出力されない。 mean(dat$x_4) ## [1] NA 関数によっては、欠損値を含むデータを使うときには欠損値の処理を指定する必要がある。例えば、mean()ならば、オプションとしてna.rm =TRUEを入れると欠損値を除いた上で平均値を計算してくれる。 mean(dat$x_4, na.rm = TRUE) ## [1] 3 2.8 パッケージのインストールとロード パッケージとは、Rの機能を拡張するためにインターネットからインストールして使うものである。 2.8.1 パッケージのインストール パッケージをインストールする。install.packages()で、インストールしたいパッケージを入力する。 ここではggplot2というパッケージをインストールするのを例として、パッケージのインストール方法について示す。 install.packages(&quot;ggplot2&quot;) もし「Please select a CRAN mirror …」というのが表示されたら、Japan (Tokyo)を選んで「OK」を押す。 2.8.2 パッケージのロード インストールしただけではパッケージを使うことができない。使う前にロードする必要がある。library()で、括弧内に使いたいパッケージ名を入力する。 library(ggplot2) 一度インストールしておけば、今後は最初にlibrary()でロードするだけで使うことができる。毎回インストールする必要はない。 マシンにインストールされているパッケージの情報は、RStudioの右下の「Packages」というタブから確認することができる。 パッケージは世界中で開発され、アップデートもなされている。RStudioならば同じく「Packages」の「Update」を選ぶことでアップデートすることができる。 2.9 データの読み込み 大抵の場合、データはExcelファイルなどに入力して使うことが多い。RでExcelなどの外部ファイルを読み込む方法として、以下の手順がある。 2.9.1 1: RStudioの機能を使って読み込む RStudioの場合、右上にある「Import Dataset」でデータの読み込みを行うことができる。 「From Text (base)」を選び、読み込むCSVファイルを選ぶ。 Nameには「dat」と入力、ファイルの1行目に変数名を入力している場合はHeadingは「Yes」を選ぶ。設定ができたら、「Import」を選ぶ。 2.9.2 2: プログラムを書いてデータを読み込む。 読み込みたいデータをデスクトップに置いた場合を例として、外部データの読み込み方について確認していく。 まず、ワーキングディレクトリを指定する必要がある（Rにデータのある場所を教える作業）。ワーキングディレクトリとは、「現在居る場所」のことである。 試しに、現在のワーキングディレクトリを確認しよう。以下のプログラムをコンソールに入力して実行する。 getwd() 出力された場所が、現在のワーキングディレクトリである。Rはワーキングディレクトリを起点として読み込むファイルを探す。読み込むファイルは、ワーキングディレクトリに置くことにしよう。 以下に、ワーキングディレクトリにデスクトップを指定する方法を例として説明する。 2.9.2.0.1 プログラムから指定する コンソールに以下のプログラムを直接書き込んで実行する。 #Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 #Macの場合 setwd(&quot;~/Desktop&quot;) #正しく設定されたかを確認する getwd() 2.9.2.0.2 RStudioの機能を使う RStudioならば、右の方にある「File」からデスクトップを表示し、「Set As Working Directory」を選ぶ。 他にも、メニューバーの「Session」から「Set Working Directory」、「Choose Working Directory」で指定することもできる。 ワーキングディレクトリの指定ができたら、プログラムを書いてデータを読み込む。 csvファイルの場合 read.csv()関数で読み込むことが出来る。 dat = read.csv(&quot;data.csv&quot;) #ファイル名をクオテーションで囲んで入れる。ここでは読み込んだデータを「dat」という名前で保存した。 dat #データの中身がコンソールに出力される Excelファイルの場合 readxlパッケージをインストール及びロードした上で、read_excel()を使う。 ＊readxl::read_excelは、readxlパッケージに入っているread_excelという関数を使うということを意味する（第4章で、また説明する）。 dat = readxl::read_excel(&quot;data.xlsx&quot;) dat 相対パス ワーキングディレクトリを起点として指定されるファイルの場所のことを相対パスという。 上記の例では、デスクトップ上に読み込みたいファイルを保存し、デスクトップをワーキングディレクトリに指定してデータを読み込んだ。しかし、例えばデスクトップにあるフォルダの中にデータを保存してあってそのファイルを読み込みたい場合、いちいちワーキングディレクトリを設定し直すのは面倒である。 このような場合、相対パスでファイルを指定するのが便利である。 「sample_data」のフォルダをダウンロードしてデスクトップに保存し、フォルダの中にある「0_sample.csv」を読み込んでみよう。 #デスクトップをワーキングディレクトリに指定する ##Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 ##Macの場合 setwd(&quot;~/Desktop&quot;) #デスクトップにあるsample_dataフォルダの中の「0_sample.csv」を読み込む dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) .（ピリオド）は、ワーキングディレクトリを意味する。/（スラッシュ）でフォルダの階層を区切り、ファイルを指定する。 2.9.3 サンプルデータ Rには予めサンプルデータがいくつか用意されている。このテキストでもところどころで、Rに入っているサンプルデータを使って解析の練習を行う。 iris #有名なフィッシャーのあやめデータ cars #自動車の速度と停止距離との関係 data() #data()で、入っているデータを確認できる 2.10 ヘルプ Rの関数など、使い方がわからない場合はhelp()でヘルプを参照することができる（英語）。RStudioならば、画面右の「Help」にヘルプが出力される。ヘルプにはプログラムの例も記されている。 help(mean) ?mean #?でもヘルプを表示させることができる。 ?iris 2.11 Rを終わらせる そのまま閉じてよい。 「Save workspace image?（作業スペースを保存しますか？）」が表示されるが、「いいえ」で良い。 確認問題 Rでのプログラムの書き方、データフレームの使い方、外部データの読み込み方について復習しよう。 問１ Rを使って以下のa, bを計算し、aとbの式どちらの方が答えが大きいかを確認せよ。 a: 1 × 1 × 2 × 2 × 3 × 4 × 5 × 9 × 8 × 7 × 6 b: 9 × 8 × 7 × 6 × 1 × 7 × 5 × 1 × 3 × 2 × 1 問２ ある細菌が、1分後に2個に分裂して増殖するとする。つまり、1個の細菌が1分後には2個に、2分後にはその2個が分裂して合計4個に、3分後にはその4個が分裂して合計8個になる。 1個の細菌は、30分後には何個になっているだろうか？ ヒント：aのx乗は^を使って求める（a^x） 問３ サンプルデータ「1_sample.csv」をデータフレームとして読み込もう。更に、データフレーム上の変数X, Yを用いて、Z = X - 3Y の新しい変数Zをデータフレームに追加しよう。 "],["02-summary.html", "Chapter 3 統計学の基礎の復習 3.1 尺度水準 3.2 基本統計量 3.3 相関(correlation) 確認問題", " Chapter 3 統計学の基礎の復習 平均値の計算など、心理統計の基礎を復習する。また、Rで平均値などの基礎統計量を計算する方法についても解説する。 変数の区別 基本統計 代表値（平均、中央値） 散布度（分散、標準偏差、分位数） 共分散、相関 3.1 尺度水準 まず、統計学における変数の種類の区別について確認する。変数の種類は大きく分けて、数値である量的変数とカテゴリーを意味するカテゴリカル変数（質的変数）の2つに区別できる。 3.1.1 量的変数 数値として扱う変数。計算することができる。量的変数は更に、間隔尺度と比率尺度に区別される。 間隔尺度 データの間隔に意味があるもの。ゼロが何もない状態を意味するものでないもの。例えば、摂氏温度など（0℃以下も-1℃があるように、ゼロは何もない状態を意味しない）。差には意味があるが、比率については意味を持たない。例えば、「10℃と20℃の差は10℃である」とはいえるが、「20℃は10℃の2倍の熱さである」とは言えない。 比率尺度 データの間隔に意味があるもの。ゼロがなにもない状態を意味するもの。例えば、身長、体重、絶対温度など。間隔を比率で表現できる。例えば、「体重100キロの人は体重50キロの人より2倍重い」といえる。 また、量的変数は離散値か連続値かでも区別できる。 離散値 離散値とは、小数の間隔を持たない数値のこと。例えば個数。1個, 2個、3個と数えるが、1.1個, 1.2個などは存在しない。 連続値 連続値とは、小数の間隔を持つ数値のこと。例えば身長。150cmから151cmの間には小数で表現できる数値が連続的に並んでいる。 3.1.2 カテゴリカル変数（質的変数） 分類や種類などを意味するデータ。数量化して計算することはできない。 カテゴリカル変数も更に、いくつかに分類される。ここでは、名義尺度と順序尺度の区別を挙げる。 名義尺度 性別（男、女）、血液型、出身地など。順序関係がないのが特徴である（男性&lt;女性といった関係はない）。 順序尺度 「優、良、可、不可」といった成績、「1. 賛成、2. どちらでもない、3. 反対」といった尺度など。心理学ではよくリッカート尺度（＊）で態度などを測定するが、これも順序尺度である。 ＊「1:そう思わない、2: 思わない、3: どちらでもない, 4: そう思う、5: 強くそう思う」といったように、自分の態度に当てはまる数字を選ばせて態度の強さを測定する方法。 順序尺度には順序関係があるが、間隔は定義されない。例えば、成績には優 &gt; 良 &gt; 可という順序関係があるが、優と良の間と良と可の間の幅は等しいといった定義はできない。また、良+可=優といった計算もできない。つまり、順序尺度は順序関係はあるがカテゴリカル変数である。 3.2 基本統計量 以下では、Rにもともと入っているirisデータをサンプルデータとして使いながら、平均値や中央値などの代表値や分散や標準偏差などの散布度について復習する。 irisと入力して実行すると、データの中身を確認できるが、これだとデータ全てが出力されて見やすくない。head()を使うとデータの変数を含めた上部数行を表示してくれる。また、str()を使うと、簡略化したデータの構成を表示してくれる。これらの関数は今後もよく出てくるのでここで覚えておこう。 head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 3.2.1 代表値 データの代表となる値のこと。平均や中央値などがよく用いられる。 平均値(mean) \\(n\\)個の数値\\(X_{1}, X_{2}, \\cdots, X_{n}\\)からなるデータの平均値\\(\\bar{X}\\)は、以下の数式で求める。 \\[ \\bar{X} = \\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\\\ \\] Rで平均値を求めるには、mean()関数を使う。カッコの中に、平均値を求めたいデータをベクトルの形式で入れる。 データフレームの場合は、「データ名$変数名」でベクトルを指定する。これはよく使う表記なので覚えておこう。 mean(iris$Sepal.Length) ## [1] 5.843333 データに欠損値が含まれている場合は、計算できない。オプションとしてna.rm = TRUEを指定すれば、欠損値を除いて平均値を算出してくれる。 a = c(1, 2, 3, NA, 4, 5) mean(a) ## [1] NA mean(a, na.rm = TRUE) ## [1] 3 中央値(median) データを小さい順から並べた場合、つまり、\\(X_{1} \\le X_{2} \\le ... \\le X_{n}\\)と並べた場合に、中央に位置する値を中央値という。 データの個数を\\(n\\)とした場合、\\(n\\)が奇数の場合は\\(X_{(n+1)/2}\\)、\\(n\\)が偶数の場合は\\((X_{n/2}+X_{n/2+1})/2\\)が中央値となる。 Rで中央値を求めたい場合は、median()関数が使える。mean()と同じく、カッコ内にデータをベクトルの形式で入れる。 median(iris$Sepal.Length) ## [1] 5.8 3.2.2 散布度 データの散らばり具合を示す値のこと。分散、標準偏差などが知られる。 分散(variance) 分散（\\(\\sigma^2\\)）は、以下の式で定義される。すなわち、各変数が平均値から離れている程度を表現したものである。 \\[ \\sigma^2 = \\frac{1}{n-1}\\sum_{k=1}^{n}(X_{k}-\\bar{X})^2\\\\ \\] Rで分散を求める際には、var()関数を使う。 var(iris$Sepal.Length) ## [1] 0.6856935 標準偏差(standard deviation) 分散の平方根が標準偏差（\\(\\sigma\\)）である。 Rで標準偏差を求める際には、sd()関数を使う。 sd(iris$Sepal.Length) ## [1] 0.8280661 分位数(quantile) データを小さい順から大きい順に並べ替えたときに、データを分割する値を分位数という。一般的に、四分位数（25%点、50%点[中央値]、75%点で分割した値）が報告によく使われる。 Rでは、quantile()で分位数を求められる。 quantile(iris$Sepal.Length) #デフォルトだと、0%, 25%, 50%, 75%, 100%が表示される） ## 0% 25% 50% 75% 100% ## 4.3 5.1 5.8 6.4 7.9 quantile(iris$Sepal.Length, probs = c(0.1, 0.3, 0.5, 0.8, 1.0)) #オプションのprobsで、分割する点を任意に指定することができる。 ## 10% 30% 50% 80% 100% ## 4.80 5.27 5.80 6.52 7.90 3.2.3 要約統計量 以上のように、Rには代表値や散布度を求めるための関数が標準で入っているが、これらをまとめて計算してくれるsummary()もある。 summary()に変数を入れると、最小値、最大値、平均、四分位数をまとめて算出してくれる。 summary(iris$Sepal.Length) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 3.2.4 グループごとの集計 グループごとに統計量を集計する場合は、tapply関数を使う。tapply()に、最初に統計量を求めたい変数のベクトル、グループを意味するベクトル、求めたい統計量を入れる。 例えば、サンプルデータのirisには、種を意味するSpeciesが入っている。種ごとに平均や標準偏差などを求める場合には、以下のようにプログラムを書く。 tapply(iris$Sepal.Length, iris$Species, mean) #平均 ## setosa versicolor virginica ## 5.006 5.936 6.588 tapply(iris$Sepal.Length, iris$Species, median) #中央値 ## setosa versicolor virginica ## 5.0 5.9 6.5 tapply(iris$Sepal.Length, iris$Species, sd) #標準偏差 ## setosa versicolor virginica ## 0.3524897 0.5161711 0.6358796 tapply(iris$Sepal.Length, iris$Species, length) #サンプル数 ## setosa versicolor virginica ## 50 50 50 3.2.5 表の作り方 カテゴリカル変数の場合、データ全体を把握するために頻度やパーセンテージを知りたいときが多い。 table()関数を使うと、頻度を集計して表にしてくれる。 table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 prop.table()を使うと、パーセンテージを求めてくれる。 tab_iris = table(iris$Species) #まず表を別の名前で保存する prop.table(tab_iris) #カッコの中に、表を入れる ## ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 3.3 相関(correlation) 2変数間の関連のことを相関という。相関の強さは相関係数で示される。 変数\\(x\\)と変数\\(y\\)の相関係数（\\(r\\)）は、以下の式で求められる。 \\[ r = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}} \\] \\(\\sigma_{xy}\\)はxとyの共分散(covariance)で、\\(\\sigma_{xy} = \\sum^n_{i=1}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\)である（\\(\\bar{x}\\)はxの平均）。\\(\\sigma_{x}\\)と\\(\\sigma_{y}\\)はxとyそれぞれの標準偏差である。 相関係数は\\(-1\\)から\\(1\\)までの範囲をとり得る。相関係数の絶対値が大きいほど、相関関係が強いことを意味する。相関係数\\(r\\)が \\(r &gt; 0\\)のときは「正の相関」、つまり一方の変数の量が増えればもう一方の変数も増える関係にあることを意味する。\\(r &lt; 0\\)のときは「負の相関」、つまり一方が増えればもう一方が減るという関係にあることを意味する。 Rで相関係数を求めるには、cor()が使える。カッコの中に、相関係数を求めたい2つの変数を入れれば良い。また、cor.test()を使うと、相関係数の検定などより詳細な結果を示してくれる。 cor(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.1175698 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 なお、共分散はcov()で求められる。 cov(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.042434 補足: 単に「相関係数」というと、一般的には「ピアソンの積率相関係数」のことを指す場合が多い。上で示した例も、ピアソンの積率相関係数を求めている。以降の章でも、特に断りがない限り「相関係数」は積率相関係数を指すものとする。 相関係数にもいくつか種類があり、他にも例えばスピアマンの順位相関係数などがある。順位相関係数は、変数を順位に変換した上で（大きい順から1, 2, …と順位を振る）その順位を使って相関係数を求めたものである。データに極端な値（外れ値）がある場合やデータ数が少ない場合は、外れ値を調整してくれる順位相関係数の方が望ましい。 cor()及びcor.test()のオプションで、method =で相関係数の算出法を指定することができる。 cor(iris$Sepal.Length, iris$Sepal.Width, method = &quot;spearman&quot;) ## [1] -0.1667777 何も指定しなければ、デフォルトでピアソンの積率相関係数を計算してくれる。 確認問題 irisデータを使って、代表値、散布度、相関係数の求め方を復習する。 問１ Petal.LengthとPetal.Widthそれぞれの平均値と標準偏差を求めよう。 問２ Petal.LengthとPetal.Widthの共分散を求めよう。 ヒント：cov()で2つの変数を入れると求まる。 問３ 問1で求めた標準偏差と、問2で求めた共分散を使って、2変数の間の相関係数を求めよう。 また、cor()でもPetal.LengthとPetal.Widthの相関係数を求め、値が一致することを確かめよう。 ヒント：相関係数の式を確認して、相関係数を求める。 "],["03-data.html", "Chapter 4 データ・ハンドリング 4.1 パッケージのロード 4.2 変数の作成 4.3 データの抽出 4.4 パイプ 4.5 グルーピング 4.6 データの変換 4.7 データの結合 4.8 データの読み込み 確認問題", " Chapter 4 データ・ハンドリング データに新しく変数を加えたり、データの形式を変えるなど、より高度で複雑なデータの操作について学んでいく。 この章では、tidyverseというパッケージに入っている関数を解説する。 変数の作成 データの抽出 パイプ グルーピング データの変換 データの結合 データの読み込み 4.1 パッケージのロード まず、この章で使うパッケージのロードをする（初めて使う場合は、マシンに予めパッケージをインストールする必要がある）。パッケージのインストール及びロードについては、第2章で解説している。 library(dplyr) library(tidyr) library(readr) library(tibble) library(readxl) 注意： 以降のプログラムでは、関数を「XXXX::YYYY」と表現しているが、これらは「XXXXパッケージに入っているYYYYという名前の関数を使う」ということを意味している。XXXX::の部分は基本的に省略しても問題ないが、例えばtidyverseパッケージ以外もロードしていて、同じ名前の関数が別のパッケージに含まれている場合には、思った通りの結果が表示されない場合もある。外部パッケージの関数を使う場合は、できる限りXXXX::を付けた方が無難である。 以降では、Rに標準で入っているirisデータを例として、ファイル操作の練習を行う。以下のプログラムを実行し、irisデータをdatという名前に置き換えて使っていこう。 dat = iris 4.2 変数の作成 dplyrパッケージに入っているmutate()を使うと、新たに変数を追加することができる。 mutate()に、データの名前、新しい変数の順番で入力すると、データの右端に新しい変数を追加してくれる。 dat2 = dplyr::mutate(dat, new_var = Sepal.Length + Petal.Length, hoge = ifelse(Species == &quot;setosa&quot;, 1, 0)) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species new_var hoge ## 1 5.1 3.5 1.4 0.2 setosa 6.5 1 ## 2 4.9 3.0 1.4 0.2 setosa 6.3 1 ## 3 4.7 3.2 1.3 0.2 setosa 6.0 1 ## 4 4.6 3.1 1.5 0.2 setosa 6.1 1 ## 5 5.0 3.6 1.4 0.2 setosa 6.4 1 ## 6 5.4 3.9 1.7 0.4 setosa 7.1 1 上の例では、Sepal.LengthとPetal.Lengthを足したnew_varという名前の新しい変数を作っている。更に、「Sepeciesが”setosa“ならば1, そうでなければ0とする」という条件で新たにhogeという変数を作っている。 4.3 データの抽出 dplyrパッケージに入っているselectや filter関数を使うと、データの中から必要な部分のみを取り出すことができる。 4.3.1 必要な列のみを取り出す（select） select()で、データの名前、取り出したい変数名（複数選択可）の順番で入力すると、指定した変数の列のみを取り出してくれる。 以下には、irisデータからSepal.LengthとPetal.Lengthのみを取り出す場合のプログラム例を示す。 dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) head(dat2) ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 上の例では、データの中からSepal.LengthとPetal.Lengthの列を取り出している。 4.3.2 条件に合う行を取り出す（filter） ある条件に合う行のみを取り出したい場合（例えばデータの中から男性のみを取り出したいなど）、filter()で、データの名前、条件式の順番で入力すると、データの中から条件に合う行のみを取り出してくれる。 以下には、irisデータから、あやめの種類（\"Species\"）のうち\"versicolor\"のみを取り出す場合のプログラム例を示す。 dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4.0 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor データから条件に合う行だけが取り出される。上の例では、「Speciesがversicolorである」行をdatから取り出している。 「イコール」は=ではなく、==と表記していることに注意。つまり、計算式と論理式ではイコールの表現の仕方が異なる。他の論理式の表現については、第2章で説明しているので確認しておこう。例えば、「Sepal.Lengthが7以上」という条件で取り出したいときは、dplyr::filter(iris, Sepal.Length &gt;= 7)とする。 4.4 パイプ 複数のプログラムをつなげることをパイプ処理という。purrrパッケージで（tidyverseパッケージをロードすれば自動で使える）、Rでパイプ処理をすることができる。 例えば、irisデータで「あやめの種類のうち\"setosa\"のみの行を取り出して、更にSpecies、Sepal.Length, Petal.Lengthのみの列を取り出したい」という複数の処理をする場合を例として考える。 先程まで学んだ内容で、以下のように複数のプラグラムを段階的に書けばできなくはないが、プログラムが非常に長くなる（プログラムを分けて書くと途中でミスも生じやすくなる）。 dat = iris #irisデータをdatという名前に置き換える dat2 = dplyr::filter(dat, Species == &quot;setosa&quot;) #まずSpeciesのうち、setosaのみを取り出す。dat2という名前で保存する。 dat3 = dplyr::select(dat2, Species, Sepal.Length, Petal.Length) #別の名前で保存し直したdat2から、Sepal.LengthとPetal.Lengthの列を取り出す。dat3という名前で保存する head(dat3) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 パイプ（%&gt;%）を使えば、このプログラムを1文で書くことができる。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::filter(., Species == &quot;setosa&quot;) %&gt;% dplyr::select(., Species, Sepal.Length, Petal.Length) head(dat2) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 %&gt;%はプログラムを渡していく関数であり、.はそのプログラム以前の結果を示している。irisデータをfilterに渡し、その結果をselectに渡している。 なお、ドットは省いても良い（以降でも、.は省略して表記する）。 dat2 = dat %&gt;% dplyr::filter(Species == &quot;setosa&quot;) %&gt;% dply::select(Species, Sepal.Length, Petal.Length) 4.5 グルーピング パイプを利用することで、グループごとに統計量（平均値や標準偏差など）を算出することができる。 irisデータを例として、グループごとに平均や標準偏差を計算する方法を覚えよう。 あやめの種類ごとに、がくの長さの平均値と標準偏差を算出してみる。 先ほど学んだパイプ処理（%&gt;%）に加え、dplyrパッケージのgroup_byとsummarise関数を利用する。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::group_by(Species) %&gt;% dplyr::summarise(Mean = mean(Sepal.Width, na.rm = TRUE), SD = sd(Sepal.Width, na.rm = TRUE), N = length(Sepal.Width)) dat2 ## # A tibble: 3 × 4 ## Species Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 group_by()はグループ変数を作成する関数である。データの中でグループとして使いたい変数を括弧内に指定する。 summarise()は、複数の関数を実行させる関数である。この例では、mean()、sd(), length()の3つの関数を実行し、それぞれの結果をMean, SD, Nという別の名前で保存している。 4.6 データの変換 tidyrパッケージに入っているpivot_longer()とpivot_wider()を使うと、データの並び替えなどをすることができる。 4.6.1 wide型とlong型の区別 まず、データのレイアウトには、wide型とlong型の二種類があることを理解しよう。 以下のデータを例として説明する。A, B, Cの3人の参加者が、X, Y, Z条件の３つの条件で実験課題を行ったとする。 それぞれの条件での課題の成績（数値）、参加者の性別、年齢をデータとして入力する。 まずは、以下のプログラムを実行してサンプルデータを作成しよう。 dat_wide = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_wide ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 このようにデータの入力方法として、１行につき１人の参加者の情報を入力するやり方がある（実験や調査でデータを入力する際も、このレイアウトの方が入力しやすいだろう）。このようなデータのレイアウトをwide型という。 同じデータを、以下のようなレイアウトで表現することもできる。同じく、以下のプログラムを実行して、サンプルデータを作ろう。 dat_long = data.frame(Subject = sort(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 3)), Condition = rep(c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;), 3), Score = c(6,9,7,2,3,5,7,4,7), Gender = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;, &quot;F&quot;), Age = sort(rep(c(18,19,20), 3))) dat_long ## Subject Condition Score Gender Age ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 実験成績ごとに１行ずつでデータが作られている。すなわち、同じ参加者1人につき3行のデータがある。このようなデータの方をlong型と呼ぶ。 どのデータ型にすべきか？ Rでデータ解析に用いる関数のほとんどは、「1つの観測値（observation）につき1行」が原則、つまりlong型でデータが入っていることを前提として作られている。このテキストで学ぶデータ解析も、基本的に分析で使うデータはlong型を前提とする。 その一方、データを入力するときなど、実務的にはwide型が扱いやすいということもあるだろう。データ入力は研究者の都合に応じてやりやすい方法で用意するとして、解析をする際に適切なデータ形式に変換するすべを身に着けておこう。 4.6.2 wide型からlong型に変換 tidyrパッケージのpivot_longer()を使う。 dat_long2 = dat_wide %&gt;% tidyr::pivot_longer(cols = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), names_to = &quot;Condition&quot;, values_to = &quot;Score&quot;) dat_long2 ## # A tibble: 9 × 5 ## Subject Gender Age Condition Score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A M 18 X 6 ## 2 A M 18 Y 9 ## 3 A M 18 Z 7 ## 4 B F 19 X 2 ## 5 B F 19 Y 3 ## 6 B F 19 Z 5 ## 7 C F 20 X 7 ## 8 C F 20 Y 4 ## 9 C F 20 Z 7 cols =で、並べ替える変数を指定する。names_to =で新しく作られるグループを意味する列の名前、values_to =で値を意味する列の名前を指定する。 4.6.3 long型からwide型に変換 tidyrパッケージのpivot_wider()を使う。 dat_wide2 = dat_long %&gt;% tidyr::pivot_wider(names_from = Condition, values_from = Score) dat_wide2 ## # A tibble: 3 × 6 ## Subject Gender Age X Y Z ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A M 18 6 9 7 ## 2 B F 19 2 3 5 ## 3 C F 20 7 4 7 names_from =で横並びにしたときの値のラベル名、values_from =で横並びの対象となる値を指定する。 4.7 データの結合 複数のデータを結合したい場合は、dplyrパッケージのjoin関数を使うとよい。join関数には、left_join, full_joinなど、いくつかの種類が用意されている。 サンプルデータを使いながら、手順について説明する。まず、以下のプログラムを実行して、サンプルデータを作ろう。 dat_sample = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) dat_sample ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 実験で3人の参加者A, B, Cについて、X, Y, Zのデータを取ったとする。 更に、2人の参加者（AとB）に追加で実験を行い、Wのデータを取ったとする。 dat_sample2 = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;), W = c(8,3)) dat_sample2 ## Subject W ## 1 A 8 ## 2 B 3 dat_sampleとdat_sample2のデータを結合して、一つのデータにしたい。 full_join()で結合したい2つのデータ、更に結合する際にキーとなる変数（2つのデータに共通して存在する変数）をby=で指定すると2つのデータを結合してくれる。 なお、by=を省くと、自動で2つのデータに共通する変数を見つけて、それを手がかりに結合してくれる。 dat_sample3 = dplyr::full_join(dat_sample, dat_sample2, by = &quot;Subject&quot;) dat_sample3 ## Subject X Y Z Gender Age W ## 1 A 6 9 7 M 18 8 ## 2 B 2 3 5 F 19 3 ## 3 C 7 4 7 F 20 NA full_join()だと、2つのデータをすべてつなげてくれる。データが含まれていない部分は、欠損になる（data_sample2に参加者Cのデータはないので、欠損になっている）。 left_join()だと、left_join()で左側に入力したデータを含む部分のみをつなげてくれる。 dat_sample3 = dplyr::left_join(dat_sample2, dat_sample, by = &quot;Subject&quot;) dat_sample3 ## Subject W X Y Z Gender Age ## 1 A 8 6 9 7 M 18 ## 2 B 3 2 3 5 F 19 4.8 データの読み込み Rにもともと入っているread.csv()を使えばcsvファイルを読み込むことができるが、readrパッケージの関数を使うと大量のデータが含まれるファイルも高速で読み込んでくれる。また、readxlパッケージの関数を使えば、Excelファイルも読み込んでくれる。 4.8.1 readr 様々な形式のファイルを高速で読み書きことを目標としたパッケージ。 dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) dat ## X Y Gender ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F dat_2 = readr::read_csv(&quot;./sample_data/0_sample.csv&quot;) dat_2 ## # A tibble: 5 × 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F #データの書き出し readr::write_excel_csv(dat_2, &quot;./sample_data/0_sample_2.csv&quot;) read.csv()ではなく、read_csv()なので注意（ドットではなく、アンダースコア）。 ファイルを書き出すための関数も用意されている。 ここではwrite_excel_csv() を使っているが、単にwrite_csv()でも可。 また、read_csv()で読み込んだファイルは、データフレームではなく、tibbleという形式になる。 tibble tibbleとは、Rのデータ形式の一つである。 データフレームの可読性を向上させたものが、tibbleである。コンソールにはデータ全てではなく、最初の10行程度のみ、列も画面に入る範囲のみが表示される。 as_tibble()でデータフレームをtibble形式にすることもできる。 dat = tibble::as_tibble(iris) #irisデータをtibble型にして、datという名前で保存する dat ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows tibbleだとデータをすべて閲覧することはできないが、すべて閲覧したい場合はView()を使えばよい。 View(dat) 4.8.2 readxl readxlパッケージで、エクセル形式（xlsx）のファイルを読み込むことができる。 dat = readxl::read_excel(&quot;./sample/0_sample.xlsx&quot;) dat 特にオプションを指定しなければ、1番目に保存されているシートの中身をtibble形式で読み込んでくれる。読み込みたいシートや読み込む範囲を指定したい場合など、細かい点についてはread_excelのヘルプを参照してほしい。 確認問題 問１ irisデータから、1)Speciesがversicolorである行を選び、2) Species, Petal.Length及びPetal.Widthの列を取り出し、3) Petal.LengthとPetal.Widthを足し合わせた変数hogeを作るという一連の処理を、パイプ処理を使ってプログラム1行でやってみよう。 問２ irisデータから、1)Speciesがvirginica以外の行を選び、2) Species, Petal.Length及びPetal.Widthの列を取り出す処理を、パイプ処理を使ってプログラム1行でやってみよう。 ヒント：Rでは、!=が「○○ではない」を意味する論理式である（第2章参照）。 問３ irisデータで、Species別にPetal.Lengthの平均値、標準偏差を求めよう。 ヒント：group_by()とsummarise()の使い方をおさらいする。 "],["04-graph.html", "Chapter 5 グラフ 5.1 なぜ可視化が重要か？ 5.2 Rのグラフィック関数 5.3 ggplot2パッケージ 5.4 オプション 5.5 図の保存 5.6 応用的なグラフ 5.7 その他の機能 確認問題", " Chapter 5 グラフ データをグラフで表現する方法について学ぶ。 データの傾向をグラフによって表現することを可視化(visualization)という。この章では、ggplot2パッケージを使って、データを可視化する方法について学んでいく。 まずは、ggplot2パッケージをロードする。 library(ggplot2) 5.1 なぜ可視化が重要か？ データを解析する際には、単に平均値など数値を確認するだけではなく、必ずデータの分布を確認する習慣を身に着けよう。 例えばデータに極端に高い値があったりする場合は、代表値として平均値を使うのは適切でないことがわかったりする。データが正規分布と違うかたちをしているか、データの中に外れ値はないかなど、分布の形状をデータ解析の前に確認しておくことは重要である。 5.2 Rのグラフィック関数 Rには標準でグラフを作成するための関数がいくつか用意されている。 5.2.1 plot plotはグラフを作る基本となる関数である。 #サンプルデータをつくる sample = data.frame( x = 1:5, y = c(2, 5, 7, 11, 9) ) sample ## x y ## 1 1 2 ## 2 2 5 ## 3 3 7 ## 4 4 11 ## 5 5 9 plot(x = sample$x, y = sample$y) plot()にx軸とy軸にプロットするデータをベクトルのかたちで入れると、図を作成してくれる。 更に、plot()関数では、自分の好みに合わせてグラフの体裁を変えることもできるオプションも用意されている。 5.2.1.1 オプションの指定 以下に例として、様々なオプションを変更したグラフを示す。 plot(x = sample$x, y = sample$y, type = &quot;b&quot;, #type: グラフの種類 pch = 2, #点の種類 cex = 1.5, #点の大きさ lty = &quot;dotted&quot;, #線の種類 lwd = 2, #線の太さ col = &quot;blue&quot;, #色 xlim = c(0,6), #x軸の範囲（最小値, 最大値） ylim = c(0,15), #y軸の範囲（最小値, 最大値） las = 1, #軸の文字の配置 main = &quot;Graph&quot;, #グラフのタイトル xlab = &quot;x value&quot;, #x軸のラベル ylab = &quot;y value&quot; #y軸のラベル ) type: グラフの種類を変える（“p” = 点（デフォルト）、 “l” = 線、 “b” = 点と線の両方、“n” = 何も描画しない、など）。 pch: 点の種類を指定（0 = 四角、1 = 丸（デフォルト）、2 = 三角、など）。 cex: 点の大きさを指定。数値を入力する。 lty: 線の種類を指定（“solid” = 実線（デフォルト）、“dotted” = ドット、“dashed” = ダッシュ、など） lwd: 線の太さの指定。数値を入力する。 col: 色を変更する。“red”, “blue”など色の名前を指定する。 xlim, ylim: それぞれx軸とy軸の最小値と最大値を指定。 las: 軸の文字の配置を指定（1 = x軸もy軸も水平方向）。 main, xlab, ylab: それぞれグラフのラベルを指定。 他にもどのようなオプションを指定できるか確認したい場合は、ヘルプを参照。?parsと入力するとヘルプが表示される。 5.2.2 その他のグラフ 以下では、Rで標準でに入っているサンプルデータirisで様々なグラフを作ってみよう。 head(iris) #データの上数行を表示して中身を確認する。 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 5.2.2.1 散布図 plot()で作成する。 plot(x = iris$Sepal.Length, y = iris$Sepal.Width) 5.2.2.2 ヒストグラム hist()で作成する。 hist(iris$Sepal.Length, breaks = 20, #breaksでビン（棒）の数を指定できる col = &quot;red&quot;, main = &quot;Histgram&quot;, xlab = &quot;Sepal Length&quot;, ylab = &quot;Frequency&quot;) #その他のオプションも指定可能 5.2.2.3 箱ひげ図 boxplot()で作成する。 boxplot(x = iris$Sepal.Length, main = &quot;Boxplot&quot;, xlab = &quot;Sepal Length&quot;) #その他のオプションも指定可能 5.2.3 グラフを重ねる 同じグラフの中でグループ別にデータを表示したい場合は、例えば以下のようにグラフを重ねて描画していくかたちで表現することができる。 以下に例として、irisデータのSpeciesごとに色や点の形を変えて散布図を示してみる。 まず、グラフの領域を作成して、その上にグループ別にpointやlinesなどで点や線を重ねていくかたちで描画していく。 #該当する部分を取り出してベクトルに格納する。 Sepal.Length_setosa &lt;- iris$Sepal.Length[iris$Species == &quot;setosa&quot;] Sepal.Width_setosa &lt;- iris$Sepal.Width[iris$Species == &quot;setosa&quot;] Sepal.Length_versicolor &lt;- iris$Sepal.Length[iris$Species == &quot;versicolor&quot;] Sepal.Width_versicolor &lt;- iris$Sepal.Width[iris$Species == &quot;versicolor&quot;] Sepal.Length_virginica &lt;- iris$Sepal.Length[iris$Species == &quot;virginica&quot;] Sepal.Width_virginica &lt;- iris$Sepal.Width[iris$Species == &quot;virginica&quot;] plot(0, 0, type = &quot;n&quot;, xlim = c(3.5, 8.5), ylim=c(0, 6), xlab = &quot;Sepal Length&quot;, ylab = &quot;Sepal Width&quot;)#まず、空のプロットを作る（軸の範囲やラベルを指定する） points(x = Sepal.Length_setosa, y = Sepal.Width_setosa, col = &quot;red&quot;, pch = 0) points(x = Sepal.Length_versicolor, y = Sepal.Width_versicolor, col = &quot;blue&quot;, pch = 1) points(x = Sepal.Length_virginica, y = Sepal.Width_virginica, col = &quot;orange&quot;, pch = 2) legend(&quot;topleft&quot;, legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), pch = c(0, 1, 2), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;orange&quot;)) #legendで凡例を重ねることもできる。 5.2.4 図の保存 #png形式で保存する場合 png(&quot;plot.png&quot;, height = 400, width = 400) #ファイル名を指定する。画像サイズも任意に指定できる（特に指定しなくても出力してくれる）。 plot(x = iris$Sepal.Length, y = iris$Petal.Length) #グラフを作る dev.off() #描画デバイスを閉じる RStudioならば、plotsウィンドウの「Export」を選んで保存することも可能。 5.3 ggplot2パッケージ 手っ取り早くデータの分布や相関を確認したい場合は、plotなどの関数を使えば十分である。plotなどでもオプションを駆使すれば研究報告向きのグラフを作れないこともないが、プログラムやオプションの指定の方法も複雑で、作成できるグラフの種類も少ない。 これに対し、様々なグラフを作る機能に特化したggplot2というパッケージがある。以降で、ggplot2の基本的な使い方について解説する。 早速、ggplot2を使ってグラフを作ってみよう。以下のプログラムを実行してみよう。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) p プログラムの解説： ggplot()：初期設定。「ggplot2を使ってグラフを書きます」という意味。必ず書く。カッコの中には何も入れなくて良い。 geom_xxxx()：グラフの種類の指定。必ず書く。xxxxには、グラフの種類を入力する。この例では、散布図を書くのでgeom_pointを指定した。更に、カッコの中に必要な設定を記す。 data =でグラフを描画するデータを指定する。 更に、aes()のカッコの中に描画に必要な要素を指定する。x=とy=でそれぞれ、 x軸とy軸に指定したい変数を指定する。点の大きさ、色、線の種類など、グラフの種類によって指定できる要素がある。 オプション：上での例では、labs()でx軸やy軸のラベルを指定している。他にも、軸の値の範囲、軸のラベル、グラフの色の設定などをオプションで指定することができる。オプションを加えなくてもグラフは出力される。 とりあえず、これらだけ知っておけばggplot2でグラフを作れる。 5.3.1 散布図 geom_pointで作成できる。 p = ggplot2::ggplot() + geom_point(data=mpg, aes(x=cty, y=hwy, shape = fl)) p この例では、shape =でグループを指定して、グループごとに点のかたちを変えた散布図を作成した。 点が重なって見えにくい場合は、geom_jitterを使うとランダムのズレを生成して表示してくれる。 p = ggplot2::ggplot() + geom_jitter(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 5.3.2 ヒストグラム geom_histogramで作成する。 p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length)) #xに、横軸にしたい変数を入れる。 p p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, fill = Species)) #種類ごとに色の塗りつぶしを変えたい場合は、fillに指定する。 p p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, color = Species)) #colorだと周りの線の色を変える。 p 5.3.3 箱ひげ図 geom_boxplotで作成する。 最小値、第一分位点、中央値、第三分位点、最大値を示す（外れ値は点で示される）。 p = ggplot2::ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot2::ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 5.3.4 バイオリンプロット データの分布を表現したグラフ。 geom_violinで作成する。 p = ggplot2::ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot2::ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 5.3.5 折れ線グラフ geom_line()を使う。geom_line()だけだと線のみだが、geom_point()で作ったグラフを重ねることで、点もつけることができる。このように、複数のグラフを重ねて描画することもできる。 #サンプルデータをつくる: 10日間の気温の変化 temperature = data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) temperature ## Days Celsius ## 1 1 17.2 ## 2 2 17.5 ## 3 3 18.1 ## 4 4 18.8 ## 5 5 19.0 ## 6 6 19.2 ## 7 7 19.7 ## 8 8 20.2 ## 9 9 20.5 ## 10 10 20.1 p = ggplot2::ggplot() + geom_line(data = temperature, aes(x=Days, y=Celsius)) + geom_point(data = temperature, aes(x=Days, y=Celsius)) p 5.3.6 エラーバーつきのグラフ geom_errorbar()でエラーバーをつけることができる。 あるいは、geom_pointrange()でも作れる。 #サンプルデータをつくる sample_dat = data.frame(Condition=c(&quot;A&quot;, &quot;B&quot; ,&quot;C&quot;), mean=c(2, 5, 8), lower=c(1.1, 4.2, 7.5), upper=c(3.0, 6.8, 9.1)) #meanが平均、lowerとupperにそれぞれ下限値と上限値。 p = ggplot2::ggplot() + geom_point(data = sample_dat, aes(x = Condition, y = mean)) + geom_errorbar(data = sample_dat, aes(x = Condition, ymax = upper, ymin = lower), width = 0.1) #まず、geom_pointで平均を点で示したグラフを作成する。そのグラフに、ymaxとyminにそれぞれ上限値と下限値を指定したエラーバーのグラフを重ねる（widthでエラーバーの横の長さを指定できる）。 p p2 = ggplot2::ggplot() + geom_pointrange(data = sample_dat, aes(x = Condition, y = mean, ymax = upper, ymin = lower)) #geom_pointrangeならば、点とエラーバーの両方を一括して指定できる。 p2 5.4 オプション 5.4.1 ファセット（Facet） グループごとにグラフを分けたい場合は、ファセット（facet）を利用すると良い。facet_wrap()を使う。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + facet_wrap(vars(Species)) p 5.4.2 ラベル（labs） x軸やy軸のラベルを変えたいときは、labsを使うと良い。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) 5.4.3 テーマ（Theme） theme()で、フォントの大きさや色などグラフのテーマも細かく変えることができる。具体的にどの部分を変えられるかは、theme()のヘルプで確認してほしい。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme(axis.title.x = element_text(size = 20)) #x軸のフォントサイズを変える p + theme(panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) #背景や枠線の色を変える 手っ取り早く一括でテーマを変えたい場合は、予め用意されている既存のテーマを選ぶと良い。theme_bw(), theme_classic()などが用意されている。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme_bw() p + theme_gray() p + theme_classic() 5.5 図の保存 ggplot2パッケージのggsave()で保存できる。plotに保存した図を、filenameにファイル名を指定すると、ワーキングディレクトリに作成した図が保存される。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Sepal Length&quot;, y = &quot;Petal Length&quot;) + theme_bw() ggplot2::ggsave(plot = p, filename = &quot;plot.png&quot;) #ファイル名を指定する。拡張子も忘れずにつける。 ggplot2::ggsave(plot = p, filename = &quot;plot_2.png&quot;, dpi = 300) #解像度（dpi）を指定可能。 ggplot2::ggsave(plot = p, filename = &quot;plot_3.png&quot;, width = 8, height = 5) #幅(width)や高さ(height)を指定可能（単位はインチ）。 5.6 応用的なグラフ 5.6.1 Rain-cloud plot 箱ひげ図、密度曲線、ドットプロットをあわせて表示したもの。データの分布がよりわかりやすい。 以下は、ggplot2とggdistパッケージを使って作図した例である。 library(ggdist) ggplot2::ggplot()+ ggdist::stat_halfeye(data = iris, aes(x = Species, y = Sepal.Length, fill = Species), width = 0.5,.width = 0, point_colour = NA, position = position_nudge(x = 0.2)) + geom_boxplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species), width=0.2, outlier.color = NA, position = position_nudge(x = 0)) + geom_jitter(data = iris, aes(x = Species, y = Sepal.Length), width = 0.05,alpha = 0.5) + coord_flip() + labs(y = &quot;Sepal Length&quot;, x = &quot;Species&quot;) + theme_bw() 5.6.2 Ridge plot 密度曲線をグループ別に示したもの。 以下は、ggplot2とggridgesパッケージを使って作図した例である。 library(ggridges) ggplot2::ggplot()+ ggridges::geom_density_ridges(data = iris, aes(x = Sepal.Length, y = Species, fill = Species), alpha = 0.5, scale = 1) + labs(x = &quot;Sepal Length&quot;, y = &quot;Species&quot;) + theme_bw() 5.6.3 Alluvial plot 複数のカテゴリの分類をフローの形式で表現したグラフ。ggalluvialパッケージを使って作図する。 library(ggalluvial) UCBAdmissions_2 = data.frame(UCBAdmissions) #サンプルデータUCBAdmissionsを使ってみる。データフレーム形式に修正する。 head(UCBAdmissions_2) #カテゴリ別に頻度を集計したデータフレームを用意する。 ## Admit Gender Dept Freq ## 1 Admitted Male A 512 ## 2 Rejected Male A 313 ## 3 Admitted Female A 89 ## 4 Rejected Female A 19 ## 5 Admitted Male B 353 ## 6 Rejected Male B 207 ggplot2::ggplot(data = UCBAdmissions_2, aes(axis1 = Gender, axis2 = Dept, axis3 = Admit, y = Freq)) + ggalluvial::geom_alluvium(aes(fill = Admit)) + ggalluvial::geom_stratum() + ggplot2::geom_text(stat = &quot;stratum&quot;, aes(label = after_stat(stratum))) + scale_x_discrete(limits = c(&quot;Gender&quot;, &quot;Dept&quot;, &quot;Admit&quot;)) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;) + theme_bw() 5.7 その他の機能 ggplot2の機能はまだまだある。このテキストで説明しきれていないことについては、関数のヘルプやRStudioのサイトにあるggplot2のCheat sheetを見てみよう（日本語訳もある）。 https://rstudio.com/resources/cheatsheets/ 確認問題 Rに標準で入っているmtcarsデータを使って、グラフを作成しよう。 問１ Rで標準で入っているhist()を使って、変数mpgのヒストグラムを作ろう。 問２ Rで標準で入っているplot()を使って、変数mpgと変数wtの散布図を作ろう。 問３ ggplot()で、変数mpgと変数wtの散布図を作ろう。グラフには以下の点を反映させること。 x軸をmpg、y軸をwtとする。 x軸のラベルは「Miles/gallon」、y軸のラベルは「Weight」とする。 テーマはtheme_classic()にする。 "],["05-probability_distribution.html", "Chapter 6 確率分布 6.1 確率変数と確率分布 6.2 一様分布 6.3 二項分布 6.4 正規分布 6.5 Rで使える確率分布関数 6.6 なぜ正規分布はよく使われるのか？ 6.7 確率分布の表現の仕方 6.8 その他の確率分布 確認問題 6.9 補足", " Chapter 6 確率分布 前章まではRの使い方について扱ってきたが、この章以降から本格的に統計解析を学んでいく。まずは、基礎統計学の復習として、「確率分布」を学んでいく。確率分布は統計学の基礎であると同時に、今後学んでいく統計モデルを理解する上でとても重要な知識である。 確率変数と確率分布 一様分布 二項分布 正規分布と中心極限定理 Rの確率分布関数 その他の確率分布 準備として、以下のプログラムを実行しよう。 この章でもggplot2パッケージを使うので、ロードしておく。 library(ggplot2) 更に、乱数の種を指定する。set.seed(1)と入力して実行する。このプログラムを実行しておくと、このテキストに書かれているプログラムの結果と同じ結果を再現できる。 set.seed(1) 6.1 確率変数と確率分布 まず、サイコロを例として、確率分布とは何かについて理解する。 サイコロを1個投げるとする。それぞれの目が出る確率は1/6である。それぞれの目を\\(X\\)（1, 2, 3, 4, 5, 6）、それぞれの目が出る確率を\\(P(X)\\)とする。\\(X\\)と\\(P(X)\\)を以下の表で示す。 このとき、Xを確率変数と呼ぶ。確率変数とは、その値と対応する確率が存在する変数のことをいう。表のように、確率変数とその変数が取り得る確率の分布を確率分布という。 確率分布には様々な種類がある。まず、一様分布、二項分布、ベルヌーイ分布について理解する。 6.2 一様分布 1個のサイコロを振ったとき、それぞれの目が出る確率はどれも1/6で等しい。このように、どの確率変数\\(X\\)についても常に一定の値の確率を取る確率分布は、一様分布(uniform distribution)と呼ばれる。 6.3 二項分布 コインを\\(n\\)回投げる。表が出る確率を\\(q\\)とすると、裏が出る確率は\\((1-q)\\)である。\\(n\\)回中表が\\(x\\)回出る確率\\(P(x)\\)は、以下の式で求められる。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] \\(x\\)を確率変数とした場合、上記の式の確率に従う確率分布を二項分布(binomial distribution)という。 つまり二項分布は、2つのカテゴリで表現されるある事象が何回生じるかの確率を表している。コイン（表か裏）を何回か投げたときに表が出る回数、学生の中から何人か選んだときの男の人数など。これらのような事象が生じる確率は、理論的には二項分布に従う。 例えば、コインの表が出る確率\\(q\\)を0.5とする。10回投げたときに表が6回出る確率を計算してみよう。Rならば、dbinom()関数を使えば計算できる（この関数の意味については、また後で説明する）。 #xは確率変数（コインの例でいうと表が出た回数）、sizeは試行回数（コインの例でいうとコインを投げた回数）、 dbinom(x=6, size=10, prob=0.5) ## [1] 0.2050781 #上の式n, x, pに実際に値を入れて計算する。dbinom()関数を使った場合と結果が一致することを確認しよう。 choose(10, 6) * 0.5^6 * (1 - 0.5) ^4 ## [1] 0.2050781 6.3.1 二項分布の期待値と分散 表が出る回数\\(x\\)が0〜10の全てについて、それぞれが生じる確率を計算すると以下のようになる。 dbinom(x=0:10, size=10, prob=0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 グラフにすると以下のようになる。横軸をx, 縦軸をP(x)とする。 d_plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) p = ggplot2::ggplot() + ggplot2::geom_bar(data = d_plot, aes(x = factor(x), y = p), stat=&quot;identity&quot;) + labs(y= &quot;P(x)&quot;, x= &quot;x&quot;) p グラフからもわかるように、表が出る確率が0.5のコインを10回投げたときに、最も出やすいのは10回中5回であることがわかる。10回中8回以上はほとんどまれであることがわかる。 この図からも、確率分布には最も出やすい変数（平均値。確率分布の場合は「期待値」と呼ぶ）と分散が存在することがわかる。 二項分布の期待値\\(E(x)\\)と分散\\(Var(x)\\)は、以下の式から計算できる。 \\[ E(x) = nq\\\\ Var(x) = nq(1-q) \\] この式から、表が出る確率が0.5（つまり、\\(q=0.5\\)）として、10回投げた場合(つまり, \\(n=10\\))における、表が出る回数の期待値と分散を計算してみよう。 #E(x) = nq 10*0.5 ## [1] 5 #Var(x) = nq(1-q) 10*0.5*(1-0.5) ## [1] 2.5 6.3.2 ベルヌーイ分布 二項分布で\\(n=1\\)のときは、ベルヌーイ分布(Bernoulli distribution)と呼ぶ。\\(x\\)は1か0を取る変数で、\\(x=1\\)のときの確率を\\(q\\)とすると、\\(x=1\\)及び\\(x=0\\)となる確率は以下の式で表される。 \\[ P(x=1) = q\\\\ P(x=0) = 1 - q\\\\ \\] 例： コインを一回だけ投げたときに、表が出るあるいは裏が出る確率。 サイコロを一回だけ振ったときに、1が出るあるいは1以外が出る確率。 dbinom(x=0:1, size=1, prob=0.5) #表が出る確率0.5のコインを投げた時に、表が出ない時の確率と表が出る時の確率が出力される。 ## [1] 0.5 0.5 dbinom(x=0:1, size=1, prob=1/6)#ある目が出る確率1/6のサイコロを振った時に、その目以外が出る確率とその目が出る確率が出力される。 ## [1] 0.8333333 0.1666667 6.4 正規分布 6.4.1 正規分布の基礎 統計学で用いられる確率分布の中でも有名なのは、正規分布(normal distribution)である。正規分布は、平均\\(\\mu\\)、標準偏差\\(\\sigma\\)を持つ確率分布で、釣鐘型（ベル・カーブ）の分布を描く。 平均\\(\\mu\\)、標準偏差\\(\\sigma\\)とする正規分布の確率密度関数\\(f(x)\\)は、以下の式から計算される（「確率密度関数」とは何かは、後で説明する）。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] 正規分布の式を覚える必要はない。式の中に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)が含まれていることだけ覚えておこう。また、正規分布はガウス分布（gaussian distribution）と呼ばれることもある。 試しに、平均0、標準偏差1の正規分布のグラフを作ってみよう。以下のプログラムではdnorm()という新しく出てきた関数もあるが、使い方については後で解説する。 x = seq(-3, 3, 0.05) # -3から3まで0.05刻みで数字の連続を作る y = dnorm(x=x, mean=0, sd=1) #平均0、標準偏差1の正規分布の確率を算出する dat_norm = data.frame(x = x, y = y) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_norm, aes(x = x, y = y)) p 6.4.2 確率密度関数 正規分布のグラフは確率分布を表しているのではなく、確率密度関数であることにも注意してほしい。先程の二項分布では、縦軸は、横軸の値（確率変数）が生じる「確率」そのものを意味していた。しかし、正規分布のグラフの縦軸は、横軸の値が生じる確率を意味しない。確率密度関数は、グラフの面積が確率を表す。 例えば、x = 0のときのyの値を確認しよう。 dnorm(x = 0, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 ## [1] 0.3989423 dnorm()は、確率密度関数の縦軸の値を出力する。x = 0のときの縦軸の値は0.4であり、先程のグラフからもわかるように、縦軸の値と一致している。しかし、x = 0が生じる確率は0.4ではない。確率密度関数の場合、縦軸の値そのものは何の意味も持たない。 二項分布のような離散型の変数である場合に対して、正規分布のように変数が連続型（小数点を含む値）であるものは、特定の値の確率を定義することができない。例えば、身長が170.5cmである確率を求めるにしても、170cmから171cmまでの間に,170.001cm, 170.002cmと無限の値が広がっている。170.5cmぴったりである確率を求めることはできず、連続型の変数において特定の値が生じる確率はゼロということになる。したがって、連続型の変数の確率については、個々の確率を求めるのではなく、確率密度関数によって区間の確率を求める方法が取られる。 x &lt; 0の範囲の面積を求めよう。正規分布の左半分なので、確率は0.5である。 pnorm(q = 0, mean = 0, sd = 1) #-∞からqの値までの範囲を求めてくれる。 ## [1] 0.5 6.4.3 正規分布の平均値と標準偏差 以下が、平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の値をそれぞれ変えた場合の正規分布である。赤が平均0で標準偏差1、青が平均1で標準偏差1, 黒が平均0で標準偏差2である。平均と標準偏差によって、正規分布の形状が変化することを理解しよう。 x = seq(-3, 3, 0.05) y_1 = dnorm(x=x, mean=0, sd=1) y_2 = dnorm(x=x, mean=1, sd=1) y_3 = dnorm(x=x, mean=0, sd=2) dat_norm_1 = data.frame(x = x, y = y_1) dat_norm_2 = data.frame(x = x, y = y_2) dat_norm_3 = data.frame(x = x, y = y_3) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_norm_1, aes(x = x, y = y), color=&quot;red&quot;)+ ggplot2::geom_line(data = dat_norm_2, aes(x = x, y = y), color= &quot;blue&quot;)+ ggplot2::geom_line(data = dat_norm_3, aes(x = x, y = y), color=&quot;black&quot;) p 6.5 Rで使える確率分布関数 ここまで、dbinom、dnorm, pnormなど、確率分布を扱う関数がいくつか出てきた。これらは、Rに標準で入っている関数である。 Rには確率分布から乱数を生成したり、確率変数の確率を求めることができる関数が実装されている。関数は、確率分布それぞれにrXXXX, qXXXX, pXXXX, dXXXXといった4種類の関数が用意されている（ XXXXには確率分布の種類が入る）。 rXXXXは、乱数(random number)を出力する。 rnorm(n = 10, mean = 0, sd = 1) #平均0、標準偏差1に従う正規分布から乱数を10個生成する ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 dXXXXは、確率変数xが生じる確率密度（確率密度関数の縦軸の値）を出力する。 dnorm(x = 0.5, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、x=0.5のときの確率密度を求める（確率密度の値であって、確率そのものではないので注意） ## [1] 0.3520653 qXXXXは、確率点(quantile)を出力する。ある確率を取る時のx軸の値を出力してくれる。 qnorm(p = 0.5, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、p ≤ 0.5の確率となるときの確率変数の値を求める ## [1] 0 pXXXXは、累積確率を出力する。 pnorm(q = 1, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、x ≤ 1の確率を求める ## [1] 0.8413447 他の確率分布についても同様に、4種類の関数が用意されている。 #乱数を作る関数 runif(n=100, min = 0, max = 1) #一様分布からの乱数 rbinom(n=100, size=10, prob=0.5) #二項分布からの乱数 6.6 なぜ正規分布はよく使われるのか？ なぜ正規分布は「正規（normal）」な分布なのか？よく言われるのは、「現実世界の様々な分布が正規分布に従う」からという説明である。 しかし、現実には正規分布に従わないデータの方が多い。体重や身長なども、実際には正規分布を描くことは少ない。年収なども釣鐘型の分布にならない。また、正規分布は確率変数が連続量の場合の確率分布である。心理学ではアンケートへの回答得点などを分析したりするが、離散値や順序尺度（すなわちカテゴリカル変数）が正規分布に従うという前提を置くのはそもそも適切ではない。 では、正規分布がよく使われる理由は何か。もっとらしい理由は、数学的な扱いやすさである。なぜならば、元の変数がどのような確率分布に従っていたとしても、変数を足し合わせた結果は正規分布に従うという都合の良い性質があるからである。この性質は、中心極限定理と呼ばれるものである。中心極限定理により、正規分布は身近に現れる「正規な分布」となり得るのである。 6.6.1 中心極限定理 中心極限定理とは、「母集団が平均及び標準偏差を持つ確率分布であるならば、たとえ母集団が正規分布でなくても、母集団から標本を無作為抽出して平均値を計算することを何回も繰り返すとその分布は正規分布に近づく」という定理である。 シミュレーションで中心極限定理を実感してみよう。6面のサイコロを100回振る実験を行うとする。 以下のプログラムを実行してみよう。runif()は一様分布から乱数を生成する関数である。round()は値を丸める関数で、以下では小数点以下の値を丸めて整数の値を出力するようにしている。 X = round(runif(n = 100, min = 1, max = 6),0) mean(X) ## [1] 3.5 それぞれの目が出る確率は1/6で一定である。すなわち、サイコロが出る目は一様分布に従う（つまり、元の分布は正規分布ではない）。一様分布の平均値は、最大値をa, 最小値をbとすると、(a+b)/2。つまり、サイコロの例の場合の平均値は理論的には(1+6)/2=3.5となる。 サイコロを100回振って平均値を求める。この平均値を求めるのを、1,000回繰り返す。求めた平均値1,000個の分布を見てみよう。 以下のプログラムで、シミュレーションとグラフの作成を行う。sapplyは、ある処理を繰り返し行う関数である。詳細な説明は省くが、以下のプログラムでは、100回サイコロを振って平均値を求める処理mean(round(runif(n = 100,min=1,max=6),0))を1,000回行ってその結果を保存して、sample.meansという名前のベクトルで保存している。 sample.means = sapply(c(1:1000), function(x) {mean(round(runif(n = 100,min=1,max=6),0))} ) hist(sample.means) ヒストグラムは正規分布に似ている。もっと回数を増やすと、より正規分布っぽいかたちになる。 #サイコロを7回振ってその合計を求める。これを10,000回行ったときの出目の合計値の分布 sample.sum = sapply(c(1:10000), function(x) {sum(round(runif(n = 7,min=1,max=6),0))} )#sum()はカッコ内のベクトルの要素を足し合わせる関数 hist(sample.sum) このように、元の母集団の分布がたとえ正規分布でなくても（この例の場合は一様分布）、その標本平均は正規分布に近似する。平均値を計算しなくても、単に値を足し合わせるだけでも同じある。 中心極限定理により、たとえ元の変数が正規分布に従っていなくても、変数を加算したものは正規分布に近似する。なので、心理学で行われる様々なデータ分析も、以下のような処理を行うことで変数が正規分布に従うという前提を置いて分析が行われる。 複数の質問項目（順序尺度）をまとめて平均化した心理尺度を分析に使う。 選挙への投票（「した」もしくは「しなかった」の二値）者の割合を県ごとに算出して、県を単位として分析に使う。 中心極限定理は簡単に言うと、「どのような確率分布に従う変数でも、足し合わせると正規分布に近づく」ということである（ただし、この章の補足でも述べているように例外もある）。 6.7 確率分布の表現の仕方 確率変数と確率分布の関係は、以下のように表現することもある。 二項分布の場合 \\[ x \\sim \\text{Binomial}(n, q) \\] Binomialのカッコ内のn, qのように、確率分布を構成する変数のことをパラメータ(parameter)と呼ぶ。 Binomialは二項分布、\\(\\sim\\)は「従う」という意味である。つまりこの式は、「\\(x\\)は、\\(n\\)と\\(q\\)をパラメータとする二項分布に従う」ということを示している。 正規分布の場合 \\[ x \\sim \\text{Normal}(\\mu, \\sigma) \\] 正規分布のパラメータは、平均\\(\\mu\\)と標準偏差\\(\\sigma\\)である。この2つが決まれば、正規分布の形状が決まる。 6.8 その他の確率分布 ここまで、確率分布として一様分布、二項分布、ベルヌーイ分布、正規分布があることを学んできた。他にも、確率分布はたくさんある。以下では、以降の章でも出てくる他の確率分布について説明する。 確率分布は大きく分けて、確率変数が離散値である離散型分布と連続値である連続型分布に分かれる。 6.8.1 離散型分布 確率変数が離散値（小数の値を取らないもの）である場合の確率分布である。ここでは、ポアソン分布について説明する。 ポアソン分布 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ x \\sim \\text{Poisson}(\\lambda)\\\\ \\] xは0以上の整数（0, 1, 2, 3, …）とする。 ポアソン分布 (poisson distribution)のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)、分散は\\(\\lambda\\)である。つまり、ポアソン分布は平均と分散が等しい分布である。 以下に、パラメータ\\(\\lambda\\)をそれぞれ変えた場合のポアソン分布を示す。 pois_1 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=1), lambda=1) pois_2 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=2), lambda=2) pois_3 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=3), lambda=3) pois = rbind(pois_1, pois_2, pois_3) ggplot() + ggplot2::geom_bar(data = pois, aes(x=factor(x), y=p, fill=factor(lambda)), stat=&quot;identity&quot;, color = &quot;black&quot;, position = &quot;dodge&quot;) + labs(y= &quot;P(x)&quot;, x = &quot;x&quot;, fill = &quot;lambda&quot;) 一定の期間中にランダムで生じる事象はポアソン分布に従う。具体的な例としては、1日の間に届くメールの件数、営業時間中に来る客の数など。 二項分布\\(Binomial(n, q)\\)の\\(n\\)が十分大きく、かつ\\(q\\)が小さい場合は平均を\\(np\\)とするポアソン分布に近似する。つまり、めったに起こらない事象はポアソン分布に従う。例えば、1年間の間に生じる交通事故の件数など（365日それぞれの日に0.1%で生じる場合など）。歴史的に有名な例として、「ドイツ軍で1年間で馬に蹴られて死亡した兵士の数」がポアソン分布に従うといったものがある。 二項分布と同じく、ポアソン分布の確率変数は離散値である。離散値は、1個、2個、3個と数える個数のような整数の値をいう（1.1個といった小数の値が存在しないもの）。それに対し、正規分布の確率変数は連続量である。 6.8.2 連続型分布 確率変数が連続値（小数の値が存在するもの）である確率分布である。正規分布に加え、他にも指数分布、対数正規分布、ガンマ分布などがある。 指数分布 指数分布(exponential distribution)のパラメータは、\\(\\lambda\\)である。指数分布の確率変数は、ゼロ以上の正の値のみを取りうる。 \\[ f(x) = \\lambda \\exp(-\\lambda x)\\\\ x \\sim \\text{Exponential}(\\lambda)\\\\ \\] ある一定期間の間に平均して\\(\\lambda\\)回生じるイベントの時間間隔は、指数分布に従う。災害が生じてから次の災害が生じるまでの期間、店に客が来てから次の客が来るまでの期間などは指数分布に従う。 対数正規分布 対数正規分布(log-normal distribution)は、正規分布と同様に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の２つのパラメータを持つ分布で、名前の通り正規分布とは深い関係にある。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}x}\\exp\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right)\\\\ x \\sim \\text{logNormal}(\\mu, \\sigma)\\\\ \\] 対数正規分布は、右の方に長い裾を描く分布である。対数正規分布に従うものとしては、年収の分布などが知られている。 ランダムな変数を足し合わせた物が正規分布に従うのに対し、ランダムな変数をかけ合わせたものは対数正規分布に従う。 #サイコロを7回振って、それぞれの値で掛け算をする。これを10,000回行ったときの出目の合計値の分布 sample.prod = sapply(c(1:10000), function(x) {prod(round(runif(n = 7,min=1,max=6),0))} ) #prod()はカッコ内のベクトルの要素をかけ合わせる関数 hist(sample.prod) 「ランダムな変数をかけ合わせたもの」の対数を取ってその分布を確認すると、正規分布のかたちになることがわかる。 sample.prod_2 = log(sample.prod) hist(sample.prod_2) 掛け算の対数を取るということは、足し算に直すことと同じである。つまり、対数を取ることで「ランダムな変数をかけ合わせたもの」が「ランダムな変数を足し合わせたもの」に変換される。「ランダムな変数を足し合わせたもの」は、中心極限定理により正規分布に従う。 したがって、対数正規分布の対数を取ったものは正規分布になる。 確認問題 問１ あなたは野球部の監督で、自分のチームの勝率はこれまでの練習の経験から32%だとわかっている。 これから遠征で、全部で10試合を行う予定である。 勝つ試合の回数をnとし、nとそれぞれのnに対応する確率を求めよ。 ヒント:dbinom()関数を使おう。 平均して何試合勝つことができるかを求めよ。 ヒント:二項分布の平均値の求め方を復習する。 問２ ある学校で小学6年生の身長を測ったところ、平均は150.2 cmで標準偏差が3.5 cmであった。 身長152 cmから155 cmの児童の割合はいくらか。 ヒント：pnorm()を使って求めよう。 身長158 cmを超える児童の割合はいくらか。 ヒント：同じく、pnorm()を使う。なお、全ての範囲の確率の合計は1である。 6.9 補足 6.9.1 中心極限定理の例外 中心極限定理より、元がどのような分布でも足し合わせると正規分布に近似するが、例外もある。例えば、コーシー分布（Cauchy distribution）と呼ばれる確率分布は、中心極限定理を適用できない。 コーシー分布は形状が正規分布に似ているが、裾が広いことが特徴である。つまり、極端な値が出る確率が正規分布よりも大きい。パラメータは、locationとscaleの2つである。以下に、locationが0、scaleパラメータが1と3の分布の例を示す。 dat_cauchy_1 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 1), scale = &quot;1&quot;) dat_cauchy2 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 3), scale = &quot;3&quot;) dat_cauchy = rbind(dat_cauchy_1, dat_cauchy2) ggplot() + geom_line(data = dat_cauchy, aes(x = x, y = y, linetype = scale)) + xlim(-6,6) + labs(x = &quot;y&quot;, y = &quot;density&quot;) コーシー分布は、極端な値が存在することにより、その分布の特徴を平均や標準偏差などで捉えることができない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) #location = 0, scale = 1のコーシー分布から乱数を100個生成する mean(d_cauchy) ## [1] -1.812771 median(d_cauchy) ## [1] -0.01332628 sd(d_cauchy) ## [1] 13.17875 コーシー分布から乱数を100個作って足し合わせることを5回やり、分布を確認する。足し合わせても、正規分布に近似しない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) hist(d_cauchy) "],["06-NHT.html", "Chapter 7 統計的仮説検定 7.1 準備 7.2 統計的仮説検定の考え方 7.3 統計的仮説検定の種類 7.4 統計的仮説検定で重要な概念 7.5 統計的仮説検定が抱える問題 確認問題", " Chapter 7 統計的仮説検定 統計的仮説検定の考え方とそれが抱える問題について理解する。 統計的仮説検定の考え方（p値とは何か？） 統計的仮説検定が抱える問題 7.1 準備 この章でも、ggplot2を使う。 library(ggplot2) 7.2 統計的仮説検定の考え方 前の章で学んだ二項分布を用いて、統計的仮説検定の考え方について学ぶ。p値とは何なのかを理解する。 7.2.1 二項分布の復習 コインを10回投げて表が出た回数\\(x\\)をカウントしていく。”理論的”には、表が\\(x\\)回出る確率\\(P(x)\\)は、コインを投げる回数\\(n\\)と表が出る確率\\(q\\)をパラメータとする二項分布に従う。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)}\\\\ x \\sim Binomial(n, q) \\] plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) ggplot2::ggplot() + ggplot2::geom_bar(data=plot, aes(x=factor(x), y=p), stat=&quot;identity&quot;) + labs(y = &quot;P(x)&quot;, x =&quot;x&quot;) 7.2.2 統計的仮説検定 ”理論的には”、表が出る回数\\(x\\)が生じる確率は上の図のようになる（平均は\\(nq = 10*0.5 = 5\\)）。 では、実際にコインを10回投げてみて表が出た回数を数えてみたところ、表が2回しか出なかったとする。この結果から、「このコインには歪みがあって、片一方の面だけが出やすい」と言ってもよいのか？ これを検討するために、表と裏それぞれが出る確率の等しいコインを投げる場合（すなわち、\\(q=0.5\\)の場合）との比較を行い、今回の実験結果がどれくらいまれな事象と言えるのかを比較する。 このとき、研究者が検証したい仮説を対立仮説（alternative hypothesis）、対立仮説を検証するために比較の対象とする「偏りを仮定しない」仮説のことを帰無仮説(null hypothesis)と呼ぶ。 では、今回の帰無仮説となる二項分布（2つのパラメータが、\\(n=10, q=0.5\\)の場合)の分布を見てみよう。理論的には、表が\\(x\\)回出る確率\\(P(x)\\)は、\\(x\\)それぞれについて以下のようになる。 d = data.frame(x=0:10, p_x =dbinom(x=0:10, size=10, prob=0.5)) d ## x p_x ## 1 0 0.0009765625 ## 2 1 0.0097656250 ## 3 2 0.0439453125 ## 4 3 0.1171875000 ## 5 4 0.2050781250 ## 6 5 0.2460937500 ## 7 6 0.2050781250 ## 8 7 0.1171875000 ## 9 8 0.0439453125 ## 10 9 0.0097656250 ## 11 10 0.0009765625 表もしくは裏が出る回数が2回以下の場合の確率を計算すると、 d$p_x[1] + d$p_x[2] + d$p_x[3] + d$p_x[9] + d$p_x[10] + d$p_x[11] ## [1] 0.109375 となる。つまり、もし歪みのないコインならば、片一方の面だけが出る回数が2回以下の確率はおおよそ0.11ということになる。 この例で求めた確率0.11のように、「帰無仮説の前提のもとで、特定の実験結果よりもまれな結果が得られる確率」をp値と呼ぶ。 p = 0.11 は小さい確率のように思える。なので、「歪みのないコインならば、一方の面が2回出る確率は本来0.11である。本来だったらあまり起こり得ない実験結果が得られたので、このコインは歪みのないコインであると結論づけるのは自然ではない。ゆえに、このコインには歪みがあって片一方の面が出やすい」という結論を出すのが妥当なように思える。 しかし、人によって0.11を小さいと評価しても良いのか、基準が分かれる。そこで、研究者の間でどこまでの数値を小さいと評価するかの基準が決まっている。この基準となる確率が、有意水準である。 一般的に有意水準には0.05（5%）とされることが多い。ただし、なぜ5％を判断基準とするのかについては特に明確な理由はない（みんなから合意されているからという以上の理由はない）。 つまり、「帰無仮説（歪みのないコインを投げる）の前提のもとでは、表が出る回数が2回以下の確率は0.11 であった。これは小さい確率のように思えるが、判断基準の5％よりかは大きい。すなわち、このコインはゆがんでいると結論付ける訳にはいかない」ことになる。 以上が、統計的仮説検定の考え方である。まとめると、 1)ある特定の理論分布（帰無仮説）のもとで今回の実験結果よりも珍しい結果が生じる確率（p値）を求め、 2)その確率が小さいかを評価し、 3)小さい場合は帰無仮説を棄却する というのが、統計的仮説検定のプロセスである。 今回のように「コインが表か裏かに関わらず、一方の面だけが出やすい」という対立仮説を検討する場合の検定は、両側検定という。仮に、今回の仮説で表と裏を区別するとして「表が出にくい」つまり「表が出る回数が2回以下の確率」を対象とする場合、このような検定を片側検定という。二項分布は左右対称の分布なので、両側p値は片側p値の2倍の値である(厳密には左右対称ではないのであくまで近似値)。多くの場合、両側検定を使うのが一般的である。 7.3 統計的仮説検定の種類 7.3.1 二項検定 コイン投げの例は、二項分布に従う事象である。二項分布に従う事象の統計的仮説検定は、二項検定と呼ばれる。 Rにも、二項検定を行うための関数binom.test()が用意されている。 binom.test()に二項分布のパラメータ（\\(n\\)と\\(q\\)にあたる数値）と実験結果を入れると、p値を求めてくれる。 上の例について、binom.test()でp値を求めてみよう。 binom.test(x = 2, n = 10, p = 0.5) #出てくる結果はデフォルトで両側検定になる。 ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 7.3.2 t検定 2つのグループの間で平均値に差があるかどうかの統計的仮説検定として、t検定を使うことが多い。連続量の変数を扱う検定の場合は、t検定がよく使われる。 t検定の考え方も、基本的に上と同じである。2つの集団の間で平均値に差がないと仮定したときの理論分布（t分布）と比べて、実際に得られた差の値がどれくらい珍しいのかを検討する。 例えば、母集団Aと母集団Bの平均値をそれぞれ\\(\\mu_{A}\\)、\\(\\mu_{B}\\)とする。帰無仮説は「\\(\\mu_{A} - \\mu_{B} = 0\\)」、対立仮説は「\\(\\mu_{A} - \\mu_{B} \\neq 0\\)」である。 2つの集団の標本平均の差が帰無仮説のもとの理論分布と比べて珍しいかを検討する。 ただし、実際に平均値の差を検討する際には、通常は理論分うとして正規分布ではなくt分布を扱う。 以下のサンプルデータを使って、平均値の差の検定をしてみよう。まず、以下のプログラムを実行する。 set.seed(1) Value = c(rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 10, mean = 1, sd = 1)) Treatment = c(rep(&quot;X&quot;, 10), rep(&quot;Y&quot;, 10)) sample_data = data.frame(Treatment = Treatment, Value = Value) head(sample_data) ## Treatment Value ## 1 X -0.6264538 ## 2 X 0.1836433 ## 3 X -0.8356286 ## 4 X 1.5952808 ## 5 X 0.3295078 ## 6 X -0.8204684 実験でXとYの２つの条件(Treatment)を設定し、ある値（Value）を測定したとする。 まず、2つの条件別にValueの平均値や標準偏差を求める。 tapply(sample_data$Value, sample_data$Treatment, mean) #グループごとに平均を求める ## X Y ## 0.1322028 1.2488450 tapply(sample_data$Value, sample_data$Treatment, sd) #グループごとに標準偏差を求める ## X Y ## 0.780586 1.069515 条件Yの方が条件Xよりも平均値が大きいよう見えるが、そう結論づけて良いのか。これをt検定で検討しよう。 まず、2つの集団間の平均値の差を元に、以下の式から「t値」を求める。 \\[ t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\sigma^2_{X}/n_{X}+\\sigma^2_{Y}/n_{Y}))}} \\] \\(\\bar{X}\\)と\\(\\bar{Y}\\)はそれぞれ条件Xと条件Yの平均値、\\(\\sigma^2_{X}\\)と\\(\\sigma^2_{Y}\\)はそれぞれ条件Xと条件Yの分散、\\(n_{X}\\)と\\(n_{Y}\\)はそれぞれ条件Xと条件Yのサンプル数である。 XとYが同じ正規分布\\(Normal(\\mu, \\sigma^2)\\)から抽出される場合、t値は自由度\\(n_{X}+n_{Y}-2\\)のt分布に従う。 t分布は、自由度によって分布が変化する（サンプルサイズの大小に応じて理論分布を調整することができる）。 x = seq(-3, 3, 0.05) y_t2 = dt(x = x, df = 2) #自由度2のt分布 y_t5 = dt(x = x, df = 5) #自由度5のt分布 y_t20 = dt(x = x, df = 20) #自由度20のt分布 dat_t2 = data.frame(df = 2,x = x, y = y_t2) dat_t5 = data.frame(df = 5,x = x, y = y_t5) dat_t20 = data.frame(df = 20,x = x, y = y_t20) dat_t = rbind(dat_t2, dat_t5, dat_t20) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_t, aes(x = x, y = y, color = factor(df))) + ggplot2::labs(x = &quot;t&quot;, y = &quot;value&quot;, color = &quot;Degree of freedom&quot;) + ggplot2::theme_bw() p 標本から得た差の値が理論分布のどこに位置するかを検討する。 Rに入っているt.test()関数を使うことで、２つの集団の間の平均値の差の検定を行える。 t.test(data = sample_data, Value ~ Treatment) #dataにデータの名前、比較の対象となる変数~グループを意味する変数とうかたちで入力すると結果が出力される。 ## ## Welch Two Sample t-test ## ## data: Value by Treatment ## t = -2.6669, df = 16.469, p-value = 0.01658 ## alternative hypothesis: true difference in means between group X and group Y is not equal to 0 ## 95 percent confidence interval: ## -2.0022169 -0.2310675 ## sample estimates: ## mean in group X mean in group Y ## 0.1322028 1.2488450 p値は0.02であった。これは5%よりも小さいので、「今回の結果が生じる確率はまれであり、XとYの母集団の平均値は等しいとする帰無仮説を棄却し、XとYは平均値が異なる集団を母集団とする」と結論づける。 t検定には、2つの標本の母集団の分散が等しいと仮定するかしないかで二種類の検定がある。母集団の分散が等しいと仮定しない場合の検定はウェルチの検定(Welch’s t-test)と呼ばれ、Rのt.test()関数でデフォルトで出る検定結果はこのウェルチの検定による結果である。一般的に2つの標本の母分散は不明であるので、それらが等しいかどうかも不明である。なので、等分散を仮定しないt検定をしておくほうが保守的である。 7.4 統計的仮説検定で重要な概念 7.4.1 第1種の過誤と第2種の過誤 「帰無仮説が真なのに、帰無仮説を棄却してしまう誤り」のことを、第1種の過誤（type Ⅰ error）という。つまり、「本当は差がないのに、”差がある”と判断してしまう誤り」のことである。 これに対し、「帰無仮説が偽なのに、帰無仮説を採択してしまう」誤りのことを、第2種の過誤（type Ⅱ error）という。つまり、「本当は差があるのに、”差がない”と判断してしまう誤り」のことである。 第1種の過誤を犯す確率\\(\\alpha\\)は、要は有意水準の値そのものである（\\(\\alpha=0.05\\)）。有意水準を高くする、すなわち「差があると判断する基準をゆるく」してしまうと、誤った仮説を採用してしまう恐れが増えてしまう。 第2種の過誤を犯す確率を\\(\\beta\\)と表現する。\\(1 - \\beta\\)は検定力と呼ばれ、検定力とは「帰無仮説が偽であるときに、正しく帰無仮説を棄却する確率」のことをいう。つまり、差があるときに、”差がある”と正しく判断できる確率である。統計的仮説検定では、この検定力をいかに高く保つかが重要となる。 第1種の過誤と第2種の過誤はトレード・オフの関係にある。第1種の過誤を避けようとして有意水準を小さくすれば（例えば\\(\\alpha=0.001\\)とする）帰無仮説の棄却が厳しくなり、逆に第2種のエラーを犯してしまう確率も高くなる（帰無仮説が偽であるにもかかわらず、棄却しない）。 7.5 統計的仮説検定が抱える問題 以下に、統計的仮説検定を行う上で留意すべき問題をいくつか示す。 7.5.1 p値と標本数の関係 p値は標本数に依存する。標本数が多くなるほどp値は小さくなる。 例えば、平均0, 標準偏差1の正規分布に従う母集団Aと平均0.1, 標準偏差1の正規分布に従う母集団Bからそれぞれ標本を抽出し、AとBの間で平均値に差があるかを検討する。帰無仮説は\\(\\mu_{A} = \\mu_{B}\\)である（母集団Aと母集団Bの平均値は等しい）。t検定でこの帰無仮説が棄却されるかを検定する。 実際の母集団の平均値の差は\\(|0-0.1|=0.1\\)である。 2つの集団はほとんど重なりあっていて、違いがなさそうに見える。 まずは、それぞれのグループで10個ずつ標本を抽出する。 set.seed(1) N = 10 g_A = rnorm(n = N, mean = 0, sd = 1) g_B = rnorm(n = N, mean = 0.1, sd = 1) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) t.test(data = d, value ~ group) ## ## Welch Two Sample t-test ## ## data: value by group ## t = -0.5174, df = 16.469, p-value = 0.6118 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -1.1022169 0.6689325 ## sample estimates: ## mean in group A mean in group B ## 0.1322028 0.3488450 p値は0.05よりも大きい。5%未満かどうかで有意な差があると判断するのならば、10人ずつ標本を抽出したこの結果からは、帰無仮説（\\(\\mu_{A}=\\mu_{B}\\)）を棄却することはできない。 次に、各グループそれぞれ1,000個標本を抽出して、t検定をしてみる。 set.seed(1) N = 1000 g_A = rnorm(n = N, mean = 0, sd = 1) g_B = rnorm(n = N, mean = 0.1, sd = 1) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) t.test(data = d, value ~ group) ## ## Welch Two Sample t-test ## ## data: value by group ## t = -2.0559, df = 1998, p-value = 0.03992 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.186376337 -0.004396124 ## sample estimates: ## mean in group A mean in group B ## -0.01164814 0.08373809 今度は、p値が0.05よりも小さい。1,000人ずつ標本を抽出したこの結果からは、帰無仮説（\\(\\mu_{A}=\\mu_{B}\\)）は棄却され、AとBとの間に平均値に有意な差があるという結論が導かれることになる。 しかし、実際にはAとBの間の差は0.1しかない。 このように、p値は標本数が大きくなるほど小さくなるという性質がある。実質的にあまり意味のない大きさの差でも、標本数を多く取れば「有意な差がある」と結論が出てしまう可能性がある。 異なるグループから標本を抽出したのならば、平均値に差が存在しないということはありえない。どんなに小さくても、差は存在する(差の大きさが0.00001でも)。小さい標本では、わずかな差は誤差として評価されて「有意差がある」という結論は導かれにくい。しかし、標本数を大きくすることで差を検出しやすくなる。 統計的仮説検定で検討しているのは、差の大きさ（効果の大きさ）ではないということには注意が必要である。 効果の大きさそのものを表す指標として、効果量（Effect size）というものも提案されている。詳細については別の文献を参照してほしいが、例えば、相関係数\\(r\\)も効果の大きさ（2変数の関連の強さ）を示す効果量の一つである。 7.5.2 第1種の過誤 今度は、同じ平均0、標準偏差1の正規分布に従う母集団AとBから20個ずつ標本を抽出しグループAとBの間で平均値に差があるかをt検定で検定する。帰無仮説は\\(\\mu_{A} = \\mu_{B}\\)である。 すなわち、帰無仮説が正しい前提のもとで抽出された標本について、検定の結果正しい結論（帰無仮説を棄却せずに採用する）が導かれるかを検討する。 有意差があるかどうかを判断する基準である有意水準は、5%（0.05）とする。 set.seed(1) N = 20 g_A = rnorm(n = N, mean = 0, sd = 1) g_B = rnorm(n = N, mean = 0, sd = 1) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) t.test(data = d, value ~ group) ## ## Welch Two Sample t-test ## ## data: value by group ## t = 0.69794, df = 37.917, p-value = 0.4895 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.3744327 0.7684235 ## sample estimates: ## mean in group A mean in group B ## 0.190523876 -0.006471519 \\(p&lt;.05\\)であり、帰無仮説は棄却されない。 では、グループA・Bそれぞれ20個ずつ標本を抽出して平均値の差があるかをt検定で検定することを100回やってみる。以下が、そのシミュレーションのプログラムである。 set.seed(1) d_sim = data.frame() for(s in 1:100){ N = 20 g_A = rnorm(n = N, mean = 0, sd = 1) g_B = rnorm(n = N, mean = 0, sd = 1) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) result = t.test(data = d, value ~ group) p = result$p.value temp = data.frame(s = s, p = p) d_sim = rbind(d_sim, temp) } \\(p&lt;.05\\)となった結果をカウントする。 d_sim$significant = ifelse(d_sim$p &lt; 0.05, &quot;significant&quot;, &quot;insignificant&quot;) table(d_sim$significant) ## ## insignificant significant ## 94 6 実際には帰無仮説が正しい（母集団の間で差がない）にも関わらず、標本抽出してt検定をするのを100回行うと、\\(p&lt;.05\\)となって帰無仮説を誤って棄却する結果が100回中6回確認された。 ggplot2::ggplot() + ggplot2::geom_point(data = d_sim, aes(x = s, y = p)) + ggplot2::geom_hline(yintercept = 0.05, linetype = &quot;dotted&quot;, color= &quot;red&quot;) + ggplot2::labs(x = &quot;Simulation&quot;, y = &quot;p&quot;) 有意水準を0.05と設定してしまうと、100回中5回は「帰無仮説が正しいにもかかわらず誤って帰無仮説を棄却してしまう誤り（第1種の過誤）」を許容することになる。 有意水準は慣例的に0.05と設定されているが、上記のシミュレーションのようにたまたま有意な結果が得られて間違った結論を導いてしまう恐れが存在する。 7.5.3 多重比較の問題 統計的仮説検定を繰り返すほど、差がなくても差があると評価してしまう確率（第1種の過誤を犯す確率）は増える。 例えば、5%水準で10回検定を行えば、少なくとも1回は帰無仮説を誤って棄却してしまう確率が0.4 になる。 1 - (1 - 0.05)^10 #全ての確率から、「10回検定を行って全て正しい判断を行う」確率を差し引いたものが、「少なくとも1回は誤った判断をしてしまう確率」 ## [1] 0.4012631 このように複数回検定を行うことを多重比較という。分散分析で3つ以上の条件間で平均値を比較するときなど、検定を複数行う場合は多重比較の補正が必要とされる（8章で触れる）。 確認問題 問１ 以下のプログラムを読み込む。 ある教授法に児童の学力向上の効果があるかを検討した。学校Bにはその教授法を実施し、学校Aには何もしなかった。その後、学校Aと学校Bそれぞれ10人の生徒に学力テストを行った。A、Bそれぞれが学校A、Bそれぞれの生徒の成績である（架空のデータである）。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(48, 40, 43, 56, 69, 53, 47, 41, 42, 91) Value = c(A, B) Treatment = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample = data.frame(Treatment = Treatment, Value = Value) str(sample) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Treatment: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Value : num 38 53 61 27 54 55 44 45 44 41 ... 学校Aと学校Bそれぞれについて、テストの得点の平均値及び標準偏差を求めて報告せよ。 この教授法に成績向上があったかどうかについてt検定（等分散を仮定しない）で検討し、結果について報告するとともに結論を述べよ。 ※t.test()関数を使う。等分散を仮定しない検定の場合は、特にオプションをしていしないでもよい。 "],["07-analysis.html", "Chapter 8 様々な解析法 8.1 t検定 8.2 分散分析 8.3 ノンパラメトリック検定 8.4 「統計モデル」との関係", " Chapter 8 様々な解析法 この章では、t検定、χ二乗検定、分散分析、ノンパラメトリック検定など、心理学の基礎統計で学んだ手法について、Rでの解析方法を見ながらおさらいしていく。 まず、この章で使うパッケージをロードする。 library(dplyr) 8.1 t検定 「分析の対象が量的変数で、２つのグループの間でその変数の平均値を比較する」ときには、t検定を使う。更に、t検定には2つのグループに対応があるかないかで、「対応のあるt検定」と「対応のないt検定」で区別される。 前の章でもみてきたように、Rにはt検定を行うためのt.test()関数が用意されている。 8.1.1 対応のないt検定 Rに標準で入っているsleepデータを使って、Rでt検定をやってみよう。 以下のプログラムを読み込み、サンプルデータを作る。 ttest_sample = sleep #ttest_sampleという名前保存する。 ttest_sample$ID = 1:nrow(ttest_sample) ttest_sample ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 11 ## 12 0.8 2 12 ## 13 1.1 2 13 ## 14 0.1 2 14 ## 15 -0.1 2 15 ## 16 4.4 2 16 ## 17 5.5 2 17 ## 18 1.6 2 18 ## 19 4.6 2 19 ## 20 3.4 2 20 IDは参加者を意味する番号で、1から20までの人がグループ1かグループ2のどれかに属し、変数extraを測定したとする。グループの間でextraに違いがあるかどうかを検討したい。 このように参加者が２つのグループのうちどれか一つに属しているケースが「対応のない場合」で、この場合は対応のないt検定で検討する。 前の章で見たように、t.test()関数で以下のように入力すれば結果が出力される。 t.test(data = ttest_sample, extra~group) ## ## Welch Two Sample t-test ## ## data: extra by group ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.75 2.33 8.1.2 対応のあるt検定 同じく、sleepデータを使って対応のある場合について解析をしてみる。 以下のプログラムを読み込み、サンプルデータを作る。 ttest_sample2 = sleep #ttest_sampleという名前保存する。 ttest_sample2 ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 今度は20名の参加者が、グループ1とグループ2の両方に属して、それぞれで変数extraを測定したとする。 このように同じ参加者が２つのグループのr兵法に属しているケースが「対応のある場合」である。 t.test()関数でオプションpaired = TRUEを入れれば、対応のあるt検定を実施できる。 t.test(data = ttest_sample2, extra~group, paired = TRUE) ## ## Paired t-test ## ## data: extra by group ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean difference ## -1.58 8.2 分散分析 t検定で比較できるのは2つのグループの間の平均値である。3グループ以上の間で平均値の比較を行いたい場合は、分散分析（ANOVA）を行う。 ここでは、一要因３水準の分散分析（単純に3つのグループの間で平均値を比較する）を例として、Rでの解析法について説明する。 Rでは、一要因の分散分析をするための関数aov()が標準で入っている。同じくRで標準で入っているPlantGrowthデータを使って解析をしてみよう。 anova_sample = PlantGrowth #PlantGrowthをanova_sampleという名前で保存する anova_sample ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl ## 7 5.17 ctrl ## 8 4.53 ctrl ## 9 5.33 ctrl ## 10 5.14 ctrl ## 11 4.81 trt1 ## 12 4.17 trt1 ## 13 4.41 trt1 ## 14 3.59 trt1 ## 15 5.87 trt1 ## 16 3.83 trt1 ## 17 6.03 trt1 ## 18 4.89 trt1 ## 19 4.32 trt1 ## 20 4.69 trt1 ## 21 6.31 trt2 ## 22 5.12 trt2 ## 23 5.54 trt2 ## 24 5.50 trt2 ## 25 5.37 trt2 ## 26 5.29 trt2 ## 27 4.92 trt2 ## 28 6.15 trt2 ## 29 5.80 trt2 ## 30 5.26 trt2 植物の生長を3つの条件で調べたデータである。 まず、３つのグループごとに平均値や標準を確認しよう。 anova_sample %&gt;% dplyr::group_by(group) %&gt;% dplyr::summarise(Mean = mean(weight), SD = sd(weight), N = length(weight)) ## # A tibble: 3 × 4 ## group Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.03 0.583 10 ## 2 trt1 4.66 0.794 10 ## 3 trt2 5.53 0.443 10 ctrl、trt1、trt2の間で平均値に下がるかを一要因の分散分析で検討する。以下のようにプログラムを書く。 result = aov(data = anova_sample, weight ~ group) summary(result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary()を使うと分散分析表が出力され、変数の効果が有意か否かを検討できる。 一要因の分散分析では「グループの間で平均値に差がない」という帰無仮説を検討する。この例の分析についてはp値は0.02であり、5%水準で帰無仮説は棄却されることとなる。これは「グループの間で平均値に差がない」という可能性は棄却されたが、どのグループの間に有意な差があるかはわからない。そこで、グループ１とグループ２、グループ２とグループ３、グループ１とグループ３との間、計３つの組み合わせで平均値の比較を行う。つまり、t検定を3回行って条件間の比較をする。 前の章でも触れたように、検定を繰り返すことは第１種の過誤を犯す確率を高めてしまう。そこで、検定を行う回数に応じてp値を厳し目に見積もるペナルティを課すことで、第１種の過誤を犯す確率を低くする工夫がなされる。この工夫が、「多重比較補正」と呼ばれるものである。 多重比較の補正を行うときは、pairwise.t.test関数を使う。各群間の比較について、補正後のp値が出力される。 Rでは、多重比較補正を行うための関数として、pairwise.t.test()がある。 pairwise.t.test(anova_sample$weight, g = anova_sample$group) #gにグループを意味する変数、p.adjust.methodに補正方法を指定する。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: anova_sample$weight and anova_sample$group ## ## ctrl trt1 ## trt1 0.194 - ## trt2 0.175 0.013 ## ## P value adjustment method: holm グループの組み合わせごとに、補正後のp値が表示される。これを見ると、trt1とtrt2との間で5%水準で有意な差があることがわかる。 分散分析は選択肢も多く、非常に複雑な解析方法である。上に述べたように、条件が複数ある場合は要因の効果が有意だったら、多重比較補正をして条件間で差の検定がなされる。多重比較補正にもボンフェローニ（Bonferroni)、チューキー(Tukey)、ホルム(Holm)の方法と、様々な方法が提案されている。 二要因の分散分析、三要因の分散分析、更には二要因対応あり一要因対応なしの分散分析と、要因の数やそれぞれの要因の対応ありなしで分散分析のやり方も非常に複雑になる！ 更には、平方和の値の計算方法もタイプ1, タイプ2、タイプ3と様々な選択肢がある。 このように複雑な分散分析であるが、統計モデルという考え方を導入すれば、解析のデザインを組みやすくなる。詳しくは次章以降でみていく。 8.3 ノンパラメトリック検定 以上のt検定や分散分析は、分析の対象の変数が「正規分布に従う」を前提とする解析手法である。変数が正規分布に従わない変数、例えば質的変数（人数の比率、順序尺度など）の場合には、t検定や分散分析を用いるのは適切でなく、「ノンパラメトリック検定」を使うべきだと指摘されている。 ノンパラメトリック検定とは「変数が正規分布に従うという前提を置かない解析手法」の総称であり、様々な種類が提案されている。 以下では、ウィルコクソンの順位和検定、クラスカル・ウォリスの検定、カイ二乗検定について触れる。 8.3.1 ウィルコクソンの順位和検定 2群間で値を比較するノンパラメトリック検定である。データを順位データに変換し、2群間でデータの大きさを比較する検定である。 「マン・ホイットニーのU検定」という名前でも知られる。 Rではwilcox.test()が用意されている。 wilcox_sample = airquality %&gt;% filter(Month &gt;= 8) #Rに入っているサンプルデータairqualityから2群だけ取り出したデータで試してみる wilcox.test(data = wilcox_sample, Ozone ~ Month) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Ozone by Month ## W = 552, p-value = 0.003248 ## alternative hypothesis: true location shift is not equal to 0 8.3.2 クラスカル・ウォリスの検定 3群以上で値を比較するノンパラメトリック検定である。同じく、データを順位データに変換して比較を行う。 Rではkruskal.test()が用意されている。 kruskal_sample = airquality #Rに入っているサンプルデータairqualityで試してみる kruskal.test(data = kruskal_sample, Ozone ~ Month) ## ## Kruskal-Wallis rank sum test ## ## data: Ozone by Month ## Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06 8.3.3 カイ二乗検定 解析の対象の変数が質的変数で、頻度に偏りがあるかを比較したい場合は、カイ二乗検定が使われる。 Rには、カイ二乗検定を行うためのchisq.test()がある。 tab = matrix(c(12, 30, 25, 16), ncol=2) #表を作成する chisq.test(tab) #chisq.testの中に表を入れる ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 7.5549, df = 1, p-value = 0.005985 カイ二乗検定は比率に偏りがあるかを検定してくれるが、どこに偏りがあるかは研究者自身が表を見て判断するしかない。 8.4 「統計モデル」との関係 この章では、心理統計で学んできた代表的な分析手法をRで行う方法について解説してきた。 一般的に心理統計では、「変数が正規分布に従う連続型でグループが２つならばt検定」、「変数が正規分布に従う連続型でグループが3つ以上ならば分散分析」、「変数がカテゴリカル変数ならばカイ二乗検定」といったように、変数のタイプや研究デザインに応じて行う解析を選ぶ方法が提案されている。 しかし、上に挙げた条件に当てはまらないデータの場合、当てはまる解析手法はあるだろうか？ 以降の章では、これまで学んできた分析手法を統計モデルという一つの枠組みで捉え直していく。 "],["08-linear_model.html", "Chapter 9 線形モデル 9.1 準備 9.2 線形モデルの概要 9.3 線形モデルによる解析 9.4 最尤法 9.5 信頼区間と予測区間 9.6 まとめ 確認問題", " Chapter 9 線形モデル これまで学んできた様々な統計手法（t検定、分散分析など）を、「線形モデル」という一つの枠組みで捉えていく。 この章のタイトルは「線形モデル」であるが、扱う内容は心理統計でも学んだ回帰分析である。 9.1 準備 データの可視化のために、ggplot2パッケージをロードする。 library(ggplot2) 9.2 線形モデルの概要 まず、線形モデルの表現の仕方を理解する。以下の式は、変数\\(x\\)から、変数\\(y\\)を予測するプロセスを記述したものである。変数\\(x\\)は予測変数（predictor variable）、変数\\(y\\)は応答変数（response variable）と呼ばれる。このように、応答変数と予測変数との関係を式で表現したものをモデルと呼ぶ。 予測変数は、「独立変数」や「説明変数」とも呼ばれる。応答変数は、「従属変数」や「被説明変数」とも呼ばれる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\tag{1}\\\\ y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] 1番目の式の右側に\\(\\alpha + \\beta x\\)という線形の式がある。この式は、線形予測子(linear predictor)と呼ばれる。変数\\(x\\)に係る\\(\\beta\\)は予測変数に係る傾き(slope)、\\(\\alpha\\)は切片(intercept)である。1番目の式は、変数\\(x\\)の持つ効果（傾き）及びそれ以外の効果（切片）と変数\\(y\\)の予測値（\\(\\hat{y}\\)）との関係を示している。 予測変数は2個以上でも構わない。予測変数の個数を\\(K\\)とすると、(1)の1番目の式は以下のように表現できる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x_{k} \\\\ \\tag{2} \\end{equation} \\] \\(y \\sim \\text{Normal}(\\hat{y}, \\sigma)\\)は、「応答変数\\(y\\)が、予測値\\(\\hat{y}\\)を平均、\\(\\sigma\\)を標準偏差とする正規分布に従う」ことを示している。つまり、線形予測子から予測された値\\(\\hat{y}\\)と誤差\\(\\sigma\\)から、実際の値\\(y\\)が推定されるプロセスを表現している。 応答変数が正規分布に従うという前提をおいたモデルのことを、一般的に線形モデル(linear model)と呼ぶ。 線形モデルは、基本的に心理統計でも学んだ「回帰分析(regression analysis)」と同じである。 応答変数\\(y\\)を決定づける変数、\\(\\alpha\\), \\(\\beta\\)、及び \\(\\sigma\\)はパラメータ(parameter)と呼ばれる。このパラメータを、既知の変数である\\(x\\)と\\(y\\)から推定する。 9.2.1 まとめ まずは、「応答変数」、「予測変数」、「（応答変数が従う）確率分布」、「線形予測子」、「傾き」、「切片」など、線形モデルを構成するキーワードを覚えよう。 線形モデルは、応答変数と予測変数の関係を線形の式で表したモデルである。 線形予測子の傾き、切片及び誤差（正規分布の標準偏差）を推定する。 予測変数が応答変数に及ぼす効果を推定することが、線形モデルの目的である。 9.3 線形モデルによる解析 実際に、Rで線形モデルの解析をしてみよう。 Rには線形モデルを扱える関数lm()がある。irisデータを使って解析をしてみよう。 Sepal.LengthとPetal.Lengthの関係を散布図で確認する。 p = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) p Sepal.Lengthが大きいほどPetal.Lengthが大きいという関係（正の相関）がありそうである。そこで、Sepal.Lengthの大きさから、Petal.Lengthの大きさを予測することを試みる。 lm()関数に、「応答変数~ 1 + 予測変数」のかたちで入力する。以下のプログラムを実行してみよう。 ＊1 +の部分は省略しても構わない。1 +は線形予測子の切片の部分を表している。省略しても、lm()は自動で切片の値を求めてくれる。 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) 結果をresultという名前でいったん保存した（名前はresult以外でも構わない）。summary()関数の中に、resultを入れて実行すると詳細な結果が出力される。 summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 色んな情報が出力されるが、まずは係数（Coefficients）の部分を見てみよう。ここでは、データから推定された切片や予測変数の傾きの結果が出力されている。 Interceptの部分が切片の推定結果である。各変数の名前の部分（ここではSepal.Length）が予測変数の傾きの推定結果を示している。 Estimateが推定された切片または傾きの値である。 Std.Errorは推定された係数の標準誤差である。 予測変数が応答変数に対して影響力を持っているか？ それは傾きの係数の推定結果からわかる。係数の値は、予測変数が1単位増えたら応答変数がどう変化するかを意味している。 係数がプラスならば、予測変数の値が増えると応答変数の値が増加する関係にあることを意味する。 係数がマイナスならば、予測変数の値が増えると応答変数が減少する関係にあることを意味する。 t value及びPrは係数の有意性検定の結果を示している（それぞれt値、p値）。ここでは、「係数がゼロである」という帰無仮説を検定している。p値が極端に低い場合は、「求めた係数の値は有意にゼロから離れている」と結論付けることができる。 この例の場合は、Sepal.Lenghtが1単位増えると、Petal.Lengthが1.86だけ上昇することを示している。 切片の値は-7.1となっているが、Sepal.Lenghtの値がゼロのときのPetal.Lengthの予測値を意味している（この例ではアヤメのがくの長さとしてありえない負の値になっているが、これは変数の標準化等を行っていないためである）。 Sepal.LenghtとPetal.Lengthの散布図に、線形モデルから推定された切片と傾きの値を持つ以下の直線を引いてみよう。 \\[ \\begin{equation} Petal.Length = -7.10 + 1.86 Sepal.Length\\\\ \\tag{3} \\end{equation} \\] p = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_smooth(data = iris, aes(x = Sepal.Length, y = Petal.Length), formula = y~x, method = &quot;lm&quot;, se = FALSE) p 線形モデルでは、直線の式で予測変数と応答変数の関係を表現する。実際のデータ（散布図の点）とのズレが最小になるような、直線の式を推定する（予測値と実測値とのズレが最小になるのが、最も良い予測である）。 9.4 最尤法 では、どうやって実測値と予測値とのずれが最小になるように傾きと切片の値を求めるのか？ 線形モデル（及び一般化線形モデル）では、パラメータの推定に最尤法（maximum likelihood method）という最適化手法が用いられる。 9.4.1 最尤法によるパラメータ推定 ここにコインが1枚ある。コインの表が出るかを決定づけるパラメータ（つまりコインを投げて表が出る確率）を\\(\\theta\\)（シータ）とする。この\\(\\theta\\)の値を何回かコインを投げる実験を通して推定する。 1回目は、表が出た。この時点で、この実験結果が生じる確率は\\(\\theta\\)である。 2回目は、裏が出た。1回目と2回目までの実験結果が生じる確率は\\(\\theta(1-\\theta)\\)である。 3回目は、表が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)\\theta\\)である。 その後、4回目は裏、5回目は裏だったとする。5回目で実験をストップすることにする。この実験結果が生じる確率\\(L\\)は以下のように表すことができる。 \\[ L = (1-\\theta)^3 \\theta^2 \\tag{4} \\] \\(L\\)のことを尤度(likelihood)と呼ぶ（”ゆうど”と読む）。 尤度とは「もっともらしさ」を示す概念である。イメージとしては、「今回の観測結果が得られる確率」である。今回の観測データに対して最も当てはまりが良くなる、すなわち尤度が最も高くなるときのパラメータを求めるのが、最尤法 (maximum likelihood method)と呼ばれる手法である。 掛け算を扱う尤度は計算が困難なので、実際のパラメータ推定の際には対数化して足し算を扱う。対数化した尤度を対数尤度(log-likelihood)と呼ぶ。対数尤度が最大となるパラメータ\\(\\theta\\)を求める。 \\[ \\log L = \\log(1-\\theta)+\\log(1-\\theta)+\\log(1-\\theta)+\\log(\\theta)+\\log(\\theta) \\tag{5} \\] 以下のプログラムで、上のコイン投げの例で最も対数尤度が高くなるときの\\(\\theta\\)を求めている。maximumが対数尤度が最も高くなるパラメータの値で、objectiveがそのときの対数尤度である。（プログラムの意味については理解しなくて良い） D = c(1, 0, 1, 0, 0) #観測データのベクトル：1=表、0=裏とする #対数尤度を求める関数 LogLikelihood = function(x){ return(function(theta){ L = 1 for(i in 1:length(x)){ L = L * theta^(x[i]) * (1-theta)^(1-x[i]) } return(log(L)) }) } #optimize関数で、対数尤度が最も高くなるパラメータthetaを推定する result_mlm = optimize(f = LogLikelihood(D), c(0, 1), maximum=TRUE) result_mlm ## $maximum ## [1] 0.4000015 ## ## $objective ## [1] -3.365058 パラメータ\\(\\theta\\)と対数尤度\\(\\log L\\)との関係を以下に示す。 #図示する theta = seq(0.01,0.99,0.01) logL = log(theta)+log(1-theta)+log(theta)+log(1-theta) + log(1-theta) data_mlm = data.frame(x=theta, y=logL) ggplot2::ggplot() + ggplot2::geom_line(data=data_mlm, aes(x=x, y= y), size=1) + ggplot2::geom_vline(xintercept = result_mlm$maximum, linetype=&quot;dashed&quot;, colour=&quot;red&quot;, size=1) + ggplot2::labs(x =&quot;theta&quot;, y= &quot;log L&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. 対数尤度が最も大きくなるのは、\\(\\theta=\\) 0.4のときである（表が出た割合である2/5と一致）。 ここではわかりやすくパラメータを1つのみ使って説明しているが、線形モデルの傾きや切片の推定も同じである。上の例の\\(\\theta\\)を線形予測子に置き換えて同様の計算をする。 線形モデルのときのパラメータ推定には、最小二乗法と呼ばれる別の推定方法もある。ただし、最小二乗法を使っても最尤法を使っても、線形モデルの傾きや切片の推定結果は同じになる。 9.5 信頼区間と予測区間 モデルでパラメータの推定を行ったあとは、そのモデルがデータを上手く予測できているかを確認することも重要である。 具体的には、パラメータの信頼区間(confidence interval)とデータの予測区間 (predictive interval)をチェックする。 信頼区間とは、パラメータが分布する区間のことをいう。今回得られた標本を用いて係数を推定したが、標本の元となる母集団の係数の値はどのくらいか？その母集団の係数の予想の範囲が、信頼区間である。 予測区間とは、標本がどの範囲に分布するかを予測する範囲のことをいう。新たな標本を取ったときに、そのデータがどの範囲に分布するか。その予想の範囲が予測区間である。 Rには、線形モデルの推定結果から信頼区間と予測区間を算出してくれるpredict()関数が用意されている。先ほどの線形モデルの解析結果を使って、信頼区間と予測区間を求めてみよう。 9.5.1 信頼区間 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) result_conf = predict(result, interval = &quot;confidence&quot;, level = 0.95) interval = \"confidence\"とすると、信頼区間を求めてくれる。 level = に信頼区間の幅を入力する（デフォルトで0.95だが、幅を変えたい場合は指定する）。 head(result_conf) ## fit lwr upr ## 1 2.376565 2.188121 2.565009 ## 2 2.004878 1.792226 2.217531 ## 3 1.633192 1.393955 1.872428 ## 4 1.447348 1.194160 1.700536 ## 5 2.190722 1.990526 2.390917 ## 6 2.934095 2.775149 3.093040 uprが95%信頼区間の上限、lwrが95%信頼区間の下限に当たる。 求めた信頼区間を図示してみよう plot_conf = cbind(iris, result_conf) #実測値のデータと予測値のデータを結合する。 ggplot2::ggplot() + ggplot2::geom_point(data = plot_conf, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_line(data = plot_conf, aes(x = Sepal.Length, y = fit)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 9.5.2 予測区間 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) new = data.frame(Sepal.Length = seq(4, 8, 0.1)) #0.1刻みで4から8まで範囲の数値ベクトルを入れたデータを仮に作る result_pred = predict(result, newdata = new, interval = &quot;prediction&quot;, level = 0.95) #newdataに先ほど作成した仮のデータを入れる。 head(result_pred) #仮データの数値に対応する予測区間が求められる ## fit lwr upr ## 1 0.3322885 -1.4165179 2.081095 ## 2 0.5181318 -1.2277203 2.263984 ## 3 0.7039751 -1.0390829 2.447033 ## 4 0.8898184 -0.8506063 2.630243 ## 5 1.0756617 -0.6622915 2.813615 ## 6 1.2615050 -0.4741389 2.997149 interval = \"prediction\"と入力する。 予測区間を図示してみよう。 plot_pred = data.frame(Sepal.Length = seq(4, 8, 0.1), result_pred) #予測区間のデータを作成する ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_line(data = plot_pred, aes(x = Sepal.Length, y = fit)) + ggplot2::geom_ribbon(data = plot_pred, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 実際のデータが予測区間の範囲に収まっているならば、そのモデルは概ねよくデータを予測できていることを示している。 9.6 まとめ この章では、線形モデルの概念について学んできた。次章では、線形モデルを扱う上で注意すべき点について見ていく。 確認問題 Rに入っているサンプルデータtreesを使って、線形モデルの結果の解釈の仕方とlm()関数の扱い方を復習をする。 head(trees) ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 問１ Heightを応答変数、Girthを予測変数として、切片と傾きの推定値を求めよ。 問２ Heightを応答変数、Volumeを予測変数として、切片と傾きの推定値を求めよ。 問３ 問2の推定結果から、Volumeが1単位増えるとHeightがどう変化するかを説明せよ。 "],["09-linear_model2.html", "Chapter 10 線形モデルの注意点 10.1 準備 10.2 線形モデルに含まれる統計解析 10.3 予測変数がカテゴリカル変数の場合 10.4 グループが複数ある場合 10.5 予測変数が複数ある場合 10.6 変数の標準化 10.7 線形モデルを扱う上での問題 10.8 モデルの予測力の評価 確認問題", " Chapter 10 線形モデルの注意点 前の章で、線形モデルの全体像を見てきた。 次に、線形モデルを扱う上で注意すべき点について見ていく。 10.1 準備 データの可視化のために、ggplot2パッケージをロードする。 library(ggplot2) 10.2 線形モデルに含まれる統計解析 線形モデルとは特定の解析を指すものではなく、正規分布を扱う様々な統計解析を包括的に扱う統計モデルである。例えば、基礎の統計学でも学んできたt検定や分散分析も線形モデルの中に含まれる。予測変数の種類や個数の違いによって、線形モデルは以下のそれぞれの統計解析と一致する。 分析 予測変数 予測変数の個数 t検定 二値(0 or 1) 1個 分散分析 二値 2個以上 共分散分析 二値及び連続量 二値が2個以上、連続量が1個以上 回帰分析 連続量（二値を含んでも可） 2個以上 10.3 予測変数がカテゴリカル変数の場合 前の章では、予測変数が量的変数の場合を例として扱ったが、予測変数はカテゴリカル変数（質的変数）でも構わない。ただし、予測変数がカテゴリカル変数の場合は、予測変数を0か1のどちらかの値を取るダミー変数(dummy variable)に変換する必要がある。 Rに入っているsleepデータを少し変えたもの使って、カテゴリカル変数を予測変数に含む線形モデルの解析をしてみよう。 dat = sleep #データを別の名前datに保存し直す #変数の名前を変える dat$x = ifelse(dat$group == 1, &quot;control&quot;, &quot;treatment&quot;) dat$y = dat$extra dat = dat[,c(&quot;y&quot;, &quot;x&quot;)] head(dat) #datの中身を確認する ## y x ## 1 0.7 control ## 2 -1.6 control ## 3 -0.2 control ## 4 -1.2 control ## 5 -0.1 control ## 6 3.4 control xはグループを意味する変数で、カテゴリカル変数である（統制群controlもしくは実験群treatment）。まずこれを、「treatmentなら1、controlなら0」とする新たな変数x_1を作る。 dat$x_1 = ifelse(dat$x == &quot;treatment&quot;, 1, 0) head(dat) ## y x x_1 ## 1 0.7 control 0 ## 2 -1.6 control 0 ## 3 -0.2 control 0 ## 4 -1.2 control 0 ## 5 -0.1 control 0 ## 6 3.4 control 0 ifelse()関数は、ifelse(XXX, A, B)と表記することで、「XXXの条件に当てはまればA、当てはまらなければB」という処理をしてくれる。ここでは、予測変数のベクトルxについて、treatmentならば1, それ以外なら0に変換し、0か1を取る変数\\(x_{1}\\)を新たに作った。 この\\(x_{1}\\)がダミー変数である。 解析に用いるモデルを確認すると、以下のようになる。 \\[ \\begin{equation} \\mu = \\alpha + \\beta x_{1} \\\\ \\tag{1}\\\\ y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] \\(x_{1}\\)は0か1のどちらかを取る変数で、\\(x_{1} = 0\\)のとき、つまり統制群のとき、応答変数の予測値は\\(\\mu = \\alpha\\)となる。\\(x_{1} = 1\\)のとき、つまり実験群のとき、応答変数の予測値は\\(\\mu = \\alpha + \\beta\\)となる。すなわち、切片\\(\\alpha\\)は統制群のときの効果、傾き\\(\\beta\\)は実験群の時に加わる実験群特有の効果を意味する。 lm()を使って、上のモデル式のパラメータの推定をしよう。 result = lm(data = dat, y ~ 1 + x_1) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x_1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.430 -1.305 -0.580 1.455 3.170 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7500 0.6004 1.249 0.2276 ## x_1 1.5800 0.8491 1.861 0.0792 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.899 on 18 degrees of freedom ## Multiple R-squared: 0.1613, Adjusted R-squared: 0.1147 ## F-statistic: 3.463 on 1 and 18 DF, p-value: 0.07919 2つの群間で平均値を比較するときにはt検定がよく使われる。t.test()関数を使って\\(x_{1}=0\\)と\\(x_{1}=1\\)との間で\\(y\\)の値の平均値を比較したときのt値及びp値の結果が、lm()の傾きのt値及びp値と一致することを確認しよう。 t.test(data = dat, y ~ x_1) ## ## Welch Two Sample t-test ## ## data: y by x_1 ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.75 2.33 lm()の傾きの検定は、「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは、予測変数\\(x_{1}\\)が1単位増えたときの応答変数\\(y\\)の変化量であった。傾きの検定は、「\\(x_{1}=0\\) から \\(x_{1}=1\\) に変化することによって、 \\(y\\) が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は、「\\(x_{1}=0\\)と\\(x_{1}=1\\)の間で\\(y\\)の値に差があるか」を検定しているのと論理的に同じである。 このように、予測変数が1つで、予測変数が二値（0もしくは1）であるときの線形モデルは、t検定に対応する。 10.4 グループが複数ある場合 先ほどの例は、統制群と実験群の二つのグループの場合であった。例えば実験で統制群、実験群1、実験群2といったように三つ以上のグループを設定した場合は、どうダミー変数を作成すればよいのか？ Rに入っているPlantGrowthを例として見ていこう。例えばやり方としては、以下の方法がある。 dat = PlantGrowth dat$y = dat$weight #名前をyに変える dat$x_c = ifelse(dat$group == &quot;ctrl&quot;, 1, 0) dat$x_t1 = ifelse(dat$group == &quot;trt1&quot;, 1, 0) dat$x_t2 = ifelse(dat$group == &quot;trt2&quot;, 1, 0) dat ## weight group y x_c x_t1 x_t2 ## 1 4.17 ctrl 4.17 1 0 0 ## 2 5.58 ctrl 5.58 1 0 0 ## 3 5.18 ctrl 5.18 1 0 0 ## 4 6.11 ctrl 6.11 1 0 0 ## 5 4.50 ctrl 4.50 1 0 0 ## 6 4.61 ctrl 4.61 1 0 0 ## 7 5.17 ctrl 5.17 1 0 0 ## 8 4.53 ctrl 4.53 1 0 0 ## 9 5.33 ctrl 5.33 1 0 0 ## 10 5.14 ctrl 5.14 1 0 0 ## 11 4.81 trt1 4.81 0 1 0 ## 12 4.17 trt1 4.17 0 1 0 ## 13 4.41 trt1 4.41 0 1 0 ## 14 3.59 trt1 3.59 0 1 0 ## 15 5.87 trt1 5.87 0 1 0 ## 16 3.83 trt1 3.83 0 1 0 ## 17 6.03 trt1 6.03 0 1 0 ## 18 4.89 trt1 4.89 0 1 0 ## 19 4.32 trt1 4.32 0 1 0 ## 20 4.69 trt1 4.69 0 1 0 ## 21 6.31 trt2 6.31 0 0 1 ## 22 5.12 trt2 5.12 0 0 1 ## 23 5.54 trt2 5.54 0 0 1 ## 24 5.50 trt2 5.50 0 0 1 ## 25 5.37 trt2 5.37 0 0 1 ## 26 5.29 trt2 5.29 0 0 1 ## 27 4.92 trt2 4.92 0 0 1 ## 28 6.15 trt2 6.15 0 0 1 ## 29 5.80 trt2 5.80 0 0 1 ## 30 5.26 trt2 5.26 0 0 1 3種類のダミー変数を作った。それぞれ、x_cは「ctrlならば1、それ以外なら0」、x_t1は「trt1ならば1、それ以外なら0」、x_t2は「trt2ならば1、それ以外なら0」となっている。これら3つのダミー変数を使ってモデルを作り、パラメータを推定する。 \\[ \\begin{equation} \\mu = \\beta_{c} x_{c} + \\beta_{t1} x_{t1} + \\beta_{t2} x_{t2} \\\\ \\tag{2} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] ここで注意が必要なのは、今回のモデルでは切片\\(\\alpha\\)が省かれていることである。その理由は後ほど説明する。 モデルをlm()で記述して、推定してみよう。以下のプログラムを実行する。 ＊lm(data = dat, y ~ x_c + x_t1 + x_t2 - 1)の中に-1が加わっている点に注意。これは「モデルから切片を除け」という命令である。 dat$y_2 = (dat$y - mean(dat$y))/sd(dat$y) #yを標準化しておく result = lm(data = dat, y_2 ~ x_c + x_t1 + x_t2 - 1) summary(result) ## ## Call: ## lm(formula = y_2 ~ x_c + x_t1 + x_t2 - 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52740 -0.59613 -0.00856 0.37472 1.95239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x_c -0.05847 0.28113 -0.208 0.8368 ## x_t1 -0.58757 0.28113 -2.090 0.0462 * ## x_t2 0.64604 0.28113 2.298 0.0295 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.889 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.1824 ## F-statistic: 3.231 on 3 and 27 DF, p-value: 0.03793 それぞれのダミー変数に係る傾きの係数、すなわち式(2)における\\(\\beta_{c}\\)、\\(\\beta_{t1}\\)、\\(\\beta_{t2}\\)の推定結果が出力される。それぞれ、ctrl、trt1, trt2における応答変数(y)の推定値を意味している。 x_t1の係数は負でp値は\\(p&lt;.05\\)に、x_t2の係数は正でp値は\\(p&lt;.05\\)となった。これらが意味していることは、「x_t1 = 1のときに、y_2の値は有意に-0.41下がる」と「x_t2 = 1のときに、y_2の値は有意に0.45上がる」ということである。言い換えれば、「実験条件1では全体の平均よりも植物の重量の値が低く」、「実験条件2では全体の平均よりも植物の重量の値が高い」傾向にあることを示している。 図でも条件別にy_2の分布を確認してみよう。分布を見ても同様の傾向があるが、線形モデルの解析の結果その効果が有意であることが確認できた。 ggplot() + geom_boxplot(data = dat, aes(x = group, y = y_2)) 10.4.1 基準となるグループと比較する もう一つの方法は、グループの数が\\(K\\)個あるのならば、基準となるグループを定めてダミー変数を\\(K-1\\)個作る方法である。 以下のプログラムを実行して、データを作り直そう。 dat = PlantGrowth dat$y = dat$weight #名前をyに変える dat$x_t1 = ifelse(dat$group == &quot;trt1&quot;, 1, 0) dat$x_t2 = ifelse(dat$group == &quot;trt2&quot;, 1, 0) dat ## weight group y x_t1 x_t2 ## 1 4.17 ctrl 4.17 0 0 ## 2 5.58 ctrl 5.58 0 0 ## 3 5.18 ctrl 5.18 0 0 ## 4 6.11 ctrl 6.11 0 0 ## 5 4.50 ctrl 4.50 0 0 ## 6 4.61 ctrl 4.61 0 0 ## 7 5.17 ctrl 5.17 0 0 ## 8 4.53 ctrl 4.53 0 0 ## 9 5.33 ctrl 5.33 0 0 ## 10 5.14 ctrl 5.14 0 0 ## 11 4.81 trt1 4.81 1 0 ## 12 4.17 trt1 4.17 1 0 ## 13 4.41 trt1 4.41 1 0 ## 14 3.59 trt1 3.59 1 0 ## 15 5.87 trt1 5.87 1 0 ## 16 3.83 trt1 3.83 1 0 ## 17 6.03 trt1 6.03 1 0 ## 18 4.89 trt1 4.89 1 0 ## 19 4.32 trt1 4.32 1 0 ## 20 4.69 trt1 4.69 1 0 ## 21 6.31 trt2 6.31 0 1 ## 22 5.12 trt2 5.12 0 1 ## 23 5.54 trt2 5.54 0 1 ## 24 5.50 trt2 5.50 0 1 ## 25 5.37 trt2 5.37 0 1 ## 26 5.29 trt2 5.29 0 1 ## 27 4.92 trt2 4.92 0 1 ## 28 6.15 trt2 6.15 0 1 ## 29 5.80 trt2 5.80 0 1 ## 30 5.26 trt2 5.26 0 1 今度は、ダミー変数は2つで各条件を表している。ctrlのときは「x_t1 = 0, x_t2 = 0」,trt1のときは「x_t1 = 1, x_t2 = 0」,trt2のときは「x_t1 = 0, x_t2 = 1」となる。 これら2つのダミー変数を予測変数として、lm()でyを推定しよう。ただし、今度は切片\\(\\alpha\\)を入れたモデルで推定する。モデルは以下のようになる。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{t1} x_{t1} + \\beta_{t2} x_{t2} \\\\ \\tag{3} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] dat$y_2 = (dat$y - mean(dat$y))/sd(dat$y) #標準化する result = lm(data = dat, y_2 ~ x_t1 + x_t2 + 1) summary(result) ## ## Call: ## lm(formula = y_2 ~ x_t1 + x_t2 + 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52740 -0.59613 -0.00856 0.37472 1.95239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.05847 0.28113 -0.208 0.8368 ## x_t1 -0.52910 0.39758 -1.331 0.1944 ## x_t2 0.70451 0.39758 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.889 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 式（3）より、切片の推定値は\\(x_{t1}=0\\)かつ\\(x_{t2}=0\\)のときの\\(\\mu\\)、つまり統制群(ctrl)のときの応答変数\\(y\\)の推定値を意味している。各ダミー変数の係数（傾き）は、切片に加わる各条件の効果を意味している。例えば、x_t2の係数は0.49であるが、これは\\(x_{t2}=1\\)のとき（つまりtrt2のとき）の応答変数の予測値は、 5.03 + 0.49 = 5.52となることを示している。 このように、グループが\\(K\\)個ある場合（\\(K &gt; 2\\)）、\\(K-1\\)個のダミー変数を作って推定する方法もある。係数の意味することは、基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。 このように、モデルを組み直すことにより、係数が意味することも変化してくる。モデル（式）を確認しながら、係数が何を意味しているのかを常に意識するようにしよう。 10.5 予測変数が複数ある場合 先の例や前の章でも見たように、予測変数は2つ以上入れても良い。予測変数が複数ある場合の注意点を見ていく。 10.5.1 変数の効果の統制 予測変数を複数加えた線形モデルの解析のメリットは、ある予測変数について他の予測変数の効果を統制(control)したときの効果を検討できることにある。 Rで標準で入っているattitudeデータを使って、予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。 head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 以下のように、complaints, privileges, learning, raisesの4つを予測変数として、ratingの値の推定を行ってみよう。 result = lm(data = attitude, rating ~ 1 + complaints + privileges + learning + raises) summary(result) ## ## Call: ## lm(formula = rating ~ 1 + complaints + privileges + learning + ## raises, data = attitude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.2663 -5.3960 0.5988 5.8000 11.2370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.83354 8.53511 1.386 0.178 ## complaints 0.69115 0.14565 4.745 7.21e-05 *** ## privileges -0.10289 0.13189 -0.780 0.443 ## learning 0.24633 0.15435 1.596 0.123 ## raises -0.02551 0.18388 -0.139 0.891 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.996 on 25 degrees of freedom ## Multiple R-squared: 0.7152, Adjusted R-squared: 0.6697 ## F-statistic: 15.7 on 4 and 25 DF, p-value: 1.509e-06 切片（Intercept）は全ての予測変数の値がゼロのときの応答変数の予測値であり、各予測変数の係数は予測変数が1単位増えた場合の応答変数の変化量を意味している。例えば、complaintsの係数は0.69であるが、これは「complaintsが1増えるとratingは0.69増える傾向にある」ことを意味している。 各係数の値は「他の変数の値がゼロであるときの効果」を意味している。先程のcomplaintsの係数0.69は、その他の予測変数privileges, learning, raisesがゼロのときの、complaintsがratingに与えるそのものの効果を示している。 このように複数の予測変数を入れたモデルで推定される係数は、他の予測変数の効果を統制した上での予測変数が応答変数に及ぼす効果を意味する。 10.5.2 交互作用 以下のプログラムを実行して、サンプルデータdを作ろう。 set.seed(1) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.1 + 0.4 * x y = rnorm(n = 20, mean = mu, sd = 1) d_M = data.frame(x = x, y = y, gender = &quot;M&quot;) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.3 + -0.6 * x y = rnorm(n = 20, mean = mu, sd = 1) d_F = data.frame(x = x, y = y, gender = &quot;F&quot;) d = rbind(d_M, d_F) head(d) ## x y gender ## 1 3 2.811781 M ## 2 4 2.089843 M ## 3 6 1.878759 M ## 4 9 1.485300 M ## 5 3 2.424931 M ## 6 9 3.655066 M このデータdには、x, y, genderの3つの変数が含まれている。genderは性別を意味する変数とする。M（男性）かF（女性）のいずれかである。男女別に、実験で2つの変数を測定したとしよう。 応答変数をy、予測変数をxとして線形モデルで切片及びxの傾きのパラメータを推定する。モデルは以下のようになる。 \\[ \\begin{equation} \\mu = \\alpha + \\beta x \\\\ \\tag{4} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] lm()関数を使って推定しよう（\\(x\\)と\\(y\\)の散布図及び係数の信頼区間も図示する）。 result = lm(data = d, y ~ 1 + x) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8209 -2.5577 -0.7021 2.4363 5.1560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7389 1.3472 0.549 0.587 ## x -0.1811 0.2060 -0.879 0.385 ## ## Residual standard error: 3.231 on 38 degrees of freedom ## Multiple R-squared: 0.01993, Adjusted R-squared: -0.005863 ## F-statistic: 0.7727 on 1 and 38 DF, p-value: 0.3849 newdat = data.frame(x = seq(1,10,0.1)) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(x = seq(1,10,0.1), result_conf) ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y), size = 3) + ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 予測変数xの傾きはほぼフラットで、yに対してあまり効果がないようにみえる。 しかし、このデータdにはもう一つ性別を意味するgenderという変数が含まれていた。genderを区別して、またxとyの散布図を見てみよう。 ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 性別が女性（F）か男性（M）かで、xとyの関係が違うようである。 このように、別の変数との組み合わせにより、変数間の関係が変化することを交互作用(interaction)という。このデータでも、応答変数yに対して性別genderとxの交互作用がありそうである。 交互作用のあるモデルは、以下のように表現する。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x + \\beta_{2} M + \\beta_{3} xM \\\\ \\tag{5} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] \\(M\\)は性別genderのダミー変数で、M（男性）ならば1、F（女性）ならば0の変数とする。 線形モデルでは、交互作用は予測変数同士の積で扱う。男性（M=1）の場合のyの推定値は、\\(\\alpha +(\\beta_{1} + \\beta_{3}) x +\\beta_{2}\\)となる。一方、女性（M=0）の場合は、\\(\\alpha +\\beta_{1} x\\)となる。\\(\\beta_{3}\\)は、男性のときの\\(x\\)に係る傾きの変化量を意味することになる。このように、交互作用を考慮する予測変数の積をモデルに加えることで、男性か女性かで切片及び傾きが変化することを表現できる。 d$M = ifelse(d$gender == &quot;M&quot;, 1, 0) #genderがMならば1, Fならば1のダミー変数を作る result = lm(data = d, y ~ 1 + x*M) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x * M, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3555 -0.6534 0.2205 0.5636 1.6618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05079 0.53625 0.095 0.925 ## x -0.53691 0.08107 -6.622 1.03e-07 *** ## M 1.04827 0.73745 1.421 0.164 ## x:M 0.77868 0.11274 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8827 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 2つの予測変数の積の傾き（\\(\\beta_{3}\\)）は、x:Mである。p値も小さく、有意な効果を持っているようである。 ここで注意が必要なのは、交互作用を含む線形モデルの係数は解釈が複雑になることである。 男性(M = 1)の予測値は、線形モデルの式に推定された傾きと切片及び\\(M=1\\)を代入して、(0.05 + 1.05) + (-0.54 + 0.78) \\(x\\) となる。女性(M = 0)の場合は、0.05 -0.54 \\(x\\) となる。 xとMの傾きの推定値は、xやMの全体としての効果、いわゆる主効果を必ずしも反映しない。 交互作用効果が見られた場合は、解釈は慎重に行う必要がある。 サンプルデータについて、推定されたパラメータを元に、男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。 new_x = seq(1,10,0.1) newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x)))) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(newdat, result_conf) plot_conf$gender = ifelse(plot_conf$M == 1, &quot;M&quot;, &quot;F&quot;) ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 10.6 変数の標準化 先述のように、交互作用を含むモデルの場合は、交互作用以外の項の解釈が複雑になる。 これへの対処として、解析に使う予測変数及び応答変数を事前に*標準化(standardizing)しておくという手がある。標準化とは、元の値を「ゼロが平均値、1が標準偏差」になるように値を変換する処理のことをいう。 変数を標準化しておくと、回帰分析の係数の解釈が直感的に理解しやすくなる。 例えば、前の章でirisデータを使って以下の線形モデルの解析を行った。 dat = iris #irisをdatという別の名前で保存 result = lm(data = dat, Petal.Length ~ 1 + Sepal.Length) summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 切片の値はSepal.LengthがゼロのときのPetal.Lengthの予測値である。しかし、アヤメのがくの長さがマイナスやゼロの値を取るというのはありえないので、この結果はどのように解釈すればよいのか困る。 標準化した変数を使って同じ解析をしたときの結果と比較してみる。具体的には、元の得点から平均値を引いて差の得点を求め、その差の得点を標準偏差で割る。 dat$Petal.Length_std = (dat$Petal.Length - mean(dat$Petal.Length, na.rm = TRUE)) / sd(dat$Petal.Length, na.rm = TRUE) dat$Sepal.Length_std = (dat$Sepal.Length - mean(dat$Sepal.Length, na.rm = TRUE)) / sd(dat$Sepal.Length, na.rm = TRUE) result_std = lm(data = dat, Petal.Length_std ~ 1 + Sepal.Length_std) summary(result_std) ## ## Call: ## lm(formula = Petal.Length_std ~ 1 + Sepal.Length_std, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.40343 -0.33463 -0.00379 0.34263 1.41343 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.333e-16 4.014e-02 0.00 1 ## Sepal.Length_std 8.718e-01 4.027e-02 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4916 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 係数の値が、変わっている。切片は0、Sepal.Lengthの効果は0.87である。切片の値は、Sepal.Length_stdがゼロのとき（つまりSepal.Lengthが平均値と等しいとき）、Petal.Length_stdはほぼゼロの値を取る（つまりPetal.Lengthの平均値）ことを意味している。また、Sepal.Length_stdの傾きは、Sepal.Length_stdが1のとき（つまりSepal.Lengthが1標準偏差分増加したとき）、Petal.Length_stdが0.87増えることを意味する。 交互作用を含むモデルの場合、変数を標準化しておくと結果がどう変化するかを確認する。 #データを再度作成する。 set.seed(1) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.1 + 0.4 * x y = rnorm(n = 20, mean = mu, sd = 1) d_M = data.frame(x = x, y = y, gender = &quot;M&quot;) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.3 + -0.6 * x y = rnorm(n = 20, mean = mu, sd = 1) d_F = data.frame(x = x, y = y, gender = &quot;F&quot;) d = rbind(d_M, d_F) d$M = ifelse(d$gender == &quot;M&quot;, 1, 0) #genderがMならば1, Fならば1のダミー変数を作る head(d) ## x y gender M ## 1 3 2.811781 M 1 ## 2 4 2.089843 M 1 ## 3 6 1.878759 M 1 ## 4 9 1.485300 M 1 ## 5 3 2.424931 M 1 ## 6 9 3.655066 M 1 #標準化する前の結果 result = lm(data = d, y ~ 1 + x*M) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x * M, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3555 -0.6534 0.2205 0.5636 1.6618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05079 0.53625 0.095 0.925 ## x -0.53691 0.08107 -6.622 1.03e-07 *** ## M 1.04827 0.73745 1.421 0.164 ## x:M 0.77868 0.11274 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8827 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 応答変数と予測変数を標準化する。ダミー変数も標準化する。 #変数を標準化 d$y_s = (d$y - mean(d$y, na.rm = TRUE))/sd(d$y, na.rm = TRUE) d$x_s = (d$x - mean(d$x, na.rm = TRUE))/sd(d$x, na.rm = TRUE) d$M_s = (d$M - mean(d$M, na.rm = TRUE))/sd(d$M, na.rm = TRUE) result = lm(data = d, y_s ~ 1 + x_s*M_s) summary(result) ## ## Call: ## lm(formula = y_s ~ 1 + x_s * M_s, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73117 -0.20282 0.06843 0.17496 0.51584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01209 0.04336 0.279 0.7821 ## x_s -0.11502 0.04394 -2.618 0.0129 * ## M_s 0.90526 0.04391 20.615 &lt; 2e-16 *** ## x_s:M_s 0.30734 0.04450 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.274 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 それぞれの係数とp値が変わった。それぞれの係数の値は、他の変数がゼロのときの応答変数の増減分を意味する、すなわち全ての他変数が平均（=0）であるときのその変数そのものの効果を意味することになる。 変数を標準化する前のモデルでは、男性の場合（M=1）のyの予測値は、「Mの係数+ xとMの交互作用項の係数」であり、Mの係数そのものは性別そのものの効果を意味するものではなかった。 これに対し、標準化した後のモデルでは、Mの係数そのものを「性別の平均的効果=主効果」として捉えることができる。Mの係数が意味することは、「他の変数がゼロのとき、つまり平均であるときに、Mがプラスに変化したとき（男性であるとき）の応答変数の変化量」を意味しており、性別そのものの効果として直感的に解釈することができる。 交互作用項の係数も、他の変数の効果が一定の場合、Mまたはsが1単位変化したときの応答変数の増分として理解できる。 10.7 線形モデルを扱う上での問題 予測変数を増やせば、他の予測変数を統制することによって、その予測変数が応答変数に及ぼすそのものの効果を検討することができる。ただし、予測変数を加えることで生じる問題もある。以降では、多重共線性と過学習の問題について触れる。 10.7.1 多重共線性 予測変数同士が非常に強く相関しあっている場合、予測変数の係数の推定結果が信頼できなくなる恐れがある。この問題は、多重共線性(multicollinearity)と呼ばれる。 サンプルデータを使って確認してみよう。Rには多重共線性の例としてlongleyというサンプルデータがある。 head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 まず、このデータに入っている変数間の相関を確認してみよう。 cor(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population ## GNP.deflator 1.0000000 0.9915892 0.6206334 0.4647442 0.9791634 ## GNP 0.9915892 1.0000000 0.6042609 0.4464368 0.9910901 ## Unemployed 0.6206334 0.6042609 1.0000000 -0.1774206 0.6865515 ## Armed.Forces 0.4647442 0.4464368 -0.1774206 1.0000000 0.3644163 ## Population 0.9791634 0.9910901 0.6865515 0.3644163 1.0000000 ## Year 0.9911492 0.9952735 0.6682566 0.4172451 0.9939528 ## Employed 0.9708985 0.9835516 0.5024981 0.4573074 0.9603906 ## Year Employed ## GNP.deflator 0.9911492 0.9708985 ## GNP 0.9952735 0.9835516 ## Unemployed 0.6682566 0.5024981 ## Armed.Forces 0.4172451 0.4573074 ## Population 0.9939528 0.9603906 ## Year 1.0000000 0.9713295 ## Employed 0.9713295 1.0000000 Employedを応答変数、GNP.deflatorを予測変数としたモデル（model01）と、Employedを応答変数、GNPを予測変数としたモデル（model02）でそれぞれ解析してみよう。 model01 = lm(data = longley, Employed ~ 1 + GNP.deflator) summary(model01) ## ## Call: ## lm(formula = Employed ~ 1 + GNP.deflator, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.68522 -0.44820 -0.07106 0.57166 1.61777 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.18917 2.12919 15.59 3.06e-10 *** ## GNP.deflator 0.31597 0.02083 15.17 4.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8706 on 14 degrees of freedom ## Multiple R-squared: 0.9426, Adjusted R-squared: 0.9385 ## F-statistic: 230.1 on 1 and 14 DF, p-value: 4.389e-10 model02 = lm(data = longley, Employed ~ 1 + GNP) summary(model02) ## ## Call: ## lm(formula = Employed ~ 1 + GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77958 -0.55440 -0.00944 0.34361 1.44594 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.843590 0.681372 76.09 &lt; 2e-16 *** ## GNP 0.034752 0.001706 20.37 8.36e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6566 on 14 degrees of freedom ## Multiple R-squared: 0.9674, Adjusted R-squared: 0.965 ## F-statistic: 415.1 on 1 and 14 DF, p-value: 8.363e-12 次に、Employedを応答変数、GNPとGNP.deflatorの両方を予測変数として入れて解析をしてみよう。 model03 = lm(data = longley, Employed ~ 1 + GNP.deflator + GNP) summary(model03) ## ## Call: ## lm(formula = Employed ~ 1 + GNP.deflator + GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.81315 -0.54330 0.05572 0.27894 1.40590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.94504 7.44945 7.644 3.67e-06 *** ## GNP.deflator -0.08511 0.12374 -0.688 0.5037 ## GNP 0.04391 0.01343 3.269 0.0061 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6693 on 13 degrees of freedom ## Multiple R-squared: 0.9685, Adjusted R-squared: 0.9637 ## F-statistic: 200 on 2 and 13 DF, p-value: 1.727e-10 それぞれの予測変数の係数を見てみると、一つずつ予測変数として入れたときと比べて値が変わっており、p値も低くなっている。 GNPとGNP.deflator同士は相関係数0.99とかなり強く相関している。このように、強く相関し合う変数を入れると係数の効果について信頼できる結果が得られなくなってしまう。 なぜ強く相関しあっている変数を入れるとまずいのか？モデルから考えてみよう。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x_{1} + \\beta_{2} x_{2} \\\\ \\tag{6} \\end{equation} \\] 2つの予測変数\\(x_{1}\\)と\\(x_{2}\\)が強く相関している場合、つまり\\(x_{1}=x_{2}\\)だとすると、式(6)は以下のように置き換えることができる。 \\[ \\begin{equation} \\mu = \\alpha + (\\beta_{1} + \\beta_{2}) x_{1} \\\\ \\tag{7} \\end{equation} \\] \\((\\beta_{1} + \\beta_{2})\\)について、パラメータ\\(\\beta_{1}\\)と\\(\\beta_{2}\\)の組み合わせは無限に考えられる。このように、強く相関する予測変数を入れると２つの予測変数のパラメータについて推定することが難しくなってしまう（パラメータの信頼区間が大きくなってしまう）。 多重共線性への対処 多重共線性の対策として、VIF(variance inflation factor)という指標がよく用いられる。一般的に、\\(VIF &gt; 10\\)の場合は、多重共線性を疑った方が良いといわれている。VIFの高い変数同士のうちどちらか一方を予測変数から除くといった対処をして、解析し直してみるのが良い。 performanceパッケージのcheck_collinearity()関数を使えば、VIFを確認することができる。 library(performance) performance::check_collinearity(model03) ## # Check for Multicollinearity ## ## High Correlation ## ## Term VIF VIF 95% CI Increased SE Tolerance Tolerance 95% CI ## GNP.deflator 59.70 [30.69, 117.04] 7.73 0.02 [0.01, 0.03] ## GNP 59.70 [30.69, 117.04] 7.73 0.02 [0.01, 0.03] 10.7.2 過学習 以下のプログラムを実行して、サンプルデータdを作成しよう。 set.seed(10) N = 10 x = seq(1,N,1) y = runif(N, min = 1, max = 5) d = data.frame(x = x, y = y) str(d) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ x: num 1 2 3 4 5 6 7 8 9 10 ## $ y: num 3.03 2.23 2.71 3.77 1.34 ... ggplot2::ggplot() + ggplot2::geom_point(data =d, aes(x=x, y = y)) このデータについて、以下の線形モデルを当てはめ、パラメータを推定しよう。図に線形モデルの直線及び信頼区間を図示するところまでやってみる。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x\\\\ \\tag{8} \\end{equation} \\] result_1 = lm(data = d, y ~ 1 + x) newdat = data.frame(x = x) conf.int_1 = predict(result_1, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_1) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 直線はほとんど観測値から外れており、当てはまりが悪いようである。 そこで、予測変数を増やして検討してみる。lm()では、予測変数\\(x\\)のn乗を含む多項式のモデルを考慮することも可能である。例えば、以下は3次の多項式の例である。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}\\\\ \\tag{9} \\end{equation} \\] n次式のモデルは多項式回帰(polynomial regression)と呼ばれる。 lm()では、I()の中に書くかたちでn次の予測変数を入れることができる。 result_3 = lm(data = d, y ~ 1 + x + I(x^2) + I(x^3)) 同じく、3次の多項式による予測の結果を図で確認しよう。 newdat = data.frame(x = x) conf.int_3 = predict(result_3, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_3) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 9次の式でも推定してみよう。 result_9 = lm(data = d, y ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9)) newdat = data.frame(x = x) conf.int_9 = predict(result_9, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_9) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 線は全てのデータ点を通っている。当然ながら、データの観測値の分だけパラメータがあれば、そのモデルはデータ点を全て通る線を引くことができる。現在のデータ点全てを予測することができる。 しかし、そのモデルは現在のデータを全て当てられても、将来得られる未知のデータを当てられるとは限らない。予測変数を多くすると現在のデータには当てはまるが、当てはまりすぎて未知のデータの予測力が低下してしまうことを、過学習(overfitting)という。 複雑なモデルが現在のデータによく当てはまるのは、ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし、複雑なモデルは現在のデータに当てはまっても、未知のデータにもうまく当てはまるとは限らない。 理想的なモデルは、「予測力が高く、かつ予測変数ができるだけ少なくてシンプルなモデル」となる。 10.8 モデルの予測力の評価 モデルの予測力を評価するために使われる指標について説明する。 10.8.1 決定係数 線形モデルでは、データに対する回帰分析のモデルの予測力を表す指標として、決定係数（R-squared）がある。 サンプルデータattitudeを例に見てみよう。 model_01 = lm(data=attitude, rating ~ 1 + complaints + learning) summary(model_01)$r.squared #r.squaredで決定係数のみを取り出すことができる。 ## [1] 0.7080152 これは、モデルから求めた予測値と実測値の分散が、実際のデータの分散に占める割合を意味する指標である。つまり、そのモデルでどれだけ全データの分散を説明できているかを意味する。 \\[ R^2 = \\sum_{i=1}^{n} \\frac {(y_{i}-\\mu_{i})^2}{(y_{i}-\\bar{y})^2} \\tag{10} \\] ただし、決定係数は単純に、予測変数が増えるほど大きくなる（説明できる分散の量が増える）。 例えばattitudeデータ内の全ての変数を予測変数に使ってみる。 model_full = lm(data=attitude, rating ~ .) #線形予測子を入力するところにドットを入力すると、そのデータに含まれる全ての変数を予測変数として扱う summary(model_full)$r.squared ## [1] 0.732602 応答変数に影響を及ぼさない変数を含めても、決定係数は上昇してしまう。 決定係数は、「予測力が高く、シンプルなモデル」を探すには常に適切な指標であるとは言えない。 10.8.2 赤池情報量基準（AIC） モデルのシンプルさ（予測変数の少なさ）とモデルの予測力とのバランスを取った指標の一つとして、赤池情報量基準(Akaike inoformation criteria: AIC)がよく知られている。AICは以下の式で計算される。 \\[ AIC = -2 \\log L + 2k \\tag{11}\\\\ \\] \\(\\log L\\)は最大対数尤度、\\(k\\)はモデルのパラメータ数である。 第9章で、モデルのパラメータを推定する方法として「最尤法」を紹介した。最尤法は、モデルのもっともらしさ（データが生じる確率）を意味する「対数尤度」が最大となるときのパラメータを求める方法であった。最大対数尤度は、現在のモデルに対する当てはまりの良さを反映している。その最大対数尤度に対し、パラメータ数\\(k\\)に応じてペナルティ(penalty term)を加える。 AICの値が低いほど、モデルの予測力が高いと評価する。AICは余計なパラメータが多くなる（\\(k\\)が大きくなる）ほど大きい値を取る。つまり、データをうまく予測しつつ、かつパラメータ数を抑えてシンプルなモデルを探る目的にかなっている。 AIC()関数でモデルをカッコ内に入れると、AICを算出してくれる。さきほどのattitudeに当てはめた2つのモデルのAICを見てみよう。 AIC(model_full) ## [1] 210.4998 AIC(model_01) ## [1] 205.1387 model_fullよりもmodel_01のAICが低く、model_01の予想力の方が高いことを示している。 確認問題 問1 Rで標準で入っているデータwarpbreaksを使って練習をする。 prac_dat_1 = warpbreaks #別の名前で保存する head(prac_dat_1) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks)) ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks)) 1-1 変数woolについて, 「Aを1, それ以外を0」としたダミー変数を作成し、そのダミー変数を予測変数、breaksを応答変数として線形モデルを行い、切片及びダミー変数に係る傾きの推定値を報告せよ。 また、ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。 1-2 変数tensionについて, 「Lを1, それ以外を0」、「Mを1, それ以外を0」とした2種類のダミー変数を作成し、それら2つのダミー変数を予測変数、breaksを応答変数として線形モデルを行い、切片及び各ダミー変数に係る傾きの推定値を報告せよ。 更に、そのときの切片及び各ダミー変数の係数が意味することを説明せよ。 1-3 1-2で作ったダミー変数に加え、更に「Hを1, それ以外を0」としたダミー変数を追加で作成する。 更に、breaksから全体のbreaksの平均を引いた変数breaks_2を作成する。 それら3つのダミー変数を予測変数、breaks_2を応答変数として線形モデルを行い、各ダミー変数に係る傾きの推定値を報告せよ。ただし、モデルには切片の項は加えないものとする。 更に、そのときの各ダミー変数の係数が意味することを説明せよ。 問2 問1に引き続き、Rで標準で入っているデータwarpbreaksを使って練習をする。ただし、tensionがHの部分を除いたデータを用いる。 prac_dat_2 = subset(warpbreaks, tension != &quot;H&quot;) #tension == Hは除き、別の名前で保存する head(prac_dat_2) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension)) breaksを応答変数、wool, tension, wool及びtensionの交互作用項を予測変数とした線形モデルを行い、切片、woolの傾き、tensionの傾き、交互作用項の推定値を報告せよ。 問3 Rで標準で入っているairqualityを使う。 prac_dat_3 = na.omit(airquality) #欠損値を除き、別の名前で保存する head(prac_dat_3) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 3-1 Ozoneを応答変数、Solar.R, Wind, Tempの3つを予測変数とした線形モデルを行う。そして、切片及び傾きの推定値を報告せよ。 3-2 3-1で行った線形モデルについて、決定係数を報告せよ（Multiple R-squared）。 3-3 以下の3種類の線形モデルの解析を行い、 モデル1: Ozoneを応答変数、Solar.R, Wind, Tempの3つを予測変数とした線形モデル モデル2: Ozoneを応答変数、Solar.R, Tempの2つを予測変数とした線形モデル モデル3: Ozoneを応答変数、Tempを予測変数とした線形モデル それぞれのモデルのAICを報告するとともに、3つのモデルのうち予測力が高いと考えられるものはどれかを報告せよ。 "],["10-glm.html", "Chapter 11 一般化線形モデル 11.1 準備 11.2 一般化線形モデル 11.3 ロジスティック回帰 11.4 ポアソン回帰 11.5 一般化線形モデルのまとめ 確認問題", " Chapter 11 一般化線形モデル 確率分布が正規分布以外の場合の「一般化線形モデル」について学ぶ。 ロジスティック回帰 ポアソン回帰 11.1 準備 データの可視化のために、ggplot2パッケージをロードする。 更に、MASSパッケージを使うので、インストールとロードを行う。 library(ggplot2) install.packages(&quot;MASS&quot;) library(MASS) 11.2 一般化線形モデル 線形モデルは、以下の式で表されるモデルであった。 \\[ \\begin{equation} \\mu = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x \\\\ \\tag{1} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] 線形モデルでは、応答変数が正規分布に従うという前提で、応答変数\\(y\\)を予測するパラメータ（線形予測子の切片と係数、及び正規分布の分散）を求めた。 今回は、応答変数が正規分布以外の確率分布に従うモデルを扱う。 線形モデルを正規分布以外の確率分布に拡張したモデルを、一般化線形モデル(generalized linear model)という（GLMと略されることも多い）。一般化線形モデルを理解する上で重要なのは、応答変数が従う確率分布に加え、リンク関数(link function)という考え方である。 11.3 ロジスティック回帰 前の章までは応答変数が量的変数の例を扱ってきた。では、応答変数がカテゴリカル変数である場合は、どのような解析をすればよいのだろうか。 応答変数が二値のカテゴリカル変数の場合を例として見ていく。 MASSパッケージに入っているサンプルデータbiopsyを使いながら検討していこう。まず、以下のプログラムを実行して、練習用のデータdatを作成する。 library(MASS) dat = biopsy dat$y = ifelse(dat$class == &quot;malignant&quot;, 1, 0) #classがbenignならばゼロ、それ以外なら1という変数yを作る dat$x = dat$V1 #V1という変数をxという名前に変える head(dat) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class y x ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 5 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 5 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 3 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 6 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 4 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 8 xは整数の変数、yは1ならば癌、0ならば癌ではないことを意味する変数とする。 xが変化すると癌である確率が変化するかを検討したい。 まず、xとyとの関係を図で確認してみる。 ggplot2パッケージで、xをx軸、yをy軸にしてプロットしてみよう。 普通にgeom_pointで散布図を作っても点が重なって見にくいので、geom_jitterを使って描画する。geom_jitterは、ランダムで点をずらして描画してくれる。 ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) では、前章までで学んだとおりに、xを予測変数、yを応答変数とした線形モデルでxの効果を検討しよう。 \\[ \\begin{equation} \\mu = \\alpha + \\beta x \\\\ \\tag{2} y \\sim \\text{Normal}(\\mu, \\sigma) \\end{equation} \\] result_lm = lm(data = dat, y ~ 1 + x) summary(result_lm) ## ## Call: ## lm(formula = y ~ 1 + x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77804 -0.17331 -0.01994 0.06859 1.06859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.189535 0.023395 -8.102 2.43e-15 *** ## x 0.120947 0.004467 27.078 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3323 on 697 degrees of freedom ## Multiple R-squared: 0.5127, Adjusted R-squared: 0.512 ## F-statistic: 733.2 on 1 and 697 DF, p-value: &lt; 2.2e-16 xに係る傾きの推定値を数値通りに解釈すると、「xが1単位増えると、yが0.12増える」ことを示している。 では、求めた傾きと切片から直線を先程のxとyとの関係の図に引いてみよう。 predict_lm = predict(result_lm, interval = &quot;confidence&quot;, level = 0.95) #直線の95%信頼区間を求める dat_predict = cbind(dat, predict_lm) ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = dat_predict, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.5) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) 線形モデルから推定された直線は、「xが増えるほどyが増える」関係を表しているように見える。 しかし、この線形モデルの結果は、yを予測する上で問題がある。 解析の目的は、\\(y = 1\\)の確率、つまりがんにかかる確率を推定することであるが、例えば\\(x\\)が10を超えると、応答変数の予測値は1以上の値を取る。また、\\(x\\)が2.5を下回ったときも、0未満の数値が推定されてしまう。応答変数は0か1しか取らないのに、それぞれを超える値が予測されてしまう。これは確率の推定としては不都合である。 応答変数\\(y\\)は連続量ではなく、0か1の値を取るカテゴリカル変数である。連続量の確率分布である正規分布に応答変数が従うという前提を置くのは予測モデルとして適切ではない。 ではどうすれば良いのか？ 解決策として、モデルを以下のように変更する。 \\[ \\begin{equation} q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\\\ \\tag{3} y \\sim \\text{Binomial}(1, q) \\end{equation} \\] \\(\\exp(\\alpha + \\beta x)\\)は、\\(e^{(\\alpha + \\beta x)}\\)とも表記できる。 \\(y = 1\\)である確率（がんである確率）を\\(q\\)とする。 11.3.1 応答変数が従う確率分布 まず、式(3)の2つ目の式が何を意味しているのかを確認する。 \\[ y \\sim \\text{Binomial}(1, q) \\] これは、応答変数\\(y\\)が試行回数1回、成功確率\\(q\\)の二項分布に従うということを示している。 例として、二項分布から乱数を作るrbinom()関数を使って、試行回数1回、成功確率\\(q\\)を0.5とした二項分布から乱数を20個を生成してみる。 q = 0.5 rand = rbinom(n =20, size = 1, prob = q) rand ## [1] 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 このように、0か1が生成される。 11.3.2 リンク関数 式(3)の1つ目は何を意味しているのか？以下の式について、\\(z = \\alpha + \\beta x\\)として、\\(z\\)を変化させると\\(q\\)がどう変化するか図で見てみよう。 z = seq(-10, 10, 0.1) #-10から10まで0.1刻みのベクトルzを作成 q = exp(z)/(1+exp(z)) #上の式にzを代入して、qを求める d = data.frame(z = z, q = q) #グラフを作るために、データフレームを作る ggplot2::ggplot()+ ggplot2::geom_line(data = d, aes(x=z, y=q)) \\(z\\)は\\(-\\infty\\)から\\(\\infty\\)の範囲を取るが、\\(z\\)がどのような値をとっても、\\(0&lt;q&lt;1\\)となる（限りなく0もしくは1に近づく）。\\(q\\)は確率なので、この0から1の範囲に収まるようになる変換は都合が良い。 また、式(3)の一つ目は、 \\[ q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\] 右辺を線形予測子にして整理すると、以下のようにできる。 \\[ \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\] この変換は、ロジット関数（logit function）と呼ばれる。 11.3.3 ここまでのまとめ 線形予測子を変換する関数は、「リンク関数」と呼ばれる。上の例のように、応答変数が二値の場合は、推定値を0から1に収めるためにロジット関数をリンク関数として使うのが適切である。 このように、「応答変数が従う確率分布」と「線形予測子に変換をほどこすリンク関数」を選ぶことにより、線形モデルを様々なデータ解析に一般化させたものが一般化線形モデル(generalized linear model)である。一般化線形モデルは、「確率分布」と「リンク関数」を応答変数のタイプに応じてカスタマイズするというイメージで捉えると良い。 上の例で見た「応答変数が従う確率分布をベルヌーイ分布（または二項分布）」、「リンク関数をロジスティック関数（ロジット関数）」とした一般化線形モデルは、ロジスティック回帰と呼ばれる。 11.3.4 Rでのロジスティック回帰 Rには、一般化線形モデルを扱うための関数glm()が用意されている。線形モデルを扱うlm()と同じ要領でプログラムを書けばよいが、確率分布とリンク関数のオプションを自分で指定する必要がある。先程のサンプルデータdatで、glm()関数を使ってロジスティック回帰をやってみよう。 result_glm = glm(data = dat, y ~ 1 + x, family = binomial(link=&quot;logit&quot;)) glmで設定すること： 「線形予測子」、「応答変数が従う確率分布」、「リンク関数」を指定する。 familyで、応答変数が従う確率分布を指定する。 family = binomial、すなわち二項分布（binomial distribution）に従うとする。（式(3)で示しているように正確にはベルヌーイ分布であるが、binomialで構わない） (link=)で、リンク関数を指定する。ロジット関数(logit)を指定しよう。 ちなみに、(link=\"logit\")は省略してもかまわない。family=binomialとすると、デフォルトでリンク関数をlogitとしてくれる。 では、出力結果を見てみよう。 summary(result_glm) ## ## Call: ## glm(formula = y ~ 1 + x, family = binomial(link = &quot;logit&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## x 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 出力はlm()と似ている。Coefficientsの部分を見よう。Estimateがパラメータの推定結果である。Prがp値である。パラメータの推定値は、プラスならば応答変数が1の値、マイナスならば応答変数が0の値を取りやすいことを意味する。 xに係る傾きの値0.94は何を意味しているのか？ 線形モデルでは傾きの推定値は、「予測変数が1単位増えたときの応答変数の変化量」を意味していた。今回の例も、xが1増えると確率が0.94上がるということを示しているのか？ 一般化線形モデルの場合、係数の値が意味することの解釈には注意が必要である。 \\[ \\begin{equation} \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\end{equation} \\] 右辺を線形の式とすると、左辺は対数オッズとなる。つまり、\\(x\\)に係る傾き\\(\\beta\\)は、「\\(x\\)が1増えた時の\\(q\\)の対数オッズの変化量」を意味しており、確率\\(q\\)そのものの変化量ではない。このように、正規分布以外の確率分布を用いた一般化線形モデルでは、係数そのものの値を解釈するのが難しくなる点に注意が必要である。 対数オッズと確率\\(q\\)との関係を図で見てみよう。x軸を\\(\\log(q/[1-q])\\)、y軸を\\(q\\)とした図を示す。 q = seq(0, 1, 0.01) logit = log(q/(1-q)) sample_dat = data.frame(q = q, logit = logit) ggplot2::ggplot() + ggplot2::geom_line(data = sample_dat, aes(x = logit, y = q)) つまり、対数オッズがプラスだと確率\\(q\\)は0.5より大きくなり、対数オッズがマイナスだと確率\\(q\\)は0.5より小さくなる関係にある。要は、対数オッズがプラスだと\\(y = 1\\)が起こりやすくなり、マイナスだと起こりにくくなることを意味している。 求めた係数の推定値を元に、確率を予測する線を引いてみよう。 new = data.frame(x = seq(0, 11, 0.1)) predict_glm = predict(result_glm, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = y)) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) 予測線は0から1の範囲に収まっており、線形予測子から確率の予測ができている。 11.4 ポアソン回帰 応答変数が正規分布以外に従う場合の例として、先程は応答変数が0か1の二値の場合を扱った。同じく応答変数の範囲に制約がある場合の例として、次は応答変数が正の値の整数しか取らない場合（0を含む）を扱う。 具体的には、応答変数がカウントデータの場合である（非負の整数。0個、1個、2個,3個といった個数など）。この場合は、ポアソン回帰と呼ばれる一般化線形モデルを扱うのが適切とされている。 サンプルデータを用いながら、ポアソン回帰について学んでいこう。以下のプログラムを実行して、サンプルデータdat_poissonを作成しよう。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd=1) lambda = exp(0.01+ 0.6*x) y = rpois(n=N, lambda = lambda) dat_poisson = data.frame(y=y, x=x) xとyの関係を散布図で確認してみる。 ggplot2::ggplot()+ ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) xが大きいほど、yが大きいという関係がありそうである。\\(x\\)から、\\(y\\)を予測する。 まずは、線形モデルを当てはめてみよう。 model = lm(data = dat_poisson, y ~ 1 + x) summary(model) ## ## Call: ## lm(formula = y ~ 1 + x, data = dat_poisson) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0384 -1.3050 -0.0837 0.5879 8.3524 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.0977 1.0022 -2.093 0.0417 * ## x 2.9887 0.4442 6.728 1.92e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.585 on 48 degrees of freedom ## Multiple R-squared: 0.4853, Adjusted R-squared: 0.4746 ## F-statistic: 45.26 on 1 and 48 DF, p-value: 1.924e-08 求めた傾きと切片をもとに、yを予測する直線を引いてみよう。 predict_lm = predict(model, interval = &quot;confidence&quot;, level = 0.95) #直線の95%信頼区間を求める dat_predict = cbind(dat_poisson, predict_lm) ggplot2::ggplot() + ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = dat_predict, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.5) + ggplot2::scale_y_continuous(breaks = seq(0,20,1)) 直線の左側が、0より下にはみ出てしまっている。\\(y\\)は正の値を取る離散値（整数）である。しかし、線形モデルで求めた直線の式ではマイナスの値も予測されてしまう。 ポアソン回帰は、この問題を解消してくれる。ポアソン回帰を数式で表すと、以下のようになる。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\tag{4}\\\\ y \\sim \\text{Poisson}(\\lambda) \\end{equation} \\] 11.4.1 応答変数が従う確率分布 まず、2つ目の式は、 \\[ y \\sim \\text{Poisson}(\\lambda) \\] \\(\\lambda\\)をパラメータとするポアソン分布から、応答変数\\(y\\)が生成されることを示している。 例えば、以下にポアソン分布から乱数を生成するrpois()関数を使って、\\(\\lambda\\)が3のポアソン分布から乱数を20個作ってみよう。 lambda = 3 rand = rpois(n = 20, lambda = lambda) rand ## [1] 3 3 2 3 3 1 3 1 2 2 2 5 3 4 5 2 1 2 4 2 正の離散値（整数）が生成される。 ポアソン分布は、パラメータ\\(\\lambda\\)を持つ確率分布である。 \\[ P(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!} \\\\ \\] \\(y\\)は0以上の整数（0, 1, 2, 3, …）、\\(P(y)\\)は\\(y\\)が生じる確率とする。 ポアソン分布のかたちを決定づけるパラメータは、\\(\\lambda\\)のみである。\\(\\lambda\\)は、ポアソン分布の期待値（平均）と分散の両方を意味する。つまり、ポアソン分布は平均と分散が等しい分布である。 set.seed(1) x = rpois(n = 30, lambda = 2) #ポアソン分布から乱数を生成する関数 lambda =2のポアソン分布から30個乱数を生成 x #整数が生成される ## [1] 1 1 2 4 1 4 4 2 2 0 1 1 3 1 3 2 3 6 1 3 4 1 2 0 1 1 0 1 4 1 mean(x) ## [1] 2 var(x) ## [1] 2.206897 d = data.frame(x = x) ggplot2::ggplot() + ggplot2::geom_histogram(data = d, aes(x=x)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 以下に、パラメータ\\(\\lambda = 1\\), \\(\\lambda = 2\\), \\(\\lambda = 3\\)それぞれの場合のポアソン分布を図で示す。 ポアソン分布は、二項分布とも関連している。 二項分布のパラメータは、試行回数\\(n\\)と成功確率\\(p\\)であった。二項分布の期待値（平均）は\\(np\\)、分散は\\(np(1-p)\\)である。 \\[ y \\sim \\text{Binomial}(n, p) \\\\ E(y) = np\\\\ Var(y) = np(1-p)\\\\ \\] 二項分布の試行回数\\(n\\)が大きく、成功確率\\(p\\)が小さい場合、二項分布の平均と分散はほとんど等しくなり、ポアソン分布に近似する。つまり、めったに起こらないイベントが生じる回数は、ポアソン分布に従うとされている。 11.4.2 リンク関数 線形予測子とポアソン分布のパラメータ\\(\\lambda\\)との関係をもう一度確認しよう。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\tag{4} \\\\ \\end{equation} \\] なぜ指数関数（\\(\\exp()\\)）を用いるのか？\\(z=\\alpha + \\beta x\\)として、\\(\\lambda=\\exp(z)\\)との関係を図で見てみよう。 z = seq(-5, 5, 0.1) #-10から10まで0.1刻みのベクトルzを作成 lambda = exp(z) #上の式にzを代入して、lambdaを求める d = data.frame(z = z, q = lambda) #グラフを作るために、データフレームを作る ggplot2::ggplot()+ ggplot2::geom_line(data = d, aes(x=z, y=lambda)) 図からもわかるように、\\(z\\)の値に関わらず、\\(\\lambda\\)は常に正の値を取る。ポアソン分布のパラメータ\\(\\lambda\\)は\\(\\lambda&gt;0\\)という制約があるため、このような変換をする必要がある。 また、式の右辺を線形予測子にして整理すると、以下の式になる。 \\[ \\log\\lambda_{i}=\\alpha+\\beta x \\\\ \\] つまり、ポアソン回帰では、線形予測子と応答変数をリンクさせるリンク関数として対数関数（log）を設定する。 11.4.3 Rでのポアソン回帰 Rでポアソン回帰をやってみよう。一般化線形モデルを扱う関数glm()で、以下のように確率分布にポアソン分布、リンク関数に対数を指定する。 なお、(link = \"log\")は省略しても構わない。family = poissonで確率分布をポアソン分布に指定すれば、自動でリンク関数を対数にしてくれる。 result_poisson = glm(data = dat_poisson, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_poisson) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4112 -0.7542 0.1362 0.5628 1.7629 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6829 0.2807 -2.433 0.015 * ## x 0.8951 0.1052 8.506 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 130.32 on 49 degrees of freedom ## Residual deviance: 46.52 on 48 degrees of freedom ## AIC: 197.96 ## ## Number of Fisher Scoring iterations: 5 推定された式が\\(y\\)をうまく予測できているか、図で確認してみよう。 new = data.frame(x = seq(0, 4, 0.1)) predict_glm = predict(result_poisson, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot2::ggplot() + ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = y)) ゼロよりも大きい値が予測されており、予測線も各データ（点）の近くに位置している。 ロジスティック回帰のときと同様に、ポアソン回帰の予測変数の傾きの値も単純に「予測変数が1単位増えたときの応答変数の変化量」を意味するわけではない点に注意が必要である。式(10)をもう一度確認すると、 \\[ \\log\\lambda_{i}=\\alpha+\\beta x \\tag{10}\\\\ \\] であった。つまり、予測変数の傾きは「その予測変数が1単位増えたときの、応答変数（パラメータ\\(\\lambda\\)）の対数の変化量」を意味する。しかし、これでは数値をどう解釈すればいいのか直感的に理解しにくい。 式(10)は、以下の式にも直すことができる。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\\\ \\lambda = \\exp(\\alpha)\\exp(\\beta x)\\\\ \\end{equation} \\] つまり、予測変数の傾きを指数関数で変換した値が、応答変数（パラメータ\\(\\lambda\\)）の変化量を意味する。ポアソン回帰の係数を解釈する際には、係数を指数関数で変換した後の値を使う方が解釈がしやすい。 11.4.4 ポアソン回帰の注意点：過分散 次のプログラムを実行して、サンプルデータdat_disを作成する。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd = 1) e = rnorm(n=N, mean = 0, sd = 1) lambda = exp(0.01 + 0.1*x + e) y = rpois(n=N, lambda = lambda) dat_dis = data.frame(y=y, x=x) 先ほどと同様に、このデータの変数xとyを用いて、xからyを予測するポアソン回帰をやってみる。 result_dis = glm(data = dat_dis, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_dis) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_dis) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9022 -1.3362 -0.6405 0.3623 6.2029 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1417 0.3139 -0.451 0.65177 ## x 0.4393 0.1268 3.465 0.00053 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.37 on 49 degrees of freedom ## Residual deviance: 153.39 on 48 degrees of freedom ## AIC: 260.41 ## ## Number of Fisher Scoring iterations: 6 xはyを予測する上で、かなり強い効果を有しているように見える。しかし、単純にそのような結論を出すことはできない。このデータでは、分散が平均よりも過剰に大きい過分散が生じているためである。 mean(dat_dis$y) ## [1] 2.32 var(dat_dis$y) ## [1] 12.8751 11.4.4.1 過分散とは？ （yのヒストグラム。白いバーが実際のyの分布、薄い赤のバーはパラメータ\\(\\lambda\\)がyの実際の平均と等しい場合のポアソン分布） ポアソン分布は、平均と分散が等しい分布である。しかし、現実にはデータの平均と分散が等しいケースは少ない。逆に、分散が平均よりもかなり大きいケースがよく見られる。 分散が平均よりも大きいときにポアソン回帰を使うと、分散を実際よりも小さいと推定してしまい、予測変数が平均に与える効果を過大に評価してしまう恐れがある（p値を過剰に小さく判断してしまい、第一種の過誤を犯しやすくなる。ポアソン回帰を行うとかなり低いp値が出ることが多いのは、このためである）。この問題は、過分散(overdispersion)と呼ばれる。 11.4.4.2 過分散の確認方法 performanceパッケージのcheck_overdispersion()関数で過分散の有無を確認することができる。 library(performance) performance::check_overdispersion(result_dis) ## # Overdispersion test ## ## dispersion ratio = 4.048 ## Pearson&#39;s Chi-Squared = 194.291 ## p-value = &lt; 0.001 ## Overdispersion detected. このデータでは過分散が生じてしまっていることがわかる。 11.4.4.3 過分散への対処法 ポアソン回帰で過分散が疑われる場合、対処法としては例えば以下の方法がある。 応答変数が従う確率分布として、負の二項分布を用いたモデルで分析する。 一般化線形混合モデルで、個体差を意味するパラメータを追加したモデルを用いる（分散を個体差パラメータに吸収させる）。 対処法1については、12章で解説する。対処法2については、13章で説明する。 11.5 一般化線形モデルのまとめ 線形モデルを正規分布以外の別の確率分布に拡張したモデルのことを、一般化線形モデルという。 応答変数が二値のデータや割合である場合は、ロジスティック回帰を用いる（確率分布はベルヌーイ分布もしくは二項分布、リンク関数はロジット）。 応答変数がカウントデータである場合は、ポアソン回帰を用いる（確率分布はポアソン分布、リンク関数は対数）。 確認問題 ここまで予測変数が1つの場合を例として扱ってきたが、一般化線形モデルでももちろん予測変数を複数加えたモデルを扱うことができる（第10章参照）。以降の確認問題では、予測変数が複数のケースで練習する。 問１ 以下のプログラムを実行し、サンプルデータを作成する。 変数の意味は以下の通りである。 Disease: ある病気にかかっているか（1=かかっている、0=かかっていない） BMI: BMI（肥満度を表す指標） Exercise: 1週間あたりの運動時間（単位：時間） Sleep: 1日の睡眠時間（単位：時間） Disease = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) BMI = c(15, 16, 16, 18, 19, 20, 21, 22, 22, 23, 23, 23, 24, 24, 24, 30, 31, 31, 33, 34, 34, 34, 35, 36, 40, 40, 40, 41, 43, 43) Exercise = c(2, 1, 1, 2, 0, 3, 1, 1, 4, 4, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 0, 2, 2, 3, 0, 4) Sleep = c(7, 4, 5, 4, 4, 6, 5, 6, 4, 6, 4, 7, 4, 7, 4, 6, 5, 4, 5, 6, 7, 5, 4, 6, 4, 7, 5, 5, 4, 7) data_q01 = data.frame(Disease = Disease, BMI = BMI, Exercise = Exercise, Sleep = Sleep) Diseaseを応答変数、BMI、Exercise、Sleepの3つを予測変数としたロジスティック回帰を行い、それぞれの予測変数の係数について報告せよ。また、5%水準で有意な効果を持っていた予測変数を報告せよ。 問２ 以下のプログラムを実行し、サンプルデータを作成する。 変数の意味は以下の通りである。 Birds: その日に観測した鳥の数 Temp: 気温（摂氏） Cloud: 天気（0 = 晴れ, 1 = くもり） Humid: 湿度（%） Birds = c(2, 9, 8, 3, 6, 6, 5, 5, 7, 2, 9, 3, 13, 5, 7, 5, 5, 10, 10, 13) Temp = c(23.8, 25.3, 26.1, 22.7, 25.4, 25.5, 24.4, 24.5, 24.4, 24.1, 24.5, 24.0, 24.2, 25.1, 26.0, 24.9, 24.5, 24.1, 24.2, 27.4) Cloud = c(1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1) Humid = c(50.3, 49.0, 49.1, 50.9, 48.6, 47.1, 51.1, 48.0, 50.0, 48.1, 52.2, 49.0, 48.6, 49.0, 46.7, 47.7, 45.6, 47.3, 49.4, 49.1) data_q02 = data.frame(Birds = Birds, Temp = Temp, Cloud = Cloud, Humid = Humid) Birdsを応答変数、Temp、Cloud、Humidの3つを予測変数としたポアソン回帰を行い、それぞれの予測変数の係数について報告せよ。また、5%水準で有意な効果を持っていた予測変数を報告せよ。 "],["11-glm2.html", "Chapter 12 一般化線形モデルの応用 12.1 準備 12.2 負の二項回帰 12.3 順序ロジスティック回帰 12.4 多項ロジスティック回帰 確認問題", " Chapter 12 一般化線形モデルの応用 前の章では、代表的な一般化線形モデルとして、ロジスティック回帰とポアソン回帰を学んだ。この章では、一般化線形モデルを応用したその他の解析法について説明する。 負の二項回帰（過分散対策） 順序ロジスティック回帰 多項ロジスティック回帰 12.1 準備 可視化のためのggplot2パッケージに加え、MASS、ordinal、nnetパッケージを使う。 MASSパッケージは負の二項分布を用いたモデルのときに、ordinalパッケージは順序ロジスティック回帰のときに、nnetパッケージは多項ロジスティック回帰のときに必要になる。初めて使う際には、事前にインストールが必要なので注意。 library(ggplot2) library(MASS) library(ordinal) library(nnet) 12.2 負の二項回帰 前の章で、応答変数がカウントデータの場合、ポアソン回帰で解析するのが適切であると学んだ。しかし、実際のデータは分散が平均よりも大きい場合が多く、平均と分散が等しいという前提のポアソン分布を用いると予測変数の効果を誤って判断してしまう恐れがある。これが、過分散（overdispersion）と呼ばれる問題である。 過分散対策として、応答変数が従う分布としてポアソン分布の代わりに、負の二項分布(negative binomial distribution)を用いる方法がよく使われる。 12.2.1 負の二項分布 例えばコインを投げて表が出る確率を0.5として、表が3回出るまで投げると決めたとする。8回投げたところで表が3回出た場合、表が3回出る確率は以下から求めることができる。 choose(8-1, 3-1)*0.5^2*(1-0.5)^(8-3)*0.5 #つまり、7回中表が2回、裏が5回出て、最後の１回で表が出る確率を求める。 ## [1] 0.08203125 これを一般化した式が以下である。成功確率を\\(q\\)として、\\(r\\)回成功するまでに試行が\\(x\\)回かかる確率を表した確率分布が、負の二項分布である。 \\[ P(x) = {}_{x-1}\\mathrm{C}_{r-1} q^{r}(1-q)^{x-r} \\] 失敗回数を\\(y\\)として、以下のように置き換えることもできる(\\(x=y+r\\)を代入する)。\\(x\\)回目までにある事象が\\(r\\)回生じる確率と言い換えることができる。 \\[ P(y) = {}_{y+r-1}\\mathrm{C}_{r-1} q^{r}(1-q)^{y} \\] Rでもnbinomで負の二項分布の確率を計算することができる。 x = 0:10 p_y = dbinom(x = x, size = 10 - 3, prob = 0.5) d_plot = data.frame(x = x, p_y = p_y) ggplot2::ggplot() + ggplot2::geom_bar(data = d_plot, aes(x = x, y = p_y), stat = &quot;identity&quot;) + ggplot2::labs(x = &quot;number of trials&quot;, y = &quot;probability&quot;, title = &quot;number of success = 3&quot;) 負の二項分布の期待値を\\(E(x)=\\mu\\)とすると、分散は\\(Var(x)=\\mu + \\mu^{2}/r\\)で、分散が期待値（平均）よりも\\(\\mu^{2}/r\\)大きい。負の二項分布によって、分散が平均よりも大きい分布を扱うことができる。 12.2.2 Rでの負の二項回帰 Rでは、MASSパッケージに含まれているglm.nb()関数で、負の二項回帰を扱うことができる。Rに入っているwarpbreaksをサンプルデータとして、ポアソン回帰と負の二項回帰の結果を比較してみよう。 d = warpbreaks #別の名前(d)で保存する d$A &lt;- ifelse(d$wool == &quot;A&quot;, 1, 0) #Aなら1, Bなら0のダミー head(d) ## breaks wool tension A ## 1 26 A L 1 ## 2 30 A L 1 ## 3 54 A L 1 ## 4 25 A L 1 ## 5 70 A L 1 ## 6 52 A L 1 ggplot2::ggplot() + ggplot2::geom_histogram(data = d, aes(x = breaks, fill = wool), binwidth = 1) breaksに対するwool(A or B)の効果を検討する。まずは、ポアソン回帰の結果を見てみる。breaksを\\(y\\)、Aを\\(x\\)とすると、モデルは以下のように表現できる。 \\[ \\lambda = \\alpha + \\beta x\\\\ y \\sim \\text{Poisson}(\\lambda) \\] model_poisson = glm(data = d, breaks ~ 1 + A, family = poisson(link = &quot;log&quot;)) summary(model_poisson) ## ## Call: ## glm(formula = breaks ~ 1 + A, family = poisson(link = &quot;log&quot;), ## data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.4071 -1.9148 -0.7138 0.8332 5.9948 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.22919 0.03829 84.331 &lt; 2e-16 *** ## A 0.20599 0.05157 3.994 6.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 297.37 on 53 degrees of freedom ## Residual deviance: 281.33 on 52 degrees of freedom ## AIC: 560 ## ## Number of Fisher Scoring iterations: 4 mean(d$breaks) ## [1] 28.14815 var(d$breaks) ## [1] 174.2041 Aに係る傾きの推定値について、かなり小さいp値が推定されている。 次に、負の二項回帰の結果と比較してみよう。 \\[ \\mu = \\alpha + \\beta x\\\\ y \\sim \\text{NegativeBinom}(\\mu, r) \\] MASSパッケージのglm.nb()を使う。 model_nb = MASS::glm.nb(data = d, breaks ~ 1 + A) #lm関数と同じ要領で、線形の式を入力する。確率分布はオプションで指定しないで良い。 summary(model_nb) ## ## Call: ## MASS::glm.nb(formula = breaks ~ 1 + A, data = d, init.theta = 6.960797279, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1703 -0.8631 -0.3242 0.3561 2.2872 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.22919 0.08238 39.197 &lt;2e-16 *** ## A 0.20599 0.11533 1.786 0.0741 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(6.9608) family taken to be 1) ## ## Null deviance: 57.400 on 53 degrees of freedom ## Residual deviance: 54.212 on 52 degrees of freedom ## AIC: 419.97 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 6.96 ## Std. Err.: 1.63 ## ## 2 x log-likelihood: -413.975 ポアソン回帰と比べるとAのp値が大きくなり、過分散が解消されたことがうかがえる。 12.3 順序ロジスティック回帰 「優、良、可」といった成績や「1=当てはまらない、…, 5 = 当てはまる」といったリッカート尺度といった順序尺度はカテゴリカル変数であるので、正規分布に従う前提を置くなど量的変数のように扱うのは本来は適切ではない。二値のカテゴリカル変数の場合は二項分布を用いるロジスティック回帰で検討できたが、3つ以上のカテゴリを持つ変数の場合はどうすればよいか？ 応答変数が順序尺度である場合は、順序ロジスティック回帰(ordred logistic regression)が適切なモデルとされる。 12.3.1 例題 例えば、Scoreを試験の成績を意味する順序尺度として「1=不可、2=可、3=良、4=優、5=秀」の値を取るとする。この成績Scoreに対して、試験前日の睡眠時間Sleepが及ぼす影響を検討するとしよう。 ###サンプルデータの作成 Sleep = c(6,1,5,2,5,6,2,6,2,5,6,2,5,3,5,3,3,7,2,7,6,1,2,1,7,1,1,7,5,3) Score = c(3,3,3,2,3,3,5,5,2,2,2,3,4,1,3,2,3,5,1,4,4,3,3,3,4,1,3,3,3,2) sample_ordered = data.frame(Score = Score, Sleep = Sleep) head(sample_ordered) ## Score Sleep ## 1 3 6 ## 2 3 1 ## 3 3 5 ## 4 2 2 ## 5 3 5 ## 6 3 6 ggplot2::ggplot() + ggplot2::geom_jitter(data = sample_ordered, aes(x = Sleep, y = Score)) 12.3.2 順序ロジスティック回帰モデルの詳細 累積確率と累積ロジット 順序のあるカテゴリカル変数を扱う場合には、累積確率（cumulative probability）で各変数が生じる確率を表現する。累積確率とは、順序変数のある値以下が生じる確率のことをいう。例えば、\\(y\\)が\\(k\\)以下の値を取る累積確率を\\(Pr(y≤k)\\)と表現する。 カテゴリ\\(k\\)が生じる確率\\(p_{k}\\)とすると、\\(p_{k}\\)は累積確率を用いて以下の式で表現することができる。 \\[ p_{k}=Pr(y≤k)−Pr(y≤k−1) \\] 例えば試験の成績（1=不可、2=可、3=良、4=優、5=秀）を\\(y\\)として考えると、\\(Pr(y≤3)\\)は、試験の成績が1, 2もしくは3である確率を示している。試験の成績が3である確率\\(p_{3}\\)は、累積確率\\(Pr(y≤3)\\)から累積確率\\(Pr(y≤2)\\)を引くことで求めることができる。 なお、カテゴリの最大値が出る確率は、全体の確率から引けば求まる。例えば、試験の成績が5である確率\\(p_{5}\\)は累積確率\\(Pr(y≤5)-Pr(y≤4)\\)を計算しなくとも、\\(1-Pr(y≤4)\\)で求めることができる。 線形予測子との関係 カテゴリ\\(k\\)が得られる累積確率\\(Pr(y ≤ k)\\)は、K-1個の切片\\(\\alpha_{k}\\)で示すことができる（上で述べたように、最大カテゴリの確率は1から\\(Pr(y≤K-1)\\)を引けば求まるので、すべての確率を表現するために切片をK個用意する必要はない）。\\(\\alpha_{k}\\)は累積確率を区切るポイントを意味し、カットポイント(cutpoint)とも呼ばれる。 \\[ Pr(y≤k) = \\frac{\\exp(\\alpha_{k})}{1+\\exp(\\alpha_{k})} \\] 更に、予測変数の効果（傾き）を考慮すると、累積確率は以下のように表現できる。 \\[ \\eta = \\beta x\\\\ Pr(y ≤ k) = \\frac{\\exp(\\alpha_{k} - \\eta)}{1+\\exp(\\alpha_{k} - \\eta)} \\] 以下のように書き換えることもできる（左辺を累積確率の対数オッズ、右辺を線形の式としたもの）。 \\[ \\eta = \\beta x\\\\ \\log\\frac{Pr(y ≤ k)}{Pr(y &gt; k)} = \\alpha_{k} - \\eta \\] 各切片から傾きの効果を引いているところに注意する必要がある。引くことによって、予測変数の値が大きいほど、累積確率の値が低くなる。言い換えれば、\\(Pr(y&gt;k)\\)が大きくなる。つまり、傾きの効果を引くことによって、予測変数の値が大きくなるほど、より大きい値のカテゴリが生じる確率が大きくなることを表現できる。 このように、順序ロジスティック回帰のモデルでは、各カテゴリの累積確率を決定づける切片\\(\\alpha_{k}\\)と傾き\\(\\beta\\)を用いて、各カテゴリが生じる確率を推定する。 12.3.3 Rでの順序ロジスティック回帰 Rで順序ロジスティック回帰を行うには、外部パッケージの関数を利用する。以下では、MASSパッケージに含まれているpolr関数を使って、先ほど作成したサンプルデータsample_orderedで順序ロジスティック回帰を行う。 準備 解析の前に、Rで順序尺度を扱う場合は、変数を順序付きの因子型(factor)変数にする必要がある。factor()もしくはordered()のいずれかの方法で作成する。 #以下のいずれかの方法で因子型に変換する sample_ordered$Score = factor(sample_ordered$Score, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;), ordered = TRUE) sample_ordered$Score = ordered(sample_ordered$Score, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;)) #levelsで、水準の順序を指定する #factor()では、オプションoredered=TRUEを加える 解析 応答変数を順序付きの因子型変数に変更したら、ordinalパッケージに含まれているclm()で解析する。lm()と同じ要領で、応答変数~予測変数のモデルを書けば結果を出力してくれる。 model_ordinal = ordinal::clm(data = sample_ordered, Score ~ 1 + Sleep) summary(model_ordinal) ## formula: Score ~ 1 + Sleep ## data: sample_ordered ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 30 -38.90 87.81 6(0) 2.11e-11 5.3e+02 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## Sleep 0.4422 0.1827 2.421 0.0155 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1|2 -0.7339 0.8376 -0.876 ## 2|3 0.7129 0.7439 0.958 ## 3|4 3.1642 0.9920 3.190 ## 4|5 4.3135 1.1377 3.791 解釈 Coefficientsに予測変数に係る傾きの係数の推定値が出力されている。傾きの解釈は、一般化線形モデルのときと同じである（累積確率の対数オッズの変化量：要は係数がプラスならば、予測変数は高順位のカテゴリが生じる確率を上昇させる効果を持つと解釈すれば良い）。Interceptsに出力されているのは、順序ロジスティック回帰モデルの各切片（カットポイント）の推定値である。 12.4 多項ロジスティック回帰 先ほどの成績（1=不可、2=可、3=良、4=優、5=秀）の例では、成績はカテゴリであるが順序関係のある順序尺度であった。順序関係がないカテゴリカル変数の場合はどうすればよいか？ 応答変数が3つ以上のカテゴリの名義尺度（順序関係がない）場合は、多項ロジスティック回帰(multinomial logistic regression)が適切である。 例題 高校生が進学先（大学の学部）を選択する場合を例として考える。学部の種類（文学部、経済学部、理学部など）には順序関係はないので、学部の種類は名義尺度である。性別、高校のときの成績が学科選択に及ぼす影響を検討する。 ###サンプルデータの作成 set.seed(1) Male = c(rep(0:1, 25)) Grade = rnorm(n=50, 5, 2) Faculty = c(rep(&quot;Literature&quot;, 15), rep(&quot;Economics&quot;, 20), rep(&quot;Physical&quot;, 15)) sample_mnl = data.frame(Faculty = Faculty, Male = Male, Grade = Grade) head(sample_mnl) ## Faculty Male Grade ## 1 Literature 0 3.747092 ## 2 Literature 1 5.367287 ## 3 Literature 0 3.328743 ## 4 Literature 1 8.190562 ## 5 Literature 0 5.659016 ## 6 Literature 1 3.359063 Maleは性別（男=1, 女=0）、Gradeは高校の時の成績、Facultyは志望学部を意味する変数とする。Facultyには、Literature（文学部）、Economics（経済学部）、Physical（理学部）の3種類のカテゴリがあるとする。 12.4.1 多項ロジスティック回帰モデルの詳細 多項ロジスティック回帰では基準となるカテゴリを設定し、基準カテゴリと比べて各カテゴリが生じやすいかを推定する複数のモデルを設定する。 例えば、経済学部(Economics)を基準カテゴリとする。他のカテゴリ(Literature, Physical)が生じる確率を線形の式で表した例を以下に示す。 \\[ \\log\\frac{Pr(Literature)}{Pr(Economics)}= \\alpha_{1} + \\beta_{1,1} Male + \\beta_{2,1} Grade \\\\ \\log\\frac{Pr(Physical)}{Pr(Economics)}= \\alpha_{2} + \\beta_{1,2} Male + \\beta_{2,2} Grade \\\\ \\] 多項ロジスティック回帰では、カテゴリごとに異なる線形予測子を設定し、それぞれ異なる切片と傾きの値を推定する。 12.4.2 Rでの多項ロジスティック回帰 Rには多項ロジスティック回帰を行うための関数として、nnetパッケージのmultinom()関数がある。lm()と同じ要領でモデルを記述すると、推定結果を出力してくれる。先ほど作ったサンプルデータsample_mnlで、多項ロジスティック回帰を行ってみる。 result_mnl = nnet::multinom(data = sample_mnl, Faculty ~ 1 + Male + Grade) ## # weights: 12 (6 variable) ## initial value 54.930614 ## final value 54.309165 ## converged summary(result_mnl) ## Call: ## nnet::multinom(formula = Faculty ~ 1 + Male + Grade, data = sample_mnl) ## ## Coefficients: ## (Intercept) Male Grade ## Literature -0.3637660 -0.1228603 0.02625298 ## Physical -0.7764841 0.1628974 0.07759005 ## ## Std. Errors: ## (Intercept) Male Grade ## Literature 1.214153 0.6892241 0.2085267 ## Physical 1.243610 0.6894528 0.2104109 ## ## Residual Deviance: 108.6183 ## AIC: 120.6183 Coeffficientsの部分に、係数の推定結果が出力される。このモデルではEconomicsが基準カテゴリとなっている（デフォルトで、アルファベット順で一番はじめに出てくるカテゴリが基準となる）。Literatureの部分に出力されるのが上の式でいう\\(\\alpha_{1}\\), \\(\\beta_{1, 1}\\), \\(\\beta_{2, 1}\\)に、Physicalの部分に出力されるのが\\(\\alpha_{2}\\), \\(\\beta_{1, 2}\\), \\(\\beta_{2, 2}\\)に相当する。 Literatureの予測変数の傾きの推定値は、基準カテゴリ（Economics）と比べた上でのその予測変数の効果を意味する（その予測変数が1単位変化したときのLiteratureとEconomicsの対数オッズの変化量）。 このように、多項ロジスティック回帰の係数はある基準カテゴリと比較した上での効果を意味するため、解釈は複雑になる。 multinom()ではp値を出力してくれないので、求めたい場合は自分で計算する必要がある。以下には、z scoreを元に計算する方法を示す。 #p値の出力 z = summary(result_mnl)$coefficients/summary(result_mnl)$standard.errors p = (1 - pnorm(abs(z), mean = 0, sd = 1)) * 2 p ## (Intercept) Male Grade ## Literature 0.7644787 0.8585197 0.8998131 ## Physical 0.5323786 0.8132228 0.7123104 確認問題 MASSパッケージに入っているサンプルデータ、housingを使って練習をする。 d = housing #dという名前で保存する d$ID = 1:nrow(d) head(d) ## Sat Infl Type Cont Freq ID ## 1 Low Low Tower Low 21 1 ## 2 Medium Low Tower Low 21 2 ## 3 High Low Tower Low 28 3 ## 4 Low Medium Tower Low 34 4 ## 5 Medium Medium Tower Low 22 5 ## 6 High Medium Tower Low 36 6 (1)Freqを応答変数、Contを予測変数としたポアソン回帰と、(2)同じくFreqを応答変数、Contを予測変数とした負の二項回帰を行い、結果を比較せよ。 "],["12-glmm.html", "Chapter 13 マルチレベルモデル 13.1 準備 13.2 個人差や集団差の問題 13.3 マルチレベルモデルの概要 13.4 Rでのマルチレベルモデル 13.5 正規分布以外を扱う例 確認問題", " Chapter 13 マルチレベルモデル 一般化線形モデルを拡張し、個人差や集団差を扱うモデルについて学ぶ。 13.1 準備 ggplot2パッケージに加え、新たにlme4及びlmerTestというパッケージを使う。lme4とlmerTestは初めて使うので、インストールした上でロードしよう。 library(ggplot2) install.packages(&quot;lme4&quot;, &quot;lmerTest&quot;) library(lme4) library(lmerTest) 13.2 個人差や集団差の問題 以下では、Rにデフォルトで入っている iris データを例として使う。 head(iris) #irisデータの上数行を表示 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa まず、がくの長さ（Sepal.Length）とがくの幅（Sepal.Width）の関係を散布図で示してみよう。 graph_1 = ggplot2::ggplot() + ggplot2::geom_point(data=iris, aes(x=Sepal.Length, y=Sepal.Width),size = 3) graph_1 まず、lm()を使って、がくの長さを応答変数、がくの幅を予測変数とした線形モデルで係数を推定する。 iris_lm = lm(data = iris, Sepal.Length ~ 1 + Sepal.Width) summary(iris_lm) ## ## Call: ## lm(formula = Sepal.Length ~ 1 + Sepal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.5561 -0.6333 -0.1120 0.5579 2.2226 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.5262 0.4789 13.63 &lt;2e-16 *** ## Sepal.Width -0.2234 0.1551 -1.44 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8251 on 148 degrees of freedom ## Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 ## F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 推定された切片及び傾きの値から予測直線を引くと、以下のようになる。 graph_lm = ggplot2::ggplot()+ ggplot2::geom_point(data = iris, aes(x = Sepal.Width, y = Sepal.Length), size = 3) + ggplot2::geom_smooth(data = iris, aes(x = Sepal.Width, y = Sepal.Length), formula = y~ 1 + x, method = &quot;lm&quot;, se = FALSE) graph_lm がくの幅（Sepal.Width）は、がくの長さに対して負の影響を持っているように見える。 では、この散布図を種（Species）ごとに色わけして示してみる。 graph_2 = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Width, y = Sepal.Length, color = Species, shape = Species), size = 3) graph_2 種を無視して検討したところ、がくの幅と長さの間には負の関係があるようにみえたが、種ごとに分けてみると「がくの長さが大きくなるほど、がくの幅が大きくなる」関係にあるように見える。 このあやめのデータのように、いくつかのデータが同じグループに属している構造の場合、グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には、統計的独立性が保証されていないためである。つまり、同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。 独立(independence)とは、各データが他のデータに影響されないという意味である。これまで学んできた確率分布では、独立同分布(independent and identically distributed: i.i.d.)が前提とされている。例えば、コインを数回投げて表が出る回数は二項分布に従うが、表が出るかどうかは前の試行に影響されることはない（前回表が出たら、次も表が出やすいということはありえないという前提を置く）。 しかし、現実のデータでは、データ間の相関などにより、事象の独立性が保たれていないケースもありえる。その場合、統計的独立性を前提とした解析を行うと、上の例のように誤った結論を導いてしまう恐れがある。 この例に限らず、階層構造を持つデータや繰り返し測定データにも、同じことがいえる。例えば、学校ごとに学力テストを行った場合、同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合、その参加者のデータは似たような傾向になる可能性も考えられる。 このようなデータに対して、個人や集団の影響を考慮した統計モデルとして、マルチレベルモデル(multilevel model)が提案されている。 マルチレベルモデルは、「階層モデル(hierarchical model)」、「混合モデル(mixied model)」など、色々な呼ばれ方がされている。 13.3 マルチレベルモデルの概要 マルチレベルモデルでは、予測変数が応答変数に及ぼす効果だけではなく、個人や集団の効果を扱う。予測変数そのものの効果は固定効果（fixed effect）と呼ばれ、個人や集団ごとの効果はランダム効果（random effect）と呼ばれて区別される。前章まで扱ってきた、一般化線形モデルは固定効果のみを含むモデルである。 例として、 繰り返し測定されたデータを扱う。以下のプログラムを実行して、サンプルデータexampleを作ろう。 set.seed(1) example = data.frame(i = 1:6, j = c(1, 1, 2, 2, 3, 3), y = round(rnorm(6), 2), x = rep(c(0, 1),3) ) example ## i j y x ## 1 1 1 -0.63 0 ## 2 2 1 0.18 1 ## 3 3 2 -0.84 0 ## 4 4 2 1.60 1 ## 5 5 3 0.33 0 ## 6 6 3 -0.82 1 \\(i\\)がデータを意味する番号（何行目か）、\\(j\\)を個人もしくはグループを意味する番号とする。例えば、個人\\(j\\)が\\(x=0\\)の場合と\\(x=1\\)の場合の2回\\(y\\)を測定している、あるいは同じ集団\\(j\\)から2人が選ばれてそれぞれの人について\\(y\\)が測定された、といったケースが当てはまる。 一般化線形モデルの線形予測子は、以下のような数式で表現できた。 \\[ \\mu_{i} = \\alpha + \\beta x_{i} \\tag{1}\\\\ y_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma) \\] \\(\\alpha\\)が切片、\\(\\beta\\)が予測変数\\(x\\)に係る傾きであった。 これに対し、マルチレベルモデルでは、以下のように線形予測子に\\(\\alpha_{j}\\)が加わる。 \\[ \\mu_{i} = \\alpha_{0} + \\beta x_{i} + \\alpha_{j} \\tag{2} \\\\ \\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma) \\] すべての個人に共通して影響する切片\\(\\alpha_{0}\\)に加え、個人ないしはグループごとに異なる切片\\(\\alpha_{j}\\)を考慮する。 更に、\\(\\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\)にあるように、個人ごとの切片\\(\\alpha_{j}\\)が「平均をゼロ、\\(\\sigma_{\\alpha}\\)を標準偏差とする正規分布から生成される」という仮定を置く。 これにより、同じグループに属するデータ（例えば\\(j=1\\)）には同じ効果（\\(\\alpha_{1}\\)）が共通して係ることを表現できる。 傾きを\\(\\beta_{ j}\\)にする、すなわち個人ごとに予測変数に係る効果が異なるという前提を置くこともできる。しかし、実際に傾きをランダム効果としたモデルは複雑で推定するのは困難であるため（最尤推定法では解が求まらない場合がある）、多くの場合、個人差の影響（ランダム効果）は切片のみを考慮したモデルで表現されることが多い。ランダム傾きを含むマルチレベルモデルを扱う際には、ベイズ統計の手法が必要になる。 13.4 Rでのマルチレベルモデル Rでマルチレベルモデルで解析を行うためには、外部パッケージが必要になる。様々なパッケージがあるが、lme4パッケージが扱いやすい。以下では、lme4パッケージに含まれるglmer()を使った解析の例を示す。 基本的に、lm()関数と似た表記で使うことができる。ランダム切片は、(1|グループを意味する変数名)のかたちで線形予測子に入れる。 model_lmm = lme4::lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) #(1|Species)を加える summary(model_lmm) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 3.4062 0.6683 5.097 ## Sepal.Width 0.7972 0.1062 7.506 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 出力結果を見てみると、Fixed effectsという部分がある。ここに、固定効果の推定結果が表示される。見方は一般化線形モデルのときと同じである。切片(intercept)と予測変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず、すべての個体共通に係る予測変数の効果）。 がくの幅（Sepal.Width)の回帰係数（Estimate）を見ると、lm()での推定結果とは逆に、プラスになっている。やはり、グループの違いを統制すると、実際にはがくの幅が大きくなるほど、がくの長さも大きくなる関係にあることが、lmer()による推定結果からわかる。 lmer()では、デフォルトで係数のp値は表示されない。p値も出したいならば、lmerTest()パッケージをインストールしておく必要がある。 model_lmm = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) summary(model_lmm) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.4062 0.6683 3.4050 5.097 0.0107 * ## Sepal.Width 0.7972 0.1062 146.6648 7.506 5.45e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 他には、直接p値を計算する方法ではないが、信頼区間を算出して有意かどうか（信頼区間にゼロが含まれていないか）を判断するという手もある。 model_lmm = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) confint(model_lmm, level = 0.95) #confintで信頼区間を計算する。デフォルトで95%信頼区間が出力される（levelで範囲を指定可能）。 ## 2.5 % 97.5 % ## .sig01 0.4320405 2.4380677 ## .sigma 0.3909640 0.4915558 ## (Intercept) 1.9780097 4.8131087 ## Sepal.Width 0.5844733 1.0030191 13.5 正規分布以外を扱う例 13.5.1 ロジスティック回帰 応答変数が正規分布以外に従う場合のマルチレベルモデルについても見ていこう。 lme4パッケージのglmer()で、正規分布以外の確率分布を指定したマルチレベルモデルの解析を行うことができる。以下では、ランダム効果を加えたロジスティック回帰分析の例を示す。 まず、以下のプログラムを実行してサンプルデータdata_sampleを作ろう。 x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0) y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) ID = c(rep(&quot;a&quot;,5),rep(&quot;b&quot;,5),rep(&quot;c&quot;,5)) data_sample = data.frame(ID, x1, y1) head(data_sample) ## ID x1 y1 ## 1 a 1.0 0 ## 2 a 2.0 0 ## 3 a 3.0 1 ## 4 a 4.2 1 ## 5 a 5.1 1 ## 6 b 3.1 0 x1を予測変数（量的変数）、y1を応答変数（0か1のいずれかを取る）、IDが個体を示す変数とする。1つの個体からx1を変えて5回、y1が計測がされた実験をイメージしてほしい。 予測変数と応答変数の関係に、個体特有の効果を加えたモデルは以下となる。 \\[ q = \\frac{\\exp(\\alpha_{0} + \\beta x + \\alpha_{j})}{1+\\exp(\\alpha_{0} + \\beta x + \\alpha_{j})} \\tag{3} \\\\ \\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y \\sim \\text{Binomial}(1, q) \\] 線形予測子をロジット（逆ロジット）変換して、\\(y=1\\)が生じる確率\\(q\\)を求める。応答変数\\(y\\)は、\\(q\\)をパラメータとする二項分布から生成される。これらの点は、一般化線形モデルで学んだ。 更に、線形予測子に、ランダム切片\\(\\alpha_{j}\\)を加えた。\\(\\alpha_{j}\\)は、平均ゼロ、標準偏差\\(\\sigma_{\\alpha}\\)の正規分布に従って生成されるとする。 正規分布以外の確率分布を扱うマルチレベルは、Rではlme4パッケージのglmer()で扱うことができる。さっきのlmer()と同じ要領で、線形予測子に個体を識別する変数（ID）を加える。以下のように、(1|ID)というかたちで入れる。 あとは、確率分布とリンク関数を指定する。指定の仕方は、glm()のときと同じ要領である。確率分布はbinomial（二項分布）、リンク関数はlogit（ロジット関数）を指定する。リンク関数の指定は省略しても構わない（二項分布を指定すれば、デフォルトでロジット関数を選択してくれる）。 model_logistic_glmm = lme4::glmer(data = data_sample, y1 ~ 1 + x1 + (1|ID), family = binomial(link=&quot;logit&quot;)) summary(model_logistic_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: y1 ~ 1 + x1 + (1 | ID) ## Data: data_sample ## ## AIC BIC logLik deviance df.resid ## 14.3 16.4 -4.2 8.3 12 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.01928 0.00000 0.00000 0.00000 0.04031 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 67795 260.4 ## Number of obs: 15, groups: ID, 3 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -220.575 31.594 -6.981 2.92e-12 *** ## x1 38.996 5.572 6.998 2.60e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x1 -0.990 13.5.2 ポアソン回帰 同じく、lme4パッケージのglmer()を使う。確率分布はpoisson（ポアソン分布）、リンク関数はlog（対数）を指定する。リンク関数の指定は省略しても構わない（ポアソン分布を指定すれば、デフォルトで対数関数を選択してくれる）。 set.seed(1) alpha = 0.5 beta = 0.2 x = rnorm(n=50, mean = 0, sd = 1) alpha_0 = rnorm(n=5, mean = 0, sd = 0.2) lambda_1 = exp(alpha + beta * x[1:10] + alpha_0[1]) lambda_2 = exp(alpha + beta * x[11:20] + alpha_0[2]) lambda_3 = exp(alpha + beta * x[21:30] + alpha_0[3]) lambda_4 = exp(alpha + beta * x[31:40] + alpha_0[4]) lambda_5 = exp(alpha + beta * x[41:50] + alpha_0[5]) y_1 = rpois(n = 10, lambda_1) y_2 = rpois(n = 10, lambda_2) y_3 = rpois(n = 10, lambda_3) y_4 = rpois(n = 10, lambda_4) y_5 = rpois(n = 10, lambda_5) dat = data.frame(y = c(y_1, y_2, y_3, y_4, y_5), x = x + 20, ID = sort(rep(1:5, 10))) dat$x_std = dat$x - mean(dat$x) ggplot2::ggplot() + ggplot2::geom_point(data = dat, aes(x = x, y = y)) + ggplot2::facet_wrap(vars(factor(ID))) result = lme4::glmer(data = dat, y ~ 1 + x_std + (1|ID), family = poisson(link=&quot;log&quot;)) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(result) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ 1 + x_std + (1 | ID) ## Data: dat ## ## AIC BIC logLik deviance df.resid ## 161.4 167.2 -77.7 155.4 47 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4045 -0.6295 -0.0597 0.4675 3.2093 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0 0 ## Number of obs: 50, groups: ID, 5 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4235 0.1157 3.660 0.000253 *** ## x_std 0.2560 0.1474 1.737 0.082439 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x_std -0.206 ## optimizer (Nelder_Mead) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 13.5.3 マルチレベルモデルによる過分散への対処 ポアソン分布の理論的な分散よりも実際のデータの分散が大きい「過分散」がある場合には、ポアソン回帰の結果が信頼できなくなる問題があった。 #サンプルデータの作成 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd = 1) e = rnorm(n=N, mean = 0, sd = 1) lambda = exp(0.01 + 0.1*x + e) y = rpois(n=N, lambda = lambda) dat_dis = data.frame(y=y, x=x) result_dis = glm(data = dat_dis, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_dis) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_dis) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9022 -1.3362 -0.6405 0.3623 6.2029 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1417 0.3139 -0.451 0.65177 ## x 0.4393 0.1268 3.465 0.00053 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.37 on 49 degrees of freedom ## Residual deviance: 153.39 on 48 degrees of freedom ## AIC: 260.41 ## ## Number of Fisher Scoring iterations: 6 library(performance) performance::check_overdispersion(result_dis) #過分散のチェック ## # Overdispersion test ## ## dispersion ratio = 4.048 ## Pearson&#39;s Chi-Squared = 194.291 ## p-value = &lt; 0.001 ## Overdispersion detected. マルチレベルモデルでは、過分散の問題にも対処することができる。 \\[ \\lambda_{i} = \\alpha_{0} + \\beta x_{i} + \\alpha_{i} \\tag{2} \\\\ \\alpha_{i} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y_{i} \\sim \\text{Poisson}(\\lambda_{i}) \\] 観測値ごとのランダム切片（\\(\\alpha_{i}\\)）を加えて、余分な分散を別のパラメータ（\\(\\sigma_{\\alpha}\\)）で表現する。 dat_dis$ID = 1:nrow(dat_dis) #観測値ごとに番号を割り振る（1からデータ数までの数値の連続） head(dat_dis) ## y x ID ## 1 1 1.373546 1 ## 2 0 2.183643 2 ## 3 1 1.164371 3 ## 4 0 3.595281 4 ## 5 3 2.329508 5 ## 6 8 1.179532 6 result_poisson_glmm = lme4::glmer(data = dat_dis, y ~ 1 + x + (1|ID), family = poisson(link = &quot;log&quot;)) summary(result_poisson_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ 1 + x + (1 | ID) ## Data: dat_dis ## ## AIC BIC logLik deviance df.resid ## 202.3 208.0 -98.1 196.3 47 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.9658 -0.2773 -0.1231 0.3444 0.8207 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.7871 0.8872 ## Number of obs: 50, groups: ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.01303 0.47791 -0.027 0.978 ## x 0.18846 0.20904 0.902 0.367 ## ## Correlation of Fixed Effects: ## (Intr) ## x -0.921 ランダム切片を入れない通常のポアソン回帰では予測変数xはyに対して有意な効果を有していたが、観測値ごとのランダム切片を加えたマルチレベルモデルでは、xの係数が小さくなり有意な効果も見られなくなった。通常のポアソン回帰では過分散の影響でxの効果を過剰に評価していたことが伺える。余計な分散をランダム切片に吸収させることで、予測変数が持つ効果を推定することができた。 確認問題 問１ carパッケージに入っているカナダにおける職業の威信度に関する調査データPrestigeを使う。102業種に関する調査結果が入っている。 library(car) head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof prestigeを応答変数、education, income及び womenを予測変数、typeをランダム効果（切片）としたマルチレベルモデルで解析せよ。応答変数が従う確率分布は、正規分布を用いるものとする。 各予測変数が応答変数に及ぼす効果について述べよ（その予測変数が1単位変化すると、応答変数がどう変化するか）。 なお、変数の意味は以下の通りである。 prestige：職業威信度（値が高いほど威信度が高い） education：在職者の平均教育年数 income：平均所得（単位はドル） women：女性の割合 type：職業のカテゴリ（bc=ブルーカラー、wc=ホワイトカラー、prof=専門職） ヒント：正規分布を扱うマルチレベルの場合は、lme4パッケージのlmer()を使えば良い。なお、出力時にメッセージが出ても無視して良い（中心化せよという命令だが、無視して良い）。 "],["13-Bayese.html", "Chapter 14 ベイズ統計 14.1 頻度主義統計学とベイズ統計学の違い 14.2 ベイズの定理 14.3 ベイズ推定の例 14.4 MCMC 確認問題", " Chapter 14 ベイズ統計 これまでの章で扱ってきた帰無仮説検定に基づく統計手法は、頻度主義の枠組みに分類される。これに対し、ベイズ統計学という別の統計学の流派が存在する。この章では、ベイズ統計学の基礎について触れる。 14.1 頻度主義統計学とベイズ統計学の違い 両者の違いは、パラメータ（母数）の値をどう捉えるかの違いである。 頻度主義統計学では、パラメータが定数（固定された一定の値）であるという仮説を立て、データを確率変数とみなしてデータが生じる確率を推定するという考え方を置く。例えば平均値の差の有無を調べる統計的仮説検定では、パラメータである2つの母集団の平均値が等しい（\\(\\mu_{1} = \\mu_{2}\\)）という前提を置き、帰無仮説のもとでデータよりもまれな結果が生じる確率であるp値を求めた。 頻度主義は、以下の式で表される尤度（あるパラメータ\\(\\theta\\)のもとでデータxが生じる確率）を求めることに相当する。 \\[ Pr(x|\\theta) \\] これに対し、ベイズ統計学ではパラメータを確率的に変動する値であると考える。データは固定された値であり、未知であるパラメータの分布を推定するのがベイズ統計学の考え方である。以下の式で表されるように、ベイズ統計は頻度主義に対しデータxのもとにおけるパラメータ\\(\\theta\\)が生じる確率を求めることに相当する。 \\[ Pr(\\theta|x) \\] ベイズ統計では、「母集団の平均値の分布が-1〜1に分布する確率は90%である」といった範囲を求める。データからパラメータの分布を推定する手法を、ベイズ推定と呼ぶ。 14.2 ベイズの定理 まず、ベイズの定理について整理する。ベイズの定理とは、条件付き確率を求めるための定理である。 条件付き確率とは、ある条件のもとである事象が生じる確率のことをいう。例えば、「事象Bが起こったときに事象Aが生じる」条件付き確率を\\(Pr(A|B)\\)と表現したとする。 ベイズの定理は、以下で表される。\\(\\bar{A}\\)はAの余事象（Aではない事象）とする。 \\[ Pr(A|B) = \\frac{Pr(B|A) Pr(A)}{Pr(B)}\\\\ = \\frac{Pr(B|A) Pr(A)}{Pr(B|A) Pr(A) + Pr(B|\\bar{A}) Pr(\\bar{A})} \\] 14.2.1 例題 ベイズの定理を使って、ある条件付き確率を求めてみよう。 ある感染症に感染している確率は0.1%だとする。 ある感染症の検査方法を受けると、99%の確率で感染者に対して陽性反応が出ることがわかっている。その一方、1%の確率で感染していない者に対して陽性反応が出ることがわかっている。 あなたがこの検査を受けたとき、陽性反応が出た。 あなたが本当にその感染症にかかっている確率は何%か？ 陽性と診断されたときに感染している確率を、ベイズの定理で解いてみる。 先程のベイズの定理の式について、A=感染、B=陽性に置き換えて考えると、 \\[ Pr(感染|陽性) = \\frac{Pr(陽性|感染) Pr(感染)}{Pr(陽性)}\\\\ = \\frac{Pr(陽性|感染) Pr(感染)}{Pr(陽性|感染) Pr(感染) + Pr(陽性|非感染) Pr(非感染)} \\] つまり、陽性と診断される確率（分母）のうち、感染かつ陽性である確率（分子）の占める割合が、「陽性と診断されたときに感染している確率」を意味する。 問題文より、 Pr(感染) = 0.001（感染している確率0.1%） Pr(非感染) = 0.999（感染していない確率[1- 0.1%]） Pr(陽性|感染) = 0.99（感染者が陽性と正しく診断される確率99%） Pr(陽性|非感染) = 0.01（非感染者が陽性と誤って診断される確率1%） これらを当てはめると、 \\[ Pr(感染|陽性) = \\frac{0.99 \\times 0.001}{0.99 \\times 0.001 + 0.01 \\times 0.999} = 0.09 \\] つまり、実際に感染している確率は約9%ということになる。 14.2.2 事前確率、事後確率 もう一度ベイズの定理をおさらいすると、 \\[ Pr(A|B) = \\frac{Pr(B|A) Pr(A)}{Pr(B)} \\] \\(Pr(A)\\)は事前確率(prior probability)、\\(Pr(B|A)\\)は尤度(likelihood)、\\(Pr(A|B)\\)は事後確率(posterior probability)という。ベイズ推定は、もとの事前確率について与えられたデータをもとに事後確率へと更新するプロセスである。 14.2.3 ベイズ統計 データxのもとにおけるパラメータ\\(\\theta\\)の分布は、ベイズの定理から以下のように求められる。 \\[ Pr(\\theta|x) = \\frac{Pr(x|\\theta) Pr(\\theta)}{Pr(x)} \\] \\(Pr(\\theta|x)\\)を事後分布(posterior distribution)、\\(Pr(\\theta)\\)を事前分布(prior distribution)、\\(Pr(x|\\theta)\\)を尤度(likelihood)と呼ぶ。 \\(Pr(x)\\)はデータの分布を意味しているが、確率変数を含まないので定数とみなすことができる（基準化定数や周辺尤度と呼ばれる）。したがって、この式は以下のようなかたちで表現されることもある。\\(\\propto\\)は「比例する」という意味である。 \\[ Pr(\\theta|x) \\propto Pr(x|\\theta) Pr(\\theta) \\] 事後分布はデータを得たあとで推定したパラメータの分布であり、事前分布と尤度の積に比例する。 14.3 ベイズ推定の例 具体的な例を使って、ベイズ推定を行ってみよう。 手元にゆがんだコインがあるとする。このコインの表が出る確率は不明なので、何回かコインを投げて推定することにする。このコインを10回投げたら、6回表が出た。このデータから、コインの表が出る確率\\(\\theta\\)の事後分布を推定しよう。 14.3.1 事前分布 まずは、コインを投げる前に考えるコインの表が出る確率\\(\\theta\\)である、\\(\\theta\\)の事前分布を設定する。しかし、事前分布をどう設定するかについては、研究者の恣意性が介入してしまう恐れがある。 仮説について情報がないときに設定する事前分布は無情報事前分布と呼ばれ、一様分布が無情報事前分布として採用される。一様分布は、どの値も生じる確率が一定であるという前提の分布であるので、恣意性を排除できる。 この例では、表が出る確率\\(\\theta\\)の事前分布を0から1の範囲の連続一様分布に設定する。連続一様分布は確率密度関数なので、縦軸は確率そのものを意味しないので注意（面積が確率を意味する。0から1までの範囲の面積が1となっている）。 \\[ Pr(\\theta) = 1 \\] theta_seq = seq(0,1,0.01) #0から1までの範囲で0.01刻みでベクトルを作る theta_prior = rep(1, length(theta_seq)) もちろん事前確率について仮説がある場合は、範囲を定めた一様分布あるいは別の確率分布を事前確率を設定しても良い。例えば、身長の分布を推定するときは、0cm ~ 250cmの範囲の一様分布を事前分布として設定するのは妥当であるといえる。 14.3.2 尤度 データから尤度を求める。コインで表が出た回数は二項分布に従うので、尤度は以下から求められる(nはコイン投げの総数、hは表が出た回数とする)。 \\[ Pr(x|\\theta) = {}_n\\mathrm{C}_h\\theta^{h}(1-\\theta)^{(n-h)} \\] theta_likelihood = dbinom(x=6, size=10, prob = theta_seq) 14.3.3 事後分布 ベイズの定理をもとに、事後分布を求める。先程の式より、 \\[ Pr(\\theta|x) \\propto Pr(x|\\theta) Pr(\\theta) \\] 尤度と事前分布をかけて事後分布を求める（正確には、更に基準化定数\\(Pr(x)\\)で割る）。 theta_posterior = theta_likelihood * theta_prior theta_posterior_std = theta_posterior/sum(theta_posterior) 14.4 MCMC パラメータが多くて複雑なモデルになると、解析的に事後分布の推定は困難になる。 事後分布をベイズ推定する方法として、コンピュータの乱数を使ってシミュレーションで事後分布を推定するマルコフ連鎖モンテカルロ法（Markov Chain Monte Carlo）が提案されている。 モンテカルロ法とは乱数を発生させて近似的に解を求める手法である（カジノで有名な土地モンテカルロに由来する）。マルコフ連鎖とは、ある状態に移る確率が現在の状態のみに依存する確率過程を意味する。例えば、今日晴れならば明日も晴れやすい（ただし、2日前の天気は明日の天気に影響しない）という過程を意味する。MCMCではパラメータの推定に乱数を生成して事後分布を評価し、その結果に応じて次の乱数を決めるというアルゴリズムでパラメータの分布を推定する手法である。MCMCはアルゴリズムの総称で、メトロポリス法やギブスサンプリング法など様々な手法が提案されている。 MCMCでは、コンピュータのシミュレーションでパラメータの事後分布を求める。近年、StanをはじめとしたMCMCを行うためのソフトウェアが開発されている。 14.4.1 MCMCの例 さきほどのコイン投げで表が出る確率\\(\\theta\\)の事後分布をMCMCで推定する方法を通して、MCMCの全体像について説明する。 MCMCでは、まずパラメータの初期値を適当に選び（例えば\\(\\theta_{1}\\)とする）、それを元に事後分布\\(Pr(\\theta_{1}|x)\\)に従う乱数を生成する。次に、\\(Pr(\\theta_{1}|x)\\)をある基準で評価した上で、その評価を元に新たなパラメータ\\(\\theta_{2}\\)を元に乱数を生成する。このようなシミュレーションを何回も繰り返す。何回も繰り返していくうち、生成される乱数はある分布に収束していく（定常分布と呼ばれる）。最終的に出来上がった定常分布をパラメータの事後分布として採用するというのが、（非常に大雑把な）MCMCの概要である。 例えば以下に、シミュレーションを2,000回行ったときのパラメータ\\(\\theta\\)の推定の推移を示している。最初の1,000回の推定結果は適当に選んだ初期値に依存するため、これらは切り捨てて（warmup期間と呼ばれる）、残りの1,000回のシミュレーション結果（MCMCサンプル）を事後分布として採用する。 事後分布をプロットしたのが、以下である。 この分布の中央値は0.59であった。10回中6回表が出たので、表が出る確率0.60と概ね一致している。また、分布の中央95%を占める部分の範囲(下位2.5%から上位97.5%)は、0.29から0.83であった。つまり、表が出る確率は95%の確率で0.29から0.83の範囲を取ることを意味する。ベイズ推定で求めたパラメータの分布の範囲は、信用区間(credible interval)と呼ばれる。 確認問題 ベイズの定理を使って、事後確率を求める方法を復習しよう。 問1 3つの袋それぞれに、玉が100個入っている。袋Aには玉100個のうち赤玉が30個、袋Bには玉100個のうち赤玉が70個、袋Cには玉100個のうち赤玉が50個入っていて、残りは黒玉である。3つの袋の中から一つをランダムに選んで、玉を1個取り出すとする。 今、あなたの目の前に赤玉が1個ある。 1-1 この赤玉が袋Aから取り出された確率を求めよ。 1-2 この赤玉が袋Bから取り出された確率を求めよ。 1-3 この赤玉が袋Cから取り出された確率を求めよ。 ヒント: 赤玉があるときに、それが袋A、袋B、もしくは袋Cから取り出された条件付き確率\\(Pr(A|赤)\\)を求める。例えば、1-1の場合は、赤玉が取り出される確率（袋Aから赤玉が取り出される確率、袋Bから赤玉が取り出される確率、袋Cから赤玉が取り出される確率の合計）のうち、袋Aから赤玉が取り出される確率が占める割合を求めれば良い。 問2 世の中に出回っている迷惑メールは全メール中80%であることがわかっている。 迷惑メールのうち80%には「無料」という単語が含まれている、普通のメールには「無料」が50%含まれている事がわかっている。「無料」を含むメールが送られてきたときに、このメールが迷惑メールである確率は何％か？ ヒント: 感染症の問題を復習しよう。 "],["14-Bayese_model.html", "Chapter 15 ベイズ統計モデリング 15.1 準備 15.2 Rによるベイズ統計モデリング 15.3 ベイズ統計モデリングのプロセス 15.4 brmsパッケージでの一般化線形モデル 15.5 brmsパッケージでのマルチレベルモデル 15.6 その他", " Chapter 15 ベイズ統計モデリング この章では、これまで学んできた一般化線形モデルなどの解析をベイズ統計の枠組みで行う方法について解説する。rstanとbrmsパッケージを使い、一般化線形モデルやマルチレベルモデルのパラメータの事後分布をMCMCで推定する。 この章で書かれている内容を行う前に、前の章で説明したベイズ統計の概要を理解しておくこと（前の章を読んでいるという前提で説明をする）。「事前分布」、「事後分布」、「MCMC」について、前の章で確認しておくこと。 15.1 準備 15.1.1 Rtanのインストール MCMCを行うために、Stanと呼ばれるプラットフォームが必要にある。RStanはRからStanを使うために開発されたインターフェースである。この章の内容の解析を行うためには、RStanパッケージのインストールが事前に必要となる。 Rstan（rstan）のインストール方法については、「Rstan Getting Started (Japanese)」https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese)のページを参照のこと。「Rtool」、「C++コンパイラ（MacならばXCode）」のインストールも必要になる。 15.1.2 brmsパッケージのインストール rstanをインストールできたら、brmsパッケージもインストールする。 Stanを使ってベイズ推定（MCMC）を行うためには、Stan言語で解析用のコードを書く必要があるが、brmsパッケージを使えばユーザーがコードを書く必要なく、線形のモデルやオプションを指定するだけで自動でStanコードを生成してMCMCを行ってくれる。 install.packages(&quot;brms&quot;) 15.1.3 パッケージのロード rstanとbrmsをロードする。 library(rstan) library(brms) また、計算の高速化のために、以下のプログラムも実行しておく。 #計算を高速化するオプション rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) 15.2 Rによるベイズ統計モデリング 回帰分析のパラメータ（傾きと切片）のベイズ推定を例として、brmsパッケージを使った解析の手順について確認していく。 第9章で、irisデータを用いて以下の回帰分析を行った。 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 同じデータについてbrmsパッケージのbrm()を使って、MCMCでパラメータの事後分布を推定してみよう。glm()と同じ書き方で、線形予測子、確率分布、リンク関数を指定すれば良い。回帰分析なので、確率分布は正規分布(gaussian)、リンク関数は恒等リンク(identity)とする。 また、オプションのseedには常に一定の値を指定すること（ここでは1と指定した）。MCMCは乱数を使ったシミュレーションであるので、やり直すたびに微妙に異なる結果が得られる可能性にある。しかし、seedに同じ値を設定すれば、やり直しても同じ結果を再現することできる。 result_brms_lm = brms::brm(data = iris, Petal.Length ~ 1 + Sepal.Length, family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(result_brms_lm) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Petal.Length ~ 1 + Sepal.Length ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -7.10 0.51 -8.09 -6.13 1.00 4301 2949 ## Sepal.Length 1.86 0.09 1.69 2.03 1.00 4316 3046 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.88 0.05 0.78 0.98 1.00 3883 3077 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 最初に、応答変数が従う確率分布(Family)、リンク関数(Links)、線形予測子(Formula)、データ(Data)の情報が出力される。更に、Samplesの部分にMCMCの設定が表記されているが、これらの意味については後ほど説明する。 lm()の出力と同様に、Population-Level Effectsという部分に各パラメータ（切片と傾き）の推定結果が出力されている。Estimateが係数の事後分布の期待値を示している。lm()で解析したときとほぼ同じ値が推定されている。 他にも、l-95% CIとu-95% CIといった数値が出力されているが、これらの意味を理解するにはパラメータの事後分布を図示するとよい。plot()に出力結果を入れると、MCMCの結果を図で示してくれる。 plot(result_brms_lm) 左側に表示されているのが、パラメータの事後分布（MCMCでサンプリングされた事後分布に従う乱数の分布）である。右側は、トレースプロット(traceplot)と呼ばれるものである。トレースプロットの下の軸はMCMCの試行数を示しており、MCMCのサンプリング結果の推移を示している。 もう一度brm()の出力のl-95% CIとu-95% CIの値を確認し、図との関係を確認しよう。l-95% CIとu-95%は、パラメータの事後分布の下位5%点と上位95%点の値を示しており、この下位5%から上位95%の範囲は95% 信用区間(credible intervals)と呼ばれる。95%信用区間とは、95%の確率で真のパラメータの値が含まれる範囲を意味する。 例えば、切片（intercept）の事後分布のプロットを確認すると、だいたい-7が分布の中央に位置しており、推定された切片の事後分布の期待値（Estimate）と概ね一致しているのがわかる。また、-8から-6の範囲に事後分布の大部分が締めており、これも95%信用区間の推定結果と概ね一致している。 最尤推定法（最小二乗法）によるパラメータ推定であるlm()の出力結果とは異なり、ベイズ推定であるbrm()の結果ではp値が表示されていていない点に注目しよう。前の章でも述べたように、データを定数、パラメータを確率変数として考えるベイズ統計には、統計的仮説検定の枠組みで扱うp値（帰無仮説のもとでデータが得られる確率）という概念はない。 ベイズ推定による一般化線形モデルで係数の効果に意味があるかを議論したいのならば、パラメータの信用区間について注目する。brm()の出力結果で係数の信用区間にゼロが含まれているかどうかが、lm()のp値が示す結果（係数がゼロから有意に離れているか）と対応している。 15.3 ベイズ統計モデリングのプロセス brm()のプログラムの書き方を確認しながら、ベイズ統計モデリングの手順について確認していこう。 15.3.1 事前分布の設定 パラメータの事後分布を推定するためには、まずパラメータ（切片と傾き）の事前分布を設定する必要がある。何か仮説があって事前にパラメータの範囲を設けることに正当な理由があるのならば、任意の範囲を設定しても構わない。例えば、身長を予測するならば切片の事前分布として0cm - 300cmの一様分布を設定するというのは妥当であろう。それに対し、特に仮説がない、パラメータの事前分布について確信がない場合は、無情報事前分布(non-informative prior)を設定する。 特に仮説がなければ、brm()では何も設定する必要はない。自動で事前分布を無情報事前分布としてくれる（例えば傾きなどをフラットな一様分布に設定してくれる）。 get_prior()に、モデル、確率分布、リンク関数を指定すれば、設定される事前分布を推定の前に確認することができる。 brms::get_prior(data= iris, Sepal.Length ~ 1 + Sepal.Width, family = gaussian(link=&quot;identity&quot;)) ## prior class coef group resp dpar nlpar lb ub ## (flat) b ## (flat) b Sepal.Width ## student_t(3, 5.8, 2.5) Intercept ## student_t(3, 0, 2.5) sigma 0 ## source ## default ## (vectorized) ## default ## default 事前分布を任意に指定したい場合は、brm()のオプションとしてset_prior()で設定することができる。以下に、プログラムの例を示す。 result_brm_lm = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width, family = gaussian(link=&quot;identity&quot;), prior = c(set_prior(&quot;normal(0,10)&quot;, class = &quot;b&quot;), #傾きbの事前分布を平均0, 標準偏差10の正規分布に設定 set_prior(&quot;cauchy(0,5)&quot;, class = &quot;sigma&quot;) #正規分布の分散の事前分布を半コーシー分布に設定 ), seed = 1 ) brms::prior_summary(result_brm_lm) #prior_summaryに結果を入れると、設定した事前分布を確認することができる 15.3.2 MCMCの設定 brm()のオプションで、MCMCシミュレーションの設定を指定することができる。 result_brms = brms::brm(data = iris, Petal.Length ~ 1 + Sepal.Length, iter = 2000, warmup = 1000, chains = 4, seed = 1) iterで乱数生成の試行数、warmupでwarmup期間の数、chainsでマルコフ連鎖の数を指定する。前の章の内容をおさらいすると、MCMCではパラメータの事後分布に従う乱数を生成するシミュレーションを繰り返し、全シミュレーションの結果から作られた分布をパラメータの事後分布として採用する。iterで、乱数生成の繰り返し数を設定する（この例では、2,000試行に設定）。また、MCMCシミュレーションの最初の部分は、乱数の初期値による影響を大きく受けていて最終的に事後分布を作成する上で使い物にならない。そのため、最初の試行は切り捨てられる。warmupで、その切り捨てる期間を指定する（この例では、最初の1,000試行を切り捨てるように設定）。MCMCでは一般的に乱数生成を1からやり直して何セットか行い、事後分布を評価する。chainで、このセット数を設定する（この例では、4セットに設定）。最終的に得られるMCMCサンプル（シミュレーションの結果）は、(iter - warmup)*chains個になる。 iter, warmup, chainsの指定をしなければ、デフォルトで設定されている値(iter = 2000, warmup = 1000, chains = 4)でMCMCが実行される。 15.3.3 事後分布の評価 事前分布、MCMCの設定ができたら、brm()を実行してMCMCを行う。 シミュレーションが終わったら、結果を確認する。先ほど示したように、summary()で事後分布の期待値などの要約を確認するのももちろん、図でも確認する。plot()で簡単な図を作成することができる。 summary(result_brms_lm) plot(result_brms_lm) 他にも、MCMCの結果を図示するためのパッケージとして、bayesplotパッケージがある（brmsパッケージをインストールすると一緒にインストールされる）。 以下に、事後分布を図示する例をいくつか示す（parsで出力したいパラメータの値を任意に指定することも可能）。 library(bayesplot) bayesplot::mcmc_trace(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #トレースプロット bayesplot::mcmc_hist(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #事後分布（ヒストグラム） bayesplot::mcmc_dens(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #事後分布（密度曲線） bayesplot::mcmc_intervals(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;), prob = 0.89, #太い線が意味する範囲（89%区間とした） prob_outer = 0.95#細い線が意味する範囲（95%区間とした） ) #パラメータの分布を線で示したグラフ bayesplot::mcmc_areas(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;), prob = 0.89, #色が塗られた部分（89%区間とした） prob_outer = 0.95#細い線が意味する範囲（95%区間とした） ) #分布も一緒に示したグラフ bayesplot::mcmc_combo(result_brms_lm, combo = c(&quot;hist&quot;, &quot;dens&quot;), pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;))#mcmc_comboで、出力する図を複数指定することができる。 15.3.4 収束の評価 MCMCによる事後分布の推定は乱数を使ったシミュレーションなので、シミュレーション結果は毎回異なる。しかし、得られた結果が毎回かなり違うのならば、その結果は信用できないということになる。summary()の出力に表示されているRhatが、MCMCの結果が安定している（収束している）かを評価する指標として用いられる。Rhatが1.00を超えている場合は収束していないことが疑われるので、MCMCの設定やモデルの修正などの対処が必要になる（詳細については、参考文献リストに挙げたベイズ統計モデリングに関する解説書を参照のこと）。 15.3.5 モデルの予測評価 推定結果が、実際のデータをうまく予測できているかを評価する。ここでは、回帰直線の信用区間や予測区間を図示して確認する方法を示す。 pred_line = brms::conditional_effects(result_brms_lm, method = &quot;posterior_epred&quot;, prob=0.95 #95%信用区間を表示 ) plot(pred_line, points=TRUE) #points=TRUEで点と一緒に示す pred_line = brms::conditional_effects(result_brms_lm, method = &quot;posterior_predict&quot;, prob=0.95 #95%予測区間を表示 ) plot(pred_line, points=TRUE) #pointsで点と一緒に示す 15.4 brmsパッケージでの一般化線形モデル brm()で確率分布やリンク関数を変更すれば、ロジスティック回帰やポアソン回帰のベイズ推定も行うことができる。プログラムの書き方はglm()とほぼ同じである。 15.4.1 ロジスティック回帰 第12章でロジスティック回帰の練習に使ったサンプルデータを使って、パラメータの事後分布の推定を行ってみよう。以下に第12章に示したサンプルデータの作成プログラムを再掲する。 library(MASS) dat = biopsy dat$y = ifelse(dat$class == &quot;malignant&quot;, 1, 0) #classがbenignならばゼロ、それ以外なら1という変数yを作る dat$x = dat$V1 #V1という変数をxという名前に変える glm()を使ったパラメータの推定結果は以下である。 result_glm = glm(data = dat, y ~ 1 + x, family = binomial(link=&quot;logit&quot;)) summary(result_glm) ## ## Call: ## glm(formula = y ~ 1 + x, family = binomial(link = &quot;logit&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## x 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 brm()で同様の解析を行い、パラメータの事後分布の推定をしよう。確率分布（family）とリンク関数（link）を適切なものに指定する。 result_brms_logistic = brms::brm(data = dat, y ~ 1 + x, family = bernoulli(link=&quot;logit&quot;) #確率分布にbernoulli（ベルヌーイ分部）、linkにlogitを指定 ) summary(result_brms_logistic) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + x ## Data: dat (Number of observations: 699) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -5.20 0.37 -5.95 -4.47 1.00 2126 2023 ## x 0.94 0.07 0.80 1.09 1.00 2591 2588 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 15.4.2 ポアソン回帰 同じく、第12章のサンプルデータを使って、brm()でポアソン回帰を行ってみる。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd=1) lambda = exp(0.01+ 0.6*x) y = rpois(n=N, lambda = lambda) dat = data.frame(y=y, x=x) glm()で推定した結果が以下である。 result_poisson = glm(data = dat, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4112 -0.7542 0.1362 0.5628 1.7629 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6829 0.2807 -2.433 0.015 * ## x 0.8951 0.1052 8.506 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 130.32 on 49 degrees of freedom ## Residual deviance: 46.52 on 48 degrees of freedom ## AIC: 197.96 ## ## Number of Fisher Scoring iterations: 5 brm()でも同様に、確率分布（family）とリンク関数（link）を指定する。 result_brms_poisson = brms::brm(data = dat, y ~ 1 + x, family = poisson(link=&quot;log&quot;)) #確率分布にpoisson、リンク関数にlogを指定する summary(result_brms_poisson) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + x ## Data: dat (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.71 0.29 -1.30 -0.16 1.00 1385 1705 ## x 0.90 0.11 0.70 1.12 1.00 1575 1958 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 15.5 brmsパッケージでのマルチレベルモデル 繰り返し測定を含むデータの場合は、マルチレベルモデルで個人差や集団差を統制する必要がある（マルチレベルモデルについては、第13章を参照のこと）。brm()では、マルチレベルモデルを扱うこともできる。 lme4パッケージのglmer()関数と同様の形式でランダム効果を加えることで、マルチレベルモデルの推定を行うことができる。 第13章で例として用いたirisデータを使って、マルチレベルモデルのベイズ推定を行ってみよう。 15.5.1 ランダム切片 グループごとに異なる切片（ランダム切片）を考慮する場合、式の中に(1|グループを意味する変数)というかたちでランダム切片を加える。 model_brm_lmm = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species),#(1|Species)をランダム切片として加える family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(model_brm_lmm) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~Species (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.51 0.86 0.57 3.91 1.00 1091 1484 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.37 0.88 1.52 5.21 1.01 916 860 ## Sepal.Width 0.79 0.11 0.58 1.00 1.00 2662 1932 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.44 0.03 0.39 0.50 1.00 2550 1891 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 上の例では、あやめの種（Species）ごとに切片（Intercept）が異なるという前提で推定を行っている。Group-Level Effectsに、ランダム切片の分散の推定結果が出力されている。 15.5.2 ランダム傾き グループごとに異なる傾き、すなわちランダム傾きをモデルに入れることもできる。式の中に(予測変数|グループを意味する変数)というかたちで加える。 model_brm_lmm_2 = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width + (Sepal.Width|Species), #(Sepal.Width|Species)を加える。Speciesごとに異なる切片とSepal.Widthに係る傾きを想定する。 family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(model_brm_lmm_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Sepal.Length ~ 1 + Sepal.Width + (Sepal.Width | Species) ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Group-Level Effects: ## ~Species (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 1.26 0.99 0.08 3.85 1.01 825 ## sd(Sepal.Width) 0.37 0.34 0.01 1.34 1.01 815 ## cor(Intercept,Sepal.Width) 0.11 0.57 -0.90 0.97 1.00 1900 ## Tail_ESS ## sd(Intercept) 691 ## sd(Sepal.Width) 559 ## cor(Intercept,Sepal.Width) 1966 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.34 0.82 1.52 4.91 1.00 1388 1627 ## Sepal.Width 0.81 0.25 0.26 1.34 1.01 601 450 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.44 0.03 0.39 0.50 1.00 2773 2512 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 上の例では、あやめの種（Species）ごとにSepal.Widthに係る傾きが異なるという前提で推定を行っている。 Group-Level Effectsの部分に、ランダム傾きとランダム切片の分散の推定結果が表示されている。同時に、ランダム効果同士（グループごとの傾きと切片）の相関の推定結果も出力される。 15.6 その他 15.6.1 Stanコードの出力 make_stancode()で、モデルのStanコードを出力することができる。モデルに直接修正を加えたいときに使える。 brms::make_stancode(data = iris, Petal.Length ~ Sepal.Length, family = gaussian(link=&quot;identity&quot;)) ## // generated with brms 2.19.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## int Kc = K - 1; ## matrix[N, Kc] Xc; // centered version of X without an intercept ## vector[Kc] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // population-level effects ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0&gt; sigma; // dispersion parameter ## } ## transformed parameters { ## real lprior = 0; // prior contributions to the log posterior ## lprior += student_t_lpdf(Intercept | 3, 4.3, 2.5); ## lprior += student_t_lpdf(sigma | 3, 0, 2.5) ## - 1 * student_t_lccdf(0 | 3, 0, 2.5); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## target += normal_id_glm_lpdf(Y | Xc, Intercept, b, sigma); ## } ## // priors including constants ## target += lprior; ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept - dot_product(means_X, b); ## } 15.6.2 より深く学ぶには このように、brm()を使えばStanに関するプログラミングの知識が特になくても、簡単にMCMCで一般化線形モデルのパラメータ推定を行うことができる。ただし、この章の内容は、あくまでMCMCの練習に過ぎない。特にマルチレベルモデルのような複雑なモデルの推定は、推定結果が収束しないなど、うまく行かないケースに直面することも多い（実際に上で示した例でも出力で警告メッセージが生じされており、対策が必要となる）。 本格的にMCMCによる一般化線形モデルのベイズ推定を行うとなると、事前分布やMCMCの設定についてもっと深い知識が必要になる。より深く学ぶには、StanのWebサイトあるいはRとStanの使い方に関する解説書などを参照してほしい（付録の参考文献リストに示している）。 "],["15-Appendix_References.html", "A 参考資料 A.1 文献 A.2 Webサイト B R Markdownの使い方", " A 参考資料 A.1 文献 A.1.1 統計学の解説書 東京大学教養学部統計学教室 編 (1991). 統計学入門 東京大学出版会 南風原朝和 (2002). 心理統計学の基礎：統合的理解のために 有斐閣 大久保街亜・岡田謙介 (2012). 伝えるための心理統計：効果量・信頼区間・検定力 勁草書房 阿部真人(2021). データ分析に必須の知識・考え方 統計学入門：仮説検定から統計モデリングまで重要トピックを完全網羅 ソシム A.1.2 Rを用いた統計解析の解説書 山田剛史・杉澤武俊・村井潤一郎 (2008). Rによるやさしい統計学 オーム社 石田基広 監修、奥村晴彦 (2016). Rで楽しむ統計 共立出版 金明哲 (2016). 定性的データ分析（Useful R 5） 共立出版 嶋田正和・阿部真人 (2017). Rで学ぶ統計学入門 東京化学同人 A.1.3 一般化線形モデル、マルチレベルモデルなどの解説書 久保拓哉 (2012). データ解析のための統計モデリング入門：一般化線形モデル・階層ベイズモデル・MCMC 岩波書店 金明哲 編、粕谷英一 (2012). 一般化線形モデル（Rで学ぶデータサイエンス 10） 共立出版 A.1.4 ベイズ統計モデリングの解説書 石田基広 監修、松浦健太郎 (2016). StanとRでベイズ統計モデリング 共立出版 馬場真哉 (2019). RとStanではじめるベイズ統計モデリングによるデータ分析入門（実践Data Science シリーズ） 講談社 McElrearth R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd Ed.) Boca Raton, FL: CRC Press A.2 Webサイト RStudio Cheatsheets（代表的なパッケージの使い方などが簡単にまとめられた”チートシート”を入手できる） https://rstudio.com/resources/cheatsheets/ tidyverseの公式ページ https://www.tidyverse.org Stan User’s Guide https://mc-stan.org/users/documentation/ B R Markdownの使い方 RStudioならば、R Markdownの機能を使うことで、自分が行った分析のコードや出力をドキュメント形式で残しておくことができて便利である。 以下に、R Markdownの簡単な使い方について示す。 New Fileから「R Markdown」を選択する 「New R Markdown」というウィンドウが出てくる。「Document」を選んで「OK」を選ぶ。Title（ドキュメントのタイトル）やAuthor（著者名）の情報を入力しても構わない。 R Markdownドキュメントがサンプルコードとともに表示される。 試しにこのサンプルコードをHTMLファイルで出力してみよう。 「Knit」のプルダウンから、「Knit to HTML」を選ぶ。 ドキュメントのプレビューが出力される。 R Markdownファイルを保存する。 「.Rmd」という拡張子のファイルで保存される。「.Rmd」ファイルをRStudioで開けば、編集することができる。 このように、R Markdownに記した文章、Rのコード、出力結果（分析結果、グラフなど）をドキュメントで保存することができる。 R Markdownの使い方の詳細については、他の資料を参照のこと。 RmarkdownのCheatsheetsにも、Rmarkdownのチートシートが掲載されている。「Translations」のところに日本語訳もある。 https://www.rstudio.com/resources/cheatsheets/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
