[
["index.html", "心理学データ応用解析法 Chapter 1 はじめに 1.1 この授業について 1.2 この資料について", " 心理学データ応用解析法 帝京大学文学部心理学科 堀田結孝 2020-08-12 Chapter 1 はじめに これは帝京大学文学部心理学科の「心理学データ応用解析法」の資料として作成したものである。 1.1 この授業について 応用的なデータ解析を学ぶ。 t検定，分散分析，回帰分析など，心理統計で学んできた手法を「一般化線形モデル」という一つの枠組みで包括的に理解する。 R及びRStudioを使って解析を行う。 各回，練習問題を出す。 1.2 この資料について 1.2.1 読み方 章の中の各セクションのタイトルに（＊）がついている箇所は応用項目です。読み飛ばしても章の大筋の理解に支障はないかと思います。 1.2.2 使用上の注意 この資料は帝京大学文学部心理学科「心理データ解析法Ⅰ」向けに作成したものです。もちろん，履修者ないしは帝京大学の学生はダウンロードして使って構いません。 ゼミや卒論のデータ分析の際にも参考にしてください。 ただし，無断で譲渡，頒布，転載等（インターネット上も含む）するなど，著作権の侵害にあたる行為は固く禁止します。 "],
["01-intro.html", "Chapter 2 Rの使い方 2.1 R及びRStudioのインストール 2.2 RStudioのインストール 2.3 プログラムの書き方 2.4 変数 2.5 データ構造 2.6 欠損値 2.7 パッケージのインストールとロード 2.8 データの読み込み 2.9 Rを終わらせる", " Chapter 2 Rの使い方 Rのインストールから使い方までを解説する。 R及びRStudioのインストール プログラムの書き方 変数 データ構造 欠損値 パッケージ データの読み込み 2.1 R及びRStudioのインストール インストールは，https://cran.r-project.orgから可能。自分のOSにあったインストーラを選ぶ。 インストーラを実行したら，あとは指示に従ってインストールをすすめる。 2.2 RStudioのインストール RStudioとは，Rの使いやすさを向上させる目的で開発されているアプリケーションである。Rをインストールしたら，RStudioもインストールしておくこともすすめる。このマニュアルでも，RStudioを使って解析することを前提に説明する。 インストールは，https://rstudio.com/products/rstudio/#rstudio-desktopからできる。 「DOWNLOAD RSTUDIO DESKTOP」を選択（無料版で良い）。 RとRstudioの両方をインストールできたら，RStudioの方を開く。 以下のような画面が表示される。 2.3 プログラムの書き方 2.3.1 コンソールとスクリプト 2.3.1.1 コンソール（Console） コンソール（Console）という部分にプログラムを入力すると，結果が出力される。 ためしに，コンソールの&gt;の部分に，以下のプログラムを入力して，Enterを押してみよう。 ※このマニュアルでは以下のように，背景が灰色の箇所にプログラムとその出力結果（行頭に##が付いている部分）を示す。 1 + 1 ## [1] 2 同じコンソールに，答えである2が出力されたはずである。 このように，コンソールに直接プログラムを入力して結果を確認することができる。 2.3.1.2 スクリプト コンソールに入力したプログラムや出力結果は，Rを閉じると消えてしまう。これでは復習できないので，プログラムは別のファイルに残しておく。 プログラムを書き込んだ別ファイルのことを「スクリプト（Script）」と呼ぶ。プログラムはなるべくコンソールではなく，スクリプトファイルに書く習慣をつけよう。 「File」から「New Script」を選ぶ。何も書かれていないファイル（R Editor）が開かれる。 名前をつけて保存する。「File」から「Save as..」を選び，名前をつけて保存する。拡張子が「.R」のファイルとして保存される。 スクリプトに，試しに以下のプログラムを入力してみよう。 1 + 1 1 - 1 2 * 3 10 / 2 2^3 (1 + 3)/2 #1 + 1 （#から改行まではコメント文として理解され，実行されない） プログラムを選択し，Ctrl+Rで実行する（「Run line or selection」を選んでも可）。すると，「R Console」にプログラムの結果が出力される。 書いたプログラムは，スクリプトに残しておこう。 スクリプトファイルを開きたいときは，RStudioを立ち上げて，「File」から「Open File」を選び，スクリプトのファイルを選ぶ。 2.3.2 注意 ためしに，コンソールに以下のプログラムを入力してEnterを押してみよう。 1 + 何も表示されないし，冒頭が&gt;ではなく+が表示される。Enterを押してももとに戻らない。 これはプログラムが不完全であるためである。1 +と中途半端な状態なので，Rはプログラムの続きがあるものと思って入力を待っている状態なのである。プログラムの続きを入力すれば，結果が出力される。例えばこの例ならば，1を入力してEnterを押せば，答えである2が出力される。 他にもカッコの閉じ忘れでも同じようなことが生じる。 なお，Escキーを押せば，プログラムを中止することができる。困ったらEscキーを押してやり直そう。 他にも，エラーが生じた場合は，エラーメッセージを読んで，プログラムの書き方に間違いがないかを確認してやり直そう。たいてい，「変数の入力間違い」など大したことのないミスが原因である。ちょっとプログラムを間違えたくらいでRが壊れるということは決してないので，冷静に対処しよう。 2.4 変数 数値を変数に代入して使うことができる。 x = 5 + 8 ## = の代わりに &lt;- を使ってもOK x &lt;- 5 + 8 x ## [1] 13 y = x - 2 y ## [1] 11 2.4.1 変数の使い方の注意 Rは小文字と大文字を区別する。たとえば，x（小文字）と入力して実行すると結果が出力されるが，X（大文字）では出力されない（変数が作られていないので）。 x = 2 #小文字のxに2を代入する。 x - 2 #ゼロが出力されるはず。 X - 2 #大文字のXでは答えが表示されない。大文字のXの変数は作られていないので。 また，半角と全角で入力を間違えていないかにも注意すること。 x = 2 #半角の2 x = ２ #全角の２ コンピュータは全角を数字ではなく文字として認識する。数字は常に半角で入力すること。 2.4.2 変数の型 Rでは変数の種類として，数値型，文字列，日付，論理型の区別をする。 2.4.2.1 数値型 数値として扱われる。数値型の変数同士は演算を行うことができる。 x = 5 y = 1.2 class(x) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; class(y) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; x + y #数値型同士は演算することができる ## [1] 6.2 2.4.2.2 文字列 文字として扱われる。文字列同士は演算をすることができない。 文字を変数として代入したい場合は，文字をクオテーションマーク(&quot;&quot;)で囲む。 x = &quot;hello&quot; y = 1 class(x) #class()でその変数の型を確認することができる ## [1] &quot;character&quot; class(y) #class()でその変数の型を確認することができる ## [1] &quot;numeric&quot; y_2 = &quot;1&quot; y_2 #数値でもクオテーションで囲めば文字列として扱われる。 ## [1] &quot;1&quot; 2.4.2.3 日付（＊） Dateは日付のみを保存し，POSIXctは日付と時間を保存する。 日付型同士で日数や秒数などの演算をすることができる。 date = as.Date(&quot;2020-06-15&quot;) date ## [1] &quot;2020-06-15&quot; class(date) #class()でその変数の型を確認することができる ## [1] &quot;Date&quot; date_1 = as.POSIXct(&quot;2020-06-14 12:00&quot;) date_1 ## [1] &quot;2020-06-14 12:00:00 JST&quot; class(date_1) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; date_2 = as.POSIXct(&quot;2020-06-15 12:00&quot;) date_2 ## [1] &quot;2020-06-15 12:00:00 JST&quot; class(date_2) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; date_2 - date_1 ## Time difference of 1 days 2.4.2.4 論理型（＊） TRUEかFALSEの2つの値のどちらかを取る変数の型である。 a = TRUE class(a) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; b = FALSE class(b) #class()でその変数の型を確認することができる ## [1] &quot;logical&quot; 2.4.2.4.1 論理式 2 == 1 #2 と 1 は同じか？ ## [1] FALSE 2 != 1 #2 と 1 は同じではないか？ ## [1] TRUE 2 &lt; 1 #2 は 1 よりも小さいか？ ## [1] FALSE 2 &lt;= 1 #2 は 1 以下か？ ## [1] FALSE 2 &gt; 1 #2 は 1 より大きいか？ ## [1] TRUE 2 &gt;= 1 #2 は 1 以上か？ ## [1] TRUE 2.5 データ構造 複数の数値などをまとめたものをデータと呼ぶ。Rには，データを扱うためのいくつかのデータ構造が用意されている。 2.5.1 ベクトル 同じ型の要素を集めたもの。Rでは，c()関数でベクトルを作成することができる。 x = c(1, 2, 3, 4, 5) y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) x * 2 ## [1] 2 4 6 8 10 x_2 = c(6, 7, 8, 9, 10) x + x_2 #（格納されている変数の数が同じならば，ベクトル同士で計算ができる） ## [1] 7 9 11 13 15 2.5.2 データフレーム 複数のベクトルを行列でまとめたデータ構造を，Rではデータフレームと呼ぶ。データフレームは頻繁に使うので，構造を覚えよう。 まず，2つのベクトルを作成する。 x = c(1, 2, 3, 4, 5) y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) 次に，以下のプログラムを実行して，データフレームを作る。 data.frame()は，データフレームを作るための関数である。 dat = data.frame(x, y) dat ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e 以下のように，データフレーム$変数名で，データフレームの特定の変数をベクトルとして取り出すが出来る。 dat$x ## [1] 1 2 3 4 5 データフレームに新たに変数を加えることも出来る。 dat$x_2 = c(6, 7, 8, 9, 10) dat ## x y x_2 ## 1 1 a 6 ## 2 2 b 7 ## 3 3 c 8 ## 4 4 d 9 ## 5 5 e 10 dat$x_3 = dat$x + dat$x_2 dat ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 2.6 欠損値 実験の失敗や質問への無回答など，データが取得できなかったケースも生じ得る。そのような場合には，データの一部を欠損値として扱う。 Rでは，欠損値はNAで扱う。 先程の例で作ったデータフレームdatに，欠損値を含むベクトルx_4を入れてみよう。 dat$x_4 &lt;- c(1, 2, NA, 4, 5) dat ## x y x_2 x_3 x_4 ## 1 1 a 6 7 1 ## 2 2 b 7 9 2 ## 3 3 c 8 11 NA ## 4 4 d 9 13 4 ## 5 5 e 10 15 5 dat$x + dat$x_4 ## [1] 2 4 NA 8 10 欠損値を含む部分は計算に用いることができない。 関数によっては，欠損値を含むデータを計算に使うときには欠損値の処理をする必要がある。 mean(dat$x_4) #mean()は平均を計算する関数。欠損値が含まれているベクトルでは，計算できない。 ## [1] NA mean(dat$x_4, na.rm = TRUE) #オプションna.rm =TRUEを指定すると，欠損値を除いて計算してくれる。 ## [1] 3 2.7 パッケージのインストールとロード パッケージとは，Rの機能を拡張するためにインターネットからインストールして使うものである。 Rに標準で入っている機能に加え，追加でtidyverseパッケージをインストールして使おう。 2.7.1 パッケージのインストール パッケージをインストールする。install.packages()で，インストールしたいパッケージを入力する。 ここでは例として，tidyverseパッケージをインストールする方法を示す。 install.packages(&quot;tidyverse&quot;) もし「Please select a CRAN mirror …」というのが表示されたら，Japan (Tokyo)を選んで「OK」を押す。 インストールしただけではパッケージは使えない。使う前にロードする必要がある。library()で，括弧内に使いたいパッケージ名を入力する。 library(&quot;tidyverse&quot;) 自分のマシンならば，一度インストールしておけば，今後はlibrary()でロードするだけで使うことができる。毎回インストールする必要はない。 2.8 データの読み込み 実験データをExcelファイルなどに入力してそのExcelファイルを読み込んで分析をしたい場合，Rでデータを読み込む手順を以下に示す。 ワーキングディレクトリを指定する。 データを読み込む 読み込みたいデータをデスクトップに置いた場合を例として，以下に手順を示す。 2.8.1 ワーキングディレクトリを指定する。 ワーキングディレクトリとは，「現在居る場所」のことである。 試しに，現在のワーキングディレクトリを確認しよう。以下のプログラムをコンソールに入力して実行する。 getwd() 出力された場所が，現在のワーキングディレクトリである。Rは読み込むファイルをワーキングディレクトリを起点として探す。読み込むファイルは，ワーキングディレクトリに置くことにしよう。 2.8.1.1 ワーキングディレクトリの指定 以下に，ワーキングディレクトリをデスクトップに指定する方法について説明する。 2.8.1.1.1 方法１ コンソールに以下のプログラムを直接書き込んで実行する。 #Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 #Macの場合 setwd(&quot;~/Desktop&quot;) #正しく設定されたかを確認する getwd() 2.8.1.1.2 方法２ RStudioならば，右の方にある「File」からデスクトップを表示し，「Set As Working Directory」を選ぶ。 2.8.1.1.3 方法３ RStudioならば，「Session」から「Set Working Directory」，「Choose Working Directory」を選び，デスクトップを選ぶ。 2.8.2 データを読み込む 2.8.2.1 csvファイルの場合 read.csv()関数で読み込むことが出来る。 dat = read.csv(&quot;data.csv&quot;) #ファイル名をクオテーションで囲んで入れる。ここでは読み込んだデータを「dat」という名前で保存した。 dat #データの中身がコンソールに出力される 2.8.2.2 Excelファイルの場合 tidyverseパッケージをロードした上で，read_excel()を使う。 dat = readxl::read_excel(&quot;data.xlsx&quot;) dat 2.8.2.3 相対パス（＊） ※ついでに覚えよう。 ワーキングディレクトリを起点として指定されるファイルの場所のことを相対パスという。 今までの例だと，デスクトップ上に読み込みたいファイルを保存し，デスクトップをワーキングディレクトリとして指定して読み込むという方法で説明してきた。しかし，例えばデスクトップにあるフォルダの中にあるファイルを読み込みたい場合，いちいちワーキングディレクトリを設定し直すのが面倒くさいだろう。 このような場合，相対パスでファイルを指定するのが便利である。 「sample_data」のフォルダをダウンロードしてデスクトップに保存し，フォルダの中にある「0_sample.csv」を読み込んでみよう。 #デスクトップをワーキングディレクトリに指定する ##Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 ##Macの場合 setwd(&quot;~/Desktop&quot;) #デスクトップにあるsample_dataフォルダの中の「0_sample.csv」を読み込む dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) .（ピリオド）でワーキングディレクトリを表現することができる。/（スラッシュ）でフォルダの階層を区切ってファイルを指定することができる。 2.8.3 サンプルデータ Rには予めサンプルデータがいくつか用意されている。 iris #有名なフィッシャーのあやめデータ cars #自動車の速度と停止距離との関係 data() #data()で，入っているデータを確認できる 2.9 Rを終わらせる そのまま閉じてよい。 「Save workspace image?（作業スペースを保存しますか？）」が表示されるが，これは「いいえ」で良い。 "],
["02-summary.html", "Chapter 3 統計学の基礎の復習 Chapter 4 変数の区別 4.1 基本統計 4.2 相関", " Chapter 3 統計学の基礎の復習 Rで基本統計量を計算する方法について解説する。 変数の区別 基本統計 代表値（平均，中央値） 散布度（分散，標準偏差，分位数） 共分散，相関 Chapter 4 変数の区別 4.0.1 量的変数 数値として扱う変数。計算することができる。 量的変数は更に，間隔尺度と比率尺度で区別ができる。 4.0.1.1 間隔尺度（＊） データの間隔に意味があるもの。ゼロが何もない状態を意味するものでないもの。セ氏温度など（0℃以下も-1℃があるように，ゼロは何もない状態を意味しない）。 差には意味があるが，比率については意味を持たない。例えば，「10℃と20℃の差は10℃である」とはいえるが，「20℃は10℃の2倍の熱さである」とは言えない。 4.0.1.2 比率尺度（＊） データの間隔に意味があるもの。ゼロがなにもない状態を意味するもの。身長，体重，絶対温度など。 間隔を比率で表現できる。例えば，「体重100キロの人は体重50キロの人より2倍重い」といえる。 また，量的変数は離散値か連続値かの区別もされる。 4.0.1.3 離散値 離散値とは，小数の間隔を持たない数値のこと。 例えば，個数。1個, 2個，3個と数えるが，1.1個, 1.2個などは存在しない。 4.0.1.4 連続値 連続値とは，小数の間隔を持つ数値。 例えば，身長。150cmから151cmの間には小数で表現できる数値が連続的に並んでいる。 4.0.2 カテゴリカル変数（質的変数） 分類や種類を意味するデータ。数量化して計算することはできない。 カテゴリカル変数も，いくつかの分類が可能。ここでは，名義尺度と順序尺度の区別をあげる。 4.0.2.1 名義尺度 性別（男，女），血液型，出身地など。 順序関係がない（男性&lt;女性といった関係はない）。 4.0.2.2 順序尺度 「優，良，可，不可」といった成績，「1. 賛成，2. どちらでもない，3. 反対」といった尺度など。 順序関係を持つが，間隔は定義されない。 4.1 基本統計 以下では，サンプルデータとして，Rにもともと入っているirisデータを使いながら説明する。 head(iris) #head()でカッコの中にデータの名前を入れると，データの上数行のみを表示してくれる ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa dat = iris #irisをdatという名前で保存し直す 基本的な要約統計量をRで計算する方法について解説する。 4.1.1 代表値 データの代表となる値のこと。平均や中央値などが知られる。 4.1.1.1 平均 mean()関数を使う。カッコの中に，平均値を求めたいベクトルを入れる。 データフレームの場合は，「データ名$変数名」でベクトルを指定すれば良い。よく使う表記なので覚えておこう。 mean(dat$Sepal.Length) ## [1] 5.843333 データに欠損値が含まれている場合は，オプションとしてrm.na = TRUEを設定する。 a = c(1, 2, 3, NA, 4, 5) mean(a) ## [1] NA mean(a, na.rm = TRUE) ## [1] 3 4.1.1.2 中央値 median()関数で出力できる。 median(dat$Sepal.Length) ## [1] 5.8 4.1.2 散布度 データの散らばり具合を示す値のこと。分散，標準偏差などが知られる。 4.1.2.1 分散 var()関数を使う。 var(iris$Sepal.Length) ## [1] 0.6856935 4.1.2.2 標準偏差 sd()関数を使う。 sd(iris$Sepal.Length) ## [1] 0.8280661 4.1.2.3 分位数 データを小さい順から大きい順に並べ替えたときに，データを分割する値。一般的に，四分位数（下位25%，中央値，上位75%の値）が報告通してよく使われる。 quantile()で，データを分割したときの分位数を求められる。 quantile(iris$Sepal.Length) ## 0% 25% 50% 75% 100% ## 4.3 5.1 5.8 6.4 7.9 quantile(iris$Sepal.Length, probs = c(0.1, 0.3, 0.5, 0.8, 1.0)) #オプションのprobsで，分割する点を任意に変えることができる。 ## 10% 30% 50% 80% 100% ## 4.80 5.27 5.80 6.52 7.90 4.1.3 要約統計量 Rには，データの代表値や散布度を一括して求めてくれる関数であるsummary()が用意されている。 最小値，最大値，平均，四分位数をまとめて算出してくれる。 summary(iris$Sepal.Length) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 4.1.4 表の作り方 カテゴリカル変数の場合は，データ全体を把握する上でそれぞれの頻度を知りたいことが多い。 table()関数を使うと頻度を集計して表にしてくれる。 table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 4.2 相関 2変数間の相関係数を算出するには，cor()を使う。cor.test()を使うと，相関係数の検定などより詳細な結果を示してくれる。 cor(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.1175698 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 "],
["03-data.html", "Chapter 5 データ・ハンドリング 5.1 tidyverseパッケージのロード 5.2 変数の作成 5.3 データの抽出 5.4 パイプ 5.5 グルーピング 5.6 データの変換 5.7 データの結合（＊） 5.8 データの読み込み（＊）", " Chapter 5 データ・ハンドリング tidyverseパッケージに入っている関数を主に使いながら，今後のデータ分析の演習で必要なデータ操作について学ぶ。 変数の作成 データの抽出 パイプ グルーピング データの変換 データの結合 データの読み込み 5.1 tidyverseパッケージのロード install.packages(&quot;tidyverse&quot;) library(tidyverse) tidyverseは様々なパッケージを含んだ，パッケージのセットである。Rを使いやすくするための便利なパッケージがまとめて入っている。具体的には，以下のパッケージが入っている。 ggplot2: グラフを作るのに特化したパッケージ readr: ファイルの読み込みに特化したパッケージ dplyr: データの整理に特化したパッケージ など 詳細はtidyverseの公式ページを参照。 https://www.tidyverse.org 注意： * 以降のプログラムで関数は「XXXX::YYYY」と表現されているが，これらは「XXXXパッケージに入っているYYYYという関数を使う」という意味である。XXXX::の部分は，基本的に省略しても問題ない。 * tidyverse以外のパッケージも読み込んでいる場合，同じ名前の関数を含むパッケージがあるとエラーが生じてしまう。もしエラーが生じたときは，XXXX::を付けてどのパッケージの関数を使いたいのかを指定しよう。 以降では，Rに最初から入っているirisデータを使ってファイル操作の練習を行う。 head(iris) #irisの先頭数行を表示する 5.2 変数の作成 dplyrパッケージに入っているmutate()を使うと，新たに変数を追加することができる。 mutate()に，データの名前，新しい変数の順番で入力すると，データの右端に新しい変数を追加してくれる。 dat = iris #irisデータをdatという名前に置き換える dat2 = mutate(dat, new_var = Sepal.Length + Petal.Length, hoge = ifelse(Species == &quot;setosa&quot;, 1, 0)) #Sepal.LengthとPetal.Lengthを足し合わせて，new_varという名前の新しい変数を作る。更に，Sepeciesが&quot;setosa&quot;ならば1, そうでなければ0とするhogeという変数を作る。 head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species new_var hoge ## 1 5.1 3.5 1.4 0.2 setosa 6.5 1 ## 2 4.9 3.0 1.4 0.2 setosa 6.3 1 ## 3 4.7 3.2 1.3 0.2 setosa 6.0 1 ## 4 4.6 3.1 1.5 0.2 setosa 6.1 1 ## 5 5.0 3.6 1.4 0.2 setosa 6.4 1 ## 6 5.4 3.9 1.7 0.4 setosa 7.1 1 5.3 データの抽出 dplyrパッケージに入っているselect, filter関数を使うと，データの中から必要な部分のみを取り出すことができる。 5.3.1 必要な変数のみを取り出す（select） select()で，データの名前，取り出したい変数名（複数選択可）の順番で入力すると，指定した変数の列のみを取り出してくれる。 以下には，irisデータからSepal.LengthとPetal.Lengthのみを取り出す場合のプログラム例を示す。 dat = iris #irisデータをdatという名前に置き換える dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) #データから必要な例を取り出す。Sepal.LengthとPetal.Lengthの列を取り出す。 head(dat2) ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 5.3.2 条件に合う行を取り出す（filter） ある条件に合う部分のみを取り出したい場合（例えばデータの中から男性のみを取り出したいなど），filter()で，データの名前，条件式の順番で入力すると，データの中から条件に合う行のみを取り出してくれる。 以下には，irisデータから，あやめの種類（&quot;Species&quot;）のうち&quot;versicolor&quot;のみを取り出す場合のプログラム例を示す。 dat = iris #irisデータをdatという名前に置き換える dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) #データから条件にあう行だけを取り出す。Speciesのうち，versicolorのみを取り出す。「イコール」は=ではなく，==にするのに注意（計算式と論理式ではイコールの表現が異なる）。 head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4.0 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor 5.4 パイプ 複数のプログラムをつなげることをパイプ処理という。purrrパッケージで，Rでパイプ処理をすることができる。 具体的な例として，irisデータで「あやめの種類のうち&quot;setosa&quot;を取り出して，かつ変数としてSpecies，Sepal.Width, Sepal.Lengthのみを取り出したい」とする。 先程まで学んだ知識だけでもできなくはないが，プログラムが数行に渡って面倒くさくなる（プログラムが長くなるとミスも生じやすくなる）。 dat = iris #irisデータをdatという名前に置き換える dat2 = dplyr::filter(dat, Species == &quot;setosa&quot;) #まずSpeciesのうち，setosaのみを取り出す。dat2という名前で保存する。 dat3 = dplyr::select(dat2, Species, Sepal.Length, Petal.Length) #別の名前で保存し直したdat2から，Sepal.LengthとPetal.Lengthの列を取り出す。dat3という名前で保存する head(dat3) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 パイプ（%&gt;%）を使えば，このプログラムを1行で書くことができる。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::filter(., Species == &quot;setosa&quot;) %&gt;% dplyr::select(., Species, Sepal.Length, Petal.Length) head(dat2) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 %&gt;%はプログラムを渡していく関数であり，.はそのプログラム以前の結果を示している。irisデータをfilterに渡し，その結果をselectに渡している。 ドットは省いてしまっても良い（以降のプログラムの例でも，.は省略することにする）。 dat2 = dat %&gt;% dplyr::filter(Species == &quot;setosa&quot;) %&gt;% dply::select(Species, Sepal.Length, Petal.Length) 5.5 グルーピング パイプを利用することで，グループごとに統計量を算出することが簡単になる。 irisデータを例として，グループごとに平均や標準偏差を計算する方法を身につけよう。 あやめの種類ごとに，がくの長さの平均と標準偏差を算出してみる。 先ほど学んだパイプ処理（%&gt;%）に加え，dplyrパッケージのgroup_byとsummarise関数を利用する。 dat = iris #irisデータをdatという名前に置き換える dat2 = dat %&gt;% dplyr::group_by(Species) %&gt;% dplyr::summarise(Mean = mean(Sepal.Width, na.rm = TRUE), SD = sd(Sepal.Width, na.rm = TRUE), N = length(Sepal.Width)) #Speciesをグループ化し，グループごとにSepal.Widthの平均，標準偏差，サンプル数（ベクトルの長さ）を出力する。 dat2 ## # A tibble: 3 x 4 ## Species Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 group_by()はグループ変数を作成する関数である。データの中でグループとして使いたい変数を括弧内に指定する。 summarise()は，複数の関数を実行させる関数である。この例では，mean()，sd(), length()の3つの関数を実行し，それぞれの結果をMean, SD, Nという別の名前で保存している。 5.6 データの変換 tidyrパッケージに入っているgather()とspread()を使うと，データの並び替えなどをすることができる。 5.6.1 wide型とlong型の区別 まず，データのレイアウトには，wide型とlong型の二種類があることを理解しよう。 以下のデータを例として説明する。A, B, Cの3人の参加者が，X, Y, Z条件の３つの条件で実験課題を行ったとする。 それぞれの条件での課題の成績（数値），参加者の性別，年齢をデータとして入力する。 dat_wide = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_wide ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 このようにデータの入力方法として，１行につき１人の参加者の情報を入力するやり方がある（実験でデータを入力する際も，このレイアウトの方が入力しやすいだろう）。このようなデータのレイアウトをwide型という。 同じデータを，以下のようなレイアウトで表現することもできる。 #以下のプログラムを実行して，サンプルデータを作る dat_long = data.frame(Subject = sort(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 3)), Condition = rep(c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;), 3), Score = c(6,9,7,2,3,5,7,4,7), Gender = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;, &quot;F&quot;), Age = sort(rep(c(18,19,20), 3))) dat_long ## Subject Condition Score Gender Age ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 実験成績ごとに１行ずつでデータが作られている。すなわち，同じ参加者１人につき３行のデータがある。このようなデータの方をlong型と呼ぶ。 5.6.1.1 どのデータ型にすべきか？ Rでのデータ分析用関数は，long型でデータが入っていることを想定として作られている。従って，wide型で入力したデータを，Rでデータ解析をする際にはlong型に変換する必要が生じる時がある。 今後学ぶデータ解析も，基本的に分析で使うデータはlong型を前提とする。 5.6.2 データレイアウトの変換 wide型をlong型に変換するには，tidyrパッケージのgather()を使う。 dat_wide #wide型のデータ ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 dat_long2 = dat_wide %&gt;% tidyr::gather(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, key = Condition, value = Score) #並び替える変数を指定する。そして，key変数，value変数として新たにつけたい名前を指定する。 dat_long2 ## Subject Gender Age Condition Score ## 1 A M 18 X 6 ## 2 B F 19 X 2 ## 3 C F 20 X 7 ## 4 A M 18 Y 9 ## 5 B F 19 Y 3 ## 6 C F 20 Y 4 ## 7 A M 18 Z 7 ## 8 B F 19 Z 5 ## 9 C F 20 Z 7 逆に，long型をwide型に変換するには，tidyrパッケージのspread()を使う。 dat_long #long型のデータ ## Subject Condition Score Gender Age ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 dat_long2 = dat_long %&gt;% tidyr::spread(key = Condition, value = Score) #key変数（ここではCondition），value変数（ここではScore）とする変数を指定する。 dat_long2 ## Subject Gender Age X Y Z ## 1 A M 18 6 9 7 ## 2 B F 19 2 3 5 ## 3 C F 20 7 4 7 5.7 データの結合（＊） 複数のデータを結合したい場合は，dplyrパッケージのjoin関数を使うとよい。join関数には，left_join, full_joinなど，いくつかの種類が用意されている。 サンプルデータを使いながら，手順について説明する。 例えば，実験で参加者3人について，X, Y, Zのデータを取ったとする。 dat_sample = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_sample ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 更に，2人の参加者（AとB）に追加で実験を行い，Wのデータを取ったとする。 dat_sample2 = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;), W = c(8,3)) dat_sample2 ## Subject W ## 1 A 8 ## 2 B 3 dat_sampleとdat_sample2のデータを結合して一つのデータにしたい。 full_join()で結合したい2つのデータ，更に結合する際にキーとなる変数（2つのデータに共通して存在する変数）をby=で指定すると2つのデータを結合してくれる。 by=を省くと，自動で2つのデータに共通する変数を見つけて，それを手がかりに結合してくれる。 dat_sample3 = dplyr::full_join(dat_sample, dat_sample2, by = &quot;Subject&quot;) dat_sample3 ## Subject X Y Z Gender Age W ## 1 A 6 9 7 M 18 8 ## 2 B 2 3 5 F 19 3 ## 3 C 7 4 7 F 20 NA full_join()だと，2つのデータをすべてつなげてくれる。データが含まれていない部分は，欠損になる（data_sample2に参加者Cのデータはないので，欠損になっている）。 left_join()だと，left_join()で左側に入力したデータを含む部分のみをつなげてくれる。 dat_sample3 = dplyr::left_join(dat_sample2, dat_sample, by = &quot;Subject&quot;) dat_sample3 ## Subject W X Y Z Gender Age ## 1 A 8 6 9 7 M 18 ## 2 B 3 2 3 5 F 19 5.8 データの読み込み（＊） Rにもともと入っているread.csv()を使えばcsvファイルを読み込むことができるが，readrパッケージの関数を使うと大量のデータが含まれるファイルも高速で読み込んでくれる。また，readxlパッケージの関数を使えば，Excelファイルも読み込んでくれる。 5.8.0.1 readr 様々な形式のファイルを高速で読み書きことを目標としたパッケージ。 dat = read.csv(&quot;./sample_data/0_sample.csv&quot;) dat ## X Y Gender ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F dat_2 = readr::read_csv(&quot;./sample_data/0_sample.csv&quot;) ## Parsed with column specification: ## cols( ## X = col_double(), ## Y = col_double(), ## Gender = col_character() ## ) dat_2 ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F #データの書き出し readr::write_excel_csv(dat_2, &quot;./sample_data/0_sample_2.csv&quot;) Rで標準で入っているread.csv()ではなく，read_csv()なので注意（ドットではなく，アンダースコア）。 ファイルを書き出すための関数も用意されている。 * ここではwrite_excel_csv() を使っているが，単にwrite_csv()でも可。 read_csv()で読み込んだファイルは，データフレームではなく，tibbleという形式になる。 5.8.0.1.1 tibble（＊） tibbleとは，データフレームに代わるものとして開発された，Rの新たなデータ形式である。 tibbleは，データフレームよりも可読性を向上させているのが特徴である。コンソールにはデータ全てではなく，最初の10行程度のみ，列も画面に入る範囲のみが表示される。 as_tibble()でデータフレームをtibble形式にすることもできる。 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows tibbleだとデータをすべて閲覧することはできないが，すべて閲覧したい場合はView()を使えばよい。 View(dat) 5.8.1 readxl エクセル形式（xlsx）のファイルを読み込むことができる。 dat = readxl::read_excel(&quot;./sample/0_sample.xlsx&quot;) #特にオプションを指定しなければ，1番目に保存されているシートの中身をtibble形式で読み込んでくれる。シートや読み込む範囲を指定したい場合は，ヘルプを参照。 dat "],
["04-graph.html", "Chapter 6 グラフ 6.1 Rのグラフィック 6.2 ggplot2の基本 6.3 ファセット（Facet） 6.4 ラベル 6.5 テーマ（Theme） 6.6 図の保存 6.7 その他の機能", " Chapter 6 グラフ データをグラフで表現する方法について学ぶ。 グラフの種類 グラフの作り方 データの傾向をグラフによって表現することを可視化と表現する。 ggplot2パッケージを使って，データの可視化をする。 ggplot2はtidyverseパッケージに含まれているので，tidyverseパッケージをロードする。 install.packages(&quot;tidyverse&quot;) library(tidyverse) 6.1 Rのグラフィック Rの利点としてデータのグラフィックに優れている点をあげることができる。 Rの数あるパッケージの中でも，ggplot2()はグラフを作るのに特化された関数を含むパッケージである。 以下では，Rのサンプルデータを使いながら，データを可視化するすべを学んでいく。 6.2 ggplot2の基本 ggplot2は，一つ一つのレイヤー（パーツ）を作り，重ね合わせてグラフを作成する。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) p プログラムの解説： ggplot()：初期設定。「ggplot2を使ってグラフを書きますよ」という意味。必ず書く。カッコの中には何も入れなくて良い。 geom_xxxx()：グラフの種類の指定。必ず書く。xxxxには，グラフの種類を入力する。この例では，散布図を書くのでgeom_pointを指定した。更に，カッコの中に必要な設定を記す。 data: グラフを描画するデータを指定する。 x, y: x軸とy軸に指定したい変数を指定する。 その他にも，グラフの種類によって指定できるものがある。 オプション：例えば，軸の値の範囲，軸のラベル，グラフの色の設定などを指定することができる。オプションは必ずしも書く必要はない。 6.2.1 散布図 geom_pointで作成できる。 p = ggplot2::ggplot() + geom_point(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 重なって見えにくい場合は，geom_jitterを使うとランダムのズレをつくって表示してくれる。 p = ggplot2::ggplot() + geom_jitter(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 6.2.2 ヒストグラム geom_histogramで作成する。 p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length)) #xに，横軸にしたい変数を入れる。 p p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, fill = Species)) #種類ごとに色の塗りつぶしを変えたい場合は，fillに指定する。 p p = ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, color = Species)) #colorだと周りの線の色を変える。 p 6.2.3 箱ひげ図 geom_boxplotで作成する。 最小値，第一分位点，中央値，第三分位点，最大値を示す（外れ値は点で示される）。 p = ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 6.2.4 バイオリンプロット データの分布を表現したグラフ。 geom_violinで作成する。 p = ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 6.2.5 折れ線グラフ geom_line()を使う。geom_line()だけだと線のみだが，geom_point()で作ったグラフを重ねることで点もつけることができる。 #サンプルデータをつくる: 10日間の気温の変化 temperature = data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) temperature ## Days Celsius ## 1 1 17.2 ## 2 2 17.5 ## 3 3 18.1 ## 4 4 18.8 ## 5 5 19.0 ## 6 6 19.2 ## 7 7 19.7 ## 8 8 20.2 ## 9 9 20.5 ## 10 10 20.1 p = ggplot() + geom_line(data = temperature, aes(x=Days, y=Celsius)) + geom_point(data = temperature, aes(x=Days, y=Celsius)) p 6.2.6 エラーバーつきのグラフ geom_errorbar()でエラーバーをつけることができる。 あるいは，geom_pointrange()でも作れる。 #サンプルデータをつくる sample_dat = data.frame(Condition=c(&quot;A&quot;, &quot;B&quot; ,&quot;C&quot;), mean=c(2, 5, 8), lower=c(1.1, 4.2, 7.5), upper=c(3.0, 6.8, 9.1)) #meanが平均，lowerとupperにそれぞれ下限値と上限値。 p = ggplot() + geom_point(data = sample_dat, aes(x = Condition, y = mean)) + geom_errorbar(data = sample_dat, aes(x = Condition, ymax = upper, ymin = lower), width = 0.1) #まず，geom_pointで平均を点で示したグラフを作成する。そのグラフに，ymaxとyminにそれぞれ上限値と下限値を指定したエラーバーのグラフを重ねる（widthでエラーバーの横の長さを指定できる）。 p p2 = ggplot() + geom_pointrange(data = sample_dat, aes(x = Condition, y = mean, ymax = upper, ymin = lower)) #geom_pointrangeならば，点とエラーバーの両方を一括して指定できる。 p2 6.3 ファセット（Facet） グループごとにグラフを分けたい場合は，ファセット（facet）を利用すると良い。facet_wrap()を使う。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + facet_wrap(vars(Species)) p 6.4 ラベル x軸やy軸のラベルを変えたいときは，labsを使うと良い。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) 6.5 テーマ（Theme） グループのテーマを変えることができる。 手っ取り早く変えたい場合は，用意されているテーマを選ぶと良い。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme_bw() p + theme_gray() p + theme_classic() 6.6 図の保存 ggsave()を使う。plotに保存した図を，filenameにファイル名を指定すると，ワーキングディレクトリに作成した図が保存される。 p = ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Sepal Length&quot;, y = &quot;Petal Length&quot;) + theme_bw() p ggsave(plot = p, filename = &quot;plot.png&quot;) ggsave(plot = p, filename = &quot;plot_2.png&quot;, dpi = 300) #解像度（dpi）を指定可能。 ggsave(plot = p, filename = &quot;plot_3.png&quot;, width = 8, height = 5) #幅(width)や高さ(height)を指定可能。 6.7 その他の機能 ggplot2のCheat sheetを見てみよう。 "],
["05-probability_distribution.html", "Chapter 7 確率分布 7.1 確率変数と確率分布 7.2 一様分布 7.3 二項分布 7.4 正規分布 7.5 ポイント 7.6 その他の確率分布（＊） 練習問題 7.7 補足", " Chapter 7 確率分布 確率分布とは何かについて学ぶ。 確率変数と確率分布 二項分布 正規分布と中心極限定理 その他の確率分布 準備として，以下のプログラムを実行する。 tidyverseパッケージのインストールとロード 乱数の種の設定（このテキストに書かれているプログラムと同じ結果が再現できる）。 install.packages(&quot;tidyverse&quot;) library(tidyverse) set.seed(1234) 7.1 確率変数と確率分布 まず，サイコロを例として，確率分布とは何かについて説明する。 サイコロを1個投げるとする。それぞれの目が出る確率は1/6である。それぞれの目をX（1, 2, 3, 4, 5, 6），それぞれの目が出る確率をP(X)とする。 XとP(X)を以下の表1で示す。 表1 このとき，Xを確率変数と呼ぶ。確率変数とは，その値と対応する確率が存在する変数のことをいう。表1のように，確率変数とその変数が取り得る確率の分布を確率分布という。 確率分布には様々な種類が知られている。 7.2 一様分布 1個のサイコロをふったときのそれぞれの目が出る確率のように，どの確率変数Xについても常に一定値の確率を取る確率分布は一様分布(uniform distribution)と呼ばれる。 7.3 二項分布 7.3.1 二項分布の例 コインをn回投げる。表が出る確率を\\(q\\)とすると, 裏が出る確率は\\((1-q)\\)となる。n回中，表がx回出る確率P(x)は，理論的には以下の式で算出される。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] xを確率変数とした場合，上記の式の確率に従う確率分布を二項分布という。 つまり二項分布は，2つのカテゴリで表現されるある事象が，何回生じるかの確率を表している。コイン（表か裏）を何回か投げたときの表が出る回数，学生（男か女）の中から選んだときの男の人数など。このような事象が生じる確率は，理論的には二項分布に従う。 コインを投げる場合の例に戻る。例えば，表が出る確率qを0.5として，10回投げたときに表が6回出る確率を計算してみよう。Rならば，dbinom()関数を使えば計算できる（この関数の意味については，また後で説明する）。 #xは確率変数（コインの例でいうと表が出た回数），sizeは試行回数（コインの例でいうとコインを投げた回数）， dbinom(x=6, size=10, prob=0.5) ## [1] 0.2050781 #上の式n, x, pに実際に値を入れて計算する場合。dbinom()関数を使った場合と結果が一致することを確認しよう。 choose(10, 6) * 0.5^6 * (1 - 0.5) ^4 ## [1] 0.2050781 7.3.2 二項分布の期待値と分散 表が出る回数xが0〜10回の場合全てについて，それぞれが生じる確率を計算すると以下のようになる。 dbinom(x=0:10, size=10, prob=0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 グラフにすると以下のようになる。横軸をx, 縦軸をP(x)とする。 グラフからもわかるように，表が出る確率が0.5のコインを10回投げたときに，最も出やすいのは10回中5回であることがわかる。10回中1回，10回中10回出るケースはほとんどまれであることがわかる。 この図からも，確率分布には最も出やすい変数（平均値。確率分布の場合は期待値と呼ぶ）と分散が存在することがわかる。 二項分布の期待値\\(E(x)\\)と分散\\(Var(x)\\)は，以下の式から計算できる。 \\[ E(x) = nq\\\\ Var(x) = nq(1-q) \\] 表が出る確率が0.5（i.e., q=0.5）のコインを10回(i.e., n=10)投げた場合における，表が出る回数xの期待値と分散を計算してみよう。 #E(x) = nq 10*0.5 ## [1] 5 #Var(x) = nq(1-q) 10*0.5*(1-0.5) ## [1] 2.5 7.3.3 ベルヌーイ分布 二項分布でn=1のときは，ベルヌーイ分布と呼ばれる。 例： コインを一回だけ投げたときに，表が出る，あるいは裏が出る確率。 dbinom(x=0:1, size=1, prob=0.5) ## [1] 0.5 0.5 dbinom(x=0:1, size=1, prob=0.4) ## [1] 0.6 0.4 7.4 正規分布 7.4.1 正規分布の基礎 統計学で用いられる確率分布の中でも有名なのは正規分布である。正規分布は，平均\\(\\mu\\)，標準偏差\\(\\sigma\\)をパラメータとする確率分布で，釣鐘型（ベル・カーブ）状の分布を描く。 平均\\(\\mu\\)，標準偏差\\(\\sigma\\)とする正規分布の確率密度関数f(x)は，以下の式から計算される。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] この式自体を覚える必要はない。 これは正確には確率分布ではなく，確率密度関数と呼ばれるものである。先程の二項分布では，縦軸は横軸の値が生じる確率を意味していた。しかし，正規分布のグラフ（確率密度関数）の縦軸は，横軸の生じる確率そのものを意味しない。グラフの面積が確率を意味する。グラフの面積全てを合計した値は1となる。 試しに，平均0，標準偏差1の正規分布のグラフを作ってみよう。 x = seq(-3, 3, 0.05) # -3から3まで0.05刻みで数字の連続を作る y = dnorm(x=x, mean=0, sd=1) #平均0，標準偏差1の正規分布の確率を算出する dat_norm = data.frame(x = x, y = y) p = ggplot() + geom_line(data = dat_norm, aes(x = x, y = y)) p 0 ≤ x ≤ 1の確率は，その範囲に対応するグラフの面積となる。 #Rならば，pnorm関数でxが-∞からqまでの範囲の確率を求めることができる #以下は，平均0，標準偏差1の正規分布で 0までの確率を求めている。正規分布の半分に相当するので，0.5である。 pnorm(q = 0, mean = 0, sd = 1) ## [1] 0.5 #特定の範囲を求めたい場合は以下のように使えば良い。例えば以下は，xが1から2の範囲の確率である。 pnorm(q = 1, mean = 0, sd = 1) - pnorm(q = 0, mean = 0, sd = 1) ## [1] 0.3413447 以下が平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の値をそれぞれ変えた場合の正規分布である。赤が平均0で標準偏差1，青が平均1で標準偏差1, 黒が平均0で標準偏差2である。 7.4.2 Rで使える確率分布の関数 ここまで，dbinom，dnorm, pnormなどの関数が出てきた。これらは，Rに標準で入っている確率分布に関する関数である。 Rには確率分布から乱数を生成したり，確率変数の確率を求めることができる関数が実装されている。関数は，確率分布につきrXXXX, qXXXX, pXXXX, dXXXXの機能が異なる4種類の関数が用意されている（ XXXXは確率分布を意味する）。 #rXXXXは，乱数(random number)を出力する。 rnorm(n = 10, mean = 0, sd = 1) #平均0，標準偏差1に従う正規分布から乱数を10個生成する ## [1] -1.2070657 0.2774292 1.0844412 -2.3456977 0.4291247 0.5060559 ## [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378 #dXXXXは，確率変数xが生じる確率密度を出力する。 dnorm(x = 0.5, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，x=0.5のときの確率密度を求める（確率密度の値であって，確率ではないので注意） ## [1] 0.3520653 #qXXXXは，確率点(quantile)を出力する。 qnorm(p = 0.5, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，p ≤ 0.5の確率となるときの確率変数のお値を求める ## [1] 0 #pXXXXは，累積確率を出力する。 pnorm(q = 1, mean = 0, sd = 1) # 平均0，標準偏差1に従う正規分布 で，x ≤ 1の確率を求める ## [1] 0.8413447 7.4.3 なぜ正規分布はよく使われるのか？ 現実世界の様々なデータが正規分布に従うから？ →実際には正規分布に従わないデータの方が多い。 体重や身長なども，実際には正規分布を描くことは少ない。年収なども釣鐘型の分布にならない 正規分布は確率変数が連続量の場合の確率分布である。テストの点数や質問紙の回答得点など，離散値や順序尺度（すなわちカテゴリカル変数）が正規分布に従うというのはそもそも前提が異なっている。 では，正規分布がよく使われる理由は何なのか。もっとらしい理由は，数学的な扱いやすさである。なぜならば，元の変数がどのような確率分布に従っていたとしても，変数を足し合わせた結果は正規分布に従うという都合の良い性質があるからである。 7.4.4 中心極限定理 中心極限定理とは，「母集団が平均及び標準偏差を持つ確率分布ならば，たとえ母集団が正規分布でなくても，母集団から標本を無作為抽出して平均値を計算することを何回も繰り返すとその分布は正規分布に近づく」という定理である。 シミュレーションで中心極限定理を実感してみよう。6面のサイコロを100回振る実験を行うとする。 X = round(runif(n = 100, min = 1, max = 6),0) #round()は値を丸める関数。ここでは小数点以下の値を0として整数の値を出力するようにしている mean(X) ## [1] 3.19 それぞれの目が出る確率は1/6で一定である。すなわち，サイコロが出る目は一様分布に従う（つまり，元の分布は正規分布ではない）。一様分布の平均値は，最大値をa, 最小値をbとすると，(a+b)/2。すなわち，サイコロの例の場合の平均値は理論的には(1+6)/2=3.5となる。 サイコロを100回振って平均値を求める。この平均値を求めるのを，1,000回繰り返し行う。求めた平均値1,000個の分布を見てみると， （以下のプログラムを実行すると，グラフを描ける。プログラムの意味の説明は省略する） sample.means = sapply(c(1:1000), function(x) {mean(round(runif(n = 100,min=1,max=6),0))} ) qplot(sample.means) 正規分布に近似する。1,000回よりももっと回数を増やすと，より正規分布っぽいかたちになる。 このように，元の母集団の分布がたとえ正規分布でなくても，その標本平均は正規分布に近似する。 平均を計算しなくても，単に値を足し合わせるだけでも同じことである。 #サイコロを7回振ってその合計を求める。これを10,000回行ったときの出目の合計値の分布 Y = round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) + round(runif(n = 10000,min=1,max=6),0) qplot(Y) 中心極限定理が成り立つため，たとえ元の変数が正規分布に従ってなくても，合成したものは正規分布に近似する。なので，心理学で行われる様々なデータ分析も，以下のような処理を行うことで変数が正規分布に従うという前提を置いてもおおきな問題はない。 複数の質問項目（順序尺度）をまとめて平均化した心理尺度を分析に使う。 選挙への投票（「した」もしくは「しなかった」の二値）者の割合を県ごとに算出して，県を単位として分析に使う。 中心極限定理は簡単に言うと，「どのような確率分布に従う変数でも，足し合わせると正規分布になる」である。 ※ただし，例外はある（後述） 7.5 ポイント 確率分布には，その形状を決定づけるパラメータが存在する。 例えば，二項分布のパラメータは確率pと回数nである。正規分布は，平均\\(\\mu\\)と標準偏差\\(\\sigma\\)である。 元がたとえ正規分布でなくても，足し合わせたものは正規分布に近似する。ゆえに，正規分布は様々な統計分析に適用できる「正規な（normal）」分布なのである。 7.5.0.1 確率分布の表現の仕方 今後，確率変数と確率分布の関係を以下の式で表現することにする。 7.5.0.1.1 二項分布の場合 \\[ x \\sim Binomial(n, q) \\] Binomialのカッコ内のn, qのように，確率分布を構成する変数のことをパラメータと呼ぶ。 Binomialは二項分布，\\(\\sim\\)は「従う」という意味である。つまりこの式は，「xは，nとqをパラメータとする二項分布に従う」ということをいっている。 7.5.0.1.2 正規分布の場合 \\[ x \\sim Normal(\\mu, \\sigma) \\] 正規分布のパラメータは，平均と標準偏差である。この2つが決まれば，正規分布の形状も決まる。 7.6 その他の確率分布（＊） 他にも，よく使う確率分布を以下に示す。 ポアソン分布 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ x \\sim Poisson(\\lambda)\\\\ \\] xは0以上の整数（0, 1, 2, 3, …）とする。 ポアソン分布のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，平均と分散が等しい分布である。 以下に，パラメータ\\(\\lambda\\)をそれぞれ変えた場合のポアソン分布を示す。 一定の期間中にランダムで生じる事象はポアソン分布に従う。具体的な例としては，1日の間に届くメールの件数，営業時間中に来る客の数など。 二項分布\\(Binomial(n, p)\\)の\\(n\\)が十分大きく，かつ\\(p\\)が小さい場合は平均を\\(np\\)とするポアソン分布に近似する。つまり，めったに起こらない事象はポアソン分布に従う。例えば，1年間の間に生じる交通事故の件数など（365日それぞれで0.1%で生じるとした場合）。歴史的に有名な例は，「ドイツ軍で1年間で馬に蹴られて死亡した兵士の数」がポアソン分布に従うというもの。 7.6.1 指数分布 7.6.2 対数正規分布 練習問題 問１ あなたは野球部の監督で，自分のチームの勝率はこれまでの練習の経験から32%だとわかっている。 これから遠征で，全部で10試合を行う予定である。 勝つ試合の回数を確率変数nとし，nとそれぞれのnに対応する確率を表で示せ。 平均して何試合勝つことができるかを述べよ。 問２ ある学校で小学6年生の身長を測ったところ，平均は150.2 cmで標準偏差が3.5 cmであった。 身長152 cmから155 cmの児童の割合はいくらか。 身長158 cmを超える児童は何割いるか。 ヒント：pnorm関数を使おう。なお，全ての範囲の確率の合計は1である。 7.7 補足 7.7.1 確率分布に関連するRの関数 Rには，確率分布から乱数や確率を求めるための関数が用意されている。例えば，以下が正規分布(normal distribution)に関する関数である。 rnorm(n=100, mean=0, sd=1) #random: 乱数 dnorm(x=-1:1,mean=0, sd=1) #density: 確率密度 pnorm(q=0.1, mean=0, sd=1) #probability: 累積分布 qnorm(p=0.5398278,mean=0, sd=1) #quantile:確率点 他の確率分布に関する関数も用意されている。 #乱数を作る関数 runif(n=100, min = 0, max = 1) #一様分布からの乱数 rbinom(n=100, size=10, prob=0.5) #二項分布からの乱数 rpois(n=100, lambda = 3) #ポアソン分布からの乱数 rexp(n=100, rate = 1) #指数関数 7.7.2 中心極限定理の例外 中心極限定理より，元がどのような分布でも足し合わせると正規分布に近似すると述べたが，例外もある。例えば，コーシー分布（Cauchy distribution）と呼ばれる確率分布は，中心極限定理を適用できない。 コーシー分布は形状が正規分布に似ているが，裾が広いことが特徴である。つまり，極端な値が出る確率が正規分布よりも大きい。 パラメータは，locationとscaleの2つである。以下に，locationが0，scaleパラメータが1と3の分布の例を示す。 コーシー分布は，極端な値が存在することにより，その分布の特徴を平均や標準偏差などで捉えることができない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) #location = 0, scale = 1のコーシー分布から乱数を100個生成する mean(d_cauchy) ## [1] 6.665693 median(d_cauchy) ## [1] -0.06446904 sd(d_cauchy) ## [1] 61.77163 コーシー分布から乱数を100個作って足し合わせることを5回やり，分布を確認する。足し合わせても，正規分布に近似しない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) hist(d_cauchy) "],
["06-test.html", "Chapter 8 ｔ検定，分散分析 8.1 t検定 8.2 分散分析 8.3 まとめ 練習問題", " Chapter 8 ｔ検定，分散分析 これまで学んできた様々な統計的検定について復習をする。 * t検定 * 分散分析 理論については深く掘り下げない（統計学の基礎授業で学んでいるので）。どの分析も前回学んだ「データが帰無仮説の分布に従うとしたときに，今回得られたデータが得られる確率がどれくらいまれか」を検討している点で共通している。様々な統計的検定の復習を通して，帰無仮説検定の考え方（p値とは何か）について理解する。 必要なパッケージのインストールと乱数の種の指定をする。 installed.packages(&quot;tidyverse&quot;) library(tidyverse) set.seed(1234) 8.1 t検定 連続量の変数を扱う検定の場合，t検定を使うのが一般的である。 8.1.1 一標本のt検定 まずサンプルデータとして，平均0, 標準偏差1の正規分布からランダムに10個サンプルしたデータXを作る。 それぞれの平均と標準偏差を求めてみる。 set.seed(1234) X = rnorm(n = 10, mean = 0, sd = 1) X ## [1] -1.2070657 0.2774292 1.0844412 -2.3456977 0.4291247 0.5060559 ## [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378 mean(X) ## [1] -0.3831574 sd(X) ## [1] 0.9957875 このデータXから，帰無仮説「母集団の平均\\(\\mu = 0\\)」を検定する。 #mu=0は入力しないでもOK t.test(X, mu=0) ## ## One Sample t-test ## ## data: X ## t = -1.2168, df = 9, p-value = 0.2546 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -1.0955009 0.3291861 ## sample estimates: ## mean of x ## -0.3831574 t値，自由度(df)，p値が出力される。 t値は以下の式から計算された統計量である。 \\[ t = \\frac{\\bar{X} - \\mu}{\\sqrt{s^2/n}} \\] Xが\\(N(\\mu, \\sigma^2)\\)に従う場合，tは自由度\\(n-1\\)のt分布に従う。 式にサンプルデータの平均と分散を入れてt値がt.test()で求めたものと一致するかを確かめてみよう。 t = (mean(X) - 0) / sqrt(var(X)/10) t ## [1] -1.216776 Rならば，t.test()を使えばt値とp値，更に母集団の平均\\(\\mu\\)の95%信頼区間を出力してくれる。 出力結果から，\\(N(\\mu, \\sigma^2)\\)から今回観測されたデータX以上のtが得られる確率は0.25であるということを示している。この確率はまれといえるか（5%未満か）について結論を下す。 8.1.2 二標本の差のt検定（対応なし） set.seed(1234) Value = c(rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 10, mean = 1, sd = 1)) Treatment = c(rep(&quot;X&quot;, 10), rep(&quot;Y&quot;, 10)) sample_data = data.frame(Treatment = Treatment, Value = Value) sample_data %&gt;% group_by(Treatment) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), N = length(Value)) ## # A tibble: 2 x 4 ## Treatment Mean SD N ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 X -0.383 0.996 10 ## 2 Y 0.882 1.07 10 Xのサンプル数をm, Yのサンプル数をnとする。 \\[ t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{s^2_{X}/m+s^2_{Y}/n))}} \\] XとYは同じ\\(N(\\mu, \\sigma^2)\\)に従う時，t値は自由度\\(m+n-2\\)のt分布に従う。 つまり，同じ母集団からXとYが抽出されたと仮定した時（\\(\\bar{X} - \\bar{Y}\\)），今回のデータが得られる確率がどのくらいまれであるかを検討する。 #2つの標本の母分散が等しいと仮定できない場合 #ウェルチの検定(Welch&#39;s t-test)と呼ばれる検定 t.test(data = sample_data, Value ~ Treatment, paired = F) ## ## Welch Two Sample t-test ## ## data: Value by Treatment ## t = -2.7404, df = 17.914, p-value = 0.01349 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.2351190 -0.2948544 ## sample estimates: ## mean in group X mean in group Y ## -0.3831574 0.8818293 #2つの標本の母分散が等しいと仮定する場合 #var.equal = Tを指定する（デフォルトでvar.equal = Fとなる） t.test(data = sample_data, Value ~ Treatment, paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: Value by Treatment ## t = -2.7404, df = 18, p-value = 0.01344 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.2347852 -0.2951882 ## sample estimates: ## mean in group X mean in group Y ## -0.3831574 0.8818293 等分散を仮定した場合しない場合いずれも，それぞれの平均の間に有意な差があるといえる。 論文などにt検定の結果を報告するときは一般的に，「t(17.9)=2.74, p = 0.01」と書く。つまり，t値，自由度，p値を報告する。 一般的に2つの標本の母分散は不明であるので，それらが等しいかどうかも不明である。なので，等分散を仮定しないt検定をしておくほうが保守的である。 8.1.3 二標本の差のt検定（対応あり） t.test(data = sample_data, Value ~ Treatment, paired = T) ## ## Paired t-test ## ## data: Value by Treatment ## t = -2.5283, df = 9, p-value = 0.03233 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.3968339 -0.1331395 ## sample estimates: ## mean of the differences ## -1.264987 #なお，等分散を仮定する場合はvar.equal = Tを指定する。 t.test(data = sample_data, Value ~ Treatment, paired = T, var.equal = T) ## ## Paired t-test ## ## data: Value by Treatment ## t = -2.5283, df = 9, p-value = 0.03233 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.3968339 -0.1331395 ## sample estimates: ## mean of the differences ## -1.264987 8.2 分散分析 t検定で比較できるのは2群までである。3群以上の間で平均値の比較を行いたい場合は，分散分析（ANOVA）を行う。 8.2.1 一要因の分散分析 #サンプルデータ Y1 = c(1, 2, 3) Y2 = c(5, 7, 5) Y3 = c(5, 4, 2) Value = c(Y1, Y2, Y3) Treatment = factor(c(1, 1, 1, 2, 2, 2, 3, 3, 3)) sample_data2 = data.frame(Treatment = Treatment, Value = Value) sample_data2 ## Treatment Value ## 1 1 1 ## 2 1 2 ## 3 1 3 ## 4 2 5 ## 5 2 7 ## 6 2 5 ## 7 3 5 ## 8 3 4 ## 9 3 2 それぞれ3つのグループ（X = 1, 2, or 3）で変数Yを取ったとする。 グループごとの平均値などは，以下のようになっている。これら3つのグループの間で平均値に差があるのかを検定する。 sample_data2 %&gt;% dplyr::group_by(Treatment) %&gt;% dplyr::summarise(Mean = mean(Value), SD = sd(Value), N = length(Value)) ## # A tibble: 3 x 4 ## Treatment Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 2 1 3 ## 2 2 5.67 1.15 3 ## 3 3 3.67 1.53 3 result_anova = aov(data = sample_data2, Value ~ Treatment) summary(result_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 20.222 10.111 6.5 0.0315 * ## Residuals 6 9.333 1.556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #以下でも同じ結果が出る oneway.test(data = sample_data2, Value ~ Treatment, var.equal = TRUE) ## ## One-way analysis of means ## ## data: Value and Treatment ## F = 6.5, num df = 2, denom df = 6, p-value = 0.03149 #lm関数でも分散分析表を出せる（ただし，lm関数を使った場合ではTukeyHSD()は使えないので注意。多重比較をしたい場合はaov()を使う。 result_anova_2 = lm(data = sample_data2, Value~Treatment) anova(result_anova_2) ## Analysis of Variance Table ## ## Response: Value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 20.2222 10.1111 6.5 0.03149 * ## Residuals 6 9.3333 1.5556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ここでは分散分析の理論の詳細は省略する。 簡単に説明すると，分散分析ではグループ間の分散がグループ内の分散と比べて大きい確率を求めて，群の間で平均値に差があるかを検定する。 回帰分析（正確には線形モデルと呼ばれる統計解析の枠組み）でも，分散分析と同様の目的の検定を行うことができる。 分散分析は要因の数やそれぞれの要因の対応ありなしによって，やり方が複雑になる（例えばテストの成績に対して，男女，学年，科目によってどう差があるかを検討するなど）。線形モデルならば，もっとシンプルに行うことができる。 分散分析の結果を論文で報告するときは，「F(2, 6) = 2.26, p = .19」といったように報告する。 anova()では，逐次平方和（Type Ⅰ SS）が出力される（要因の各セルのデータ数にかたよりがある場合，独立変数を投入した順序（x1, x2とx2, x1）で平方和の値の計算結果が異なる場合がある）。調整平方和（Type Ⅱ，Type Ⅲ）を出したいときは，carパッケージのAnova()など別のパッケージが必要になる。 8.2.2 多重比較 「3つのグループの間で平均値に差がない」という帰無仮説が棄却された場合，どのグループの間に有意な差があるのかを検定する必要がある。このサンプルデータの場合，グループ１とグループ２，グループ２とグループ３，グループ１とグループ３との間，計３つの組み合わせで平均値の比較を行う。つまり，t検定を3回行って条件間の比較をする。 ただし，第5回でも述べたように，統計的検定を繰り返し行う（多重比較）と第一種のエラーを犯す確率が増える。t検定で出てきたp値をそのまま報告するのではなく，多重比較の問題を考慮した上でp値に補正をかける（p値を過大に評価し直す）必要がある。 多重比較の補正法には，いくつかの方法がある。 * ボンフェローニ（Bonferroni）の方法: p値を比較の回数分かけて過大に評価する * チューキー(Tukey)の方法 * ホルム(Holm)の方法 多重比較の補正を行うときは，pairwise.t.test関数を使う。各群間の比較について，補正後のp値が出力される。 pairwise.t.test(sample_data2$Value, g = sample_data2$Treatment, p.adjust.method = &quot;bonferroni&quot;) #gがグループを意味する変数，p.adjust.methodに補正方法を指定する。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: sample_data2$Value and sample_data2$Treatment ## ## 1 2 ## 2 0.034 - ## 3 0.458 0.291 ## ## P value adjustment method: bonferroni pairwise.t.test(sample_data2$Value, g = sample_data2$Treatment) #オプションに何も指定しないと，ホルムの方法の結果が出力される。ボンフェローニの方法は保守的すぎるので，他の方法の方が好まれる。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: sample_data2$Value and sample_data2$Treatment ## ## 1 2 ## 2 0.034 - ## 3 0.194 0.194 ## ## P value adjustment method: holm よく使われるチューキーの方法を使う場合は，TukeyHSD()が用意されている。 TukeyHSD(result_anova) #TukeyHSD()に，分散分析表の結果を入れる ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Value ~ Treatment, data = sample_data2) ## ## $Treatment ## diff lwr upr p adj ## 2-1 3.666667 0.5420889 6.791244 0.0263939 ## 3-1 1.666667 -1.4579111 4.791244 0.3023895 ## 3-2 -2.000000 -5.1245777 1.124578 0.2019189 8.3 まとめ 統計学の基礎の復習として，これまで学んだ統計解析（＋α）の復習をしてきた。いずれも考え方は共通していて，「データから求めた統計量が帰無仮説に従うとしたときに，今回のデータよりもまれな事象が得られる確率（p値）」を求めている。 更に，これまで学んできた統計手法は統計モデルというかたちで包括的に理解することができる。詳しくはまた後日学ぶ「一般化線形モデル」の回で扱う。t検定，分散分析なども，「一般化線形モデル」の中に含まれる。「一般化線形モデル」を理解すれば，様々なデータに対して適切な分析を行うことができるようになる。 次回はp値を基準として結論を出す帰無仮説検定が抱える問題について扱う。 練習問題 問１ 以下のプログラムを読み込む。 あるサプリメントにダイエットの効果があるかを検討するために，10名を対象に実験を行った。それぞれの参加者にサプリメントを投与する前(before)と投与した後（after）で体重を測定した（架空のデータである）。 before = c(37.93, 52.77, 60.84, 26.54, 54.29, 55.06, 44.25, 44.53, 44.36, 41.1) after = c(57.84, 50.02, 53.36, 65.97, 79.39, 63.35, 57.33, 51.33, 52.44, 101.24) Value = c(before, after) Treatment = c(rep(&quot;before&quot;, 10), rep(&quot;after&quot;, 10)) Subject = c(c(1:10), c(1:10)) sample_1 = data.frame(Subject = Subject, Treatment = Treatment, Value = Value) sample_1 ## Subject Treatment Value ## 1 1 before 37.93 ## 2 2 before 52.77 ## 3 3 before 60.84 ## 4 4 before 26.54 ## 5 5 before 54.29 ## 6 6 before 55.06 ## 7 7 before 44.25 ## 8 8 before 44.53 ## 9 9 before 44.36 ## 10 10 before 41.10 ## 11 1 after 57.84 ## 12 2 after 50.02 ## 13 3 after 53.36 ## 14 4 after 65.97 ## 15 5 after 79.39 ## 16 6 after 63.35 ## 17 7 after 57.33 ## 18 8 after 51.33 ## 19 9 after 52.44 ## 20 10 after 101.24 投与前と投与後それぞれについて，体重の平均値及び標準偏差を求めて報告せよ。 このサプリメントの投与により体重が変化したかについてt検定（等分散を仮定しない）で検討し，結果について報告するとともに結論を述べよ。 問２ 以下のプログラムを読み込む。 ある教授法に児童の学力向上の効果があるかを検討した。学校Bにはその教授法を実施し，学校Aには何もしなかった。その後，学校Aと学校Bそれぞれ10人の生徒に学力テストを行った。A，Bそれぞれが学校A，Bそれぞれの生徒の成績である（架空のデータである）。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(48, 40, 43, 56, 69, 53, 47, 41, 42, 91) Value = c(A, B) Treatment = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample_2 = data.frame(Treatment = Treatment, Value = Value) sample_2 ## Treatment Value ## 1 A 38 ## 2 A 53 ## 3 A 61 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 48 ## 12 B 40 ## 13 B 43 ## 14 B 56 ## 15 B 69 ## 16 B 53 ## 17 B 47 ## 18 B 41 ## 19 B 42 ## 20 B 91 学校Aと学校Bそれぞれについて，テストの得点の平均値及び標準偏差を求めて報告せよ。 この教授法に成績向上があったかどうかについてt検定（等分散を仮定しない）で検討し，結果について報告するとともに結論を述べよ。 問３ 以下のプログラムを読み込む。 A県, B県, C県のそれぞれで学力テストを行った。各県で10名の生徒の成績を集計し，県(prefecture)ごとの得点（score）である（架空のデータである）。A，Bそれぞれが学校A，Bそれぞれの生徒の成績である。 Value = c(38, 40, 58, 27, 54, 55, 44, 45, 44, 41, 44, 38, 41, 51, 62, 49, 44, 39, 40, 79, 61, 55, 56, 65, 53, 46, 66, 50, 60, 51) Prefecture = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10), rep(&quot;C&quot;, 10)) sample_3 = data.frame(Prefecture = Prefecture, Value = Value) sample_3 ## Prefecture Value ## 1 A 38 ## 2 A 40 ## 3 A 58 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 44 ## 12 B 38 ## 13 B 41 ## 14 B 51 ## 15 B 62 ## 16 B 49 ## 17 B 44 ## 18 B 39 ## 19 B 40 ## 20 B 79 ## 21 C 61 ## 22 C 55 ## 23 C 56 ## 24 C 65 ## 25 C 53 ## 26 C 46 ## 27 C 66 ## 28 C 50 ## 29 C 60 ## 30 C 51 A県，B県，C県の成績の平均値及び標準偏差は以下の通りである。 Prefecture Mean SD N A 44.6 9.215928 10 B 48.7 12.858633 10 C 56.3 6.600505 10 県によって学力に違いがあるかについて一元配置分散分析で検討し，結果について報告するとともに結論を述べよ。 県の間で学力に有意差が見られた場合は，どの県とどの県との間に有意差があるかをホルムの方法で確認し，結果について報告せよ。 "],
["07-problems_of_NHT.html", "Chapter 9 統計的帰無仮説検定の問題 9.1 p値の問題 9.2 効果量 9.3 検定力 9.4 有意水準，効果量，検定力，標本数の関係 9.5 信頼区間 練習問題 参考文献", " Chapter 9 統計的帰無仮説検定の問題 以下の4つの関係を理解し，統計的帰無仮説検定が抱える問題点について理解する。 * 有意水準 * 効果量 * 検定力 * サンプルサイズ * 信頼区間 tidyverseとpwrパッケージをインストールして使う。 install.packages(&quot;pwr&quot;, &quot;tidyverse&quot;) library(pwr) library(tidyverse) 9.1 p値の問題 9.1.1 第Ⅰ種の過誤 第Ⅰ種の過誤（\\(\\alpha\\)）とは，「帰無仮説が真なのに，帰無仮説を棄却する誤り」であった。第Ⅰ種の過誤を犯す確率は，有意水準として設定する確率である。 有意水準は一般的に5%が用いられる。第Ⅰ種の過誤を犯す確率を少なくするためにできるだけ小さい確率ということで，慣習として5%とされている（20回に1回くらいの誤りは許す）。 9.1.2 p値と標本数の関係 しかし，p値は標本数に依存する。標本数が多くなるほどp値は小さくなる。 例えばコイン投げて表が出る回数をカウントする実験を行う。10回コインを投げて2回表が出た場合，100回コインを投げて20回表が出た場合それぞれについて（表が出た割合はどちらも0.2），二項検定を行う。 「フェアなコインと比べてこのコインはゆがんでいるか」を検討する。 binom.test(x = 2, n = 10, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 binom.test(x = 20, n = 100, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 20 and 100 ## number of successes = 20, number of trials = 100, p-value = 1.116e-09 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.1266556 0.2918427 ## sample estimates: ## probability of success ## 0.2 「有意ではない（\\(p &gt; 0.05\\)）」というのは「差がない」ということを意味しない（差自体は存在する：今回の実験結果0.2とフェアなコインの結果0.5）。少ない標本数では，珍しい結果が生じることもありえる。意味のある差かどうかが，標本数が少なすぎて判断できないということである。 逆に，実質意味のない差であっても，標本数が多ければ統計的に有意な差（\\(p &lt; .05\\)）は得られる。例えば，10,000回コインを投げて，表が出た回数が4,900回だった場合（表が出る割合は0.49），このコインがフェアなコインよりも歪んでいるかを検定してみると，有意（\\(p &lt; .05\\)）な結果が得られる。 n = 10000 x = 0.49 * n binom.test(x = x, n = n, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: x and n ## number of successes = 4900, number of trials = 10000, p-value = 0.04659 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4801563 0.4998496 ## sample estimates: ## probability of success ## 0.49 しかし，0.49と0.50の差，すなわち1%の違いを意味のある差と判断してよいのか？ 9.1.3 p-hacking（＊） 方法次第で，「有意な結果」を導くことが可能。このようなインチキはpハッキングと表現されることがある。 * たくさんの質問項目について個別に検定を行い，有意な結果だけについて議論する。 * たくさん実験を行って，有意な結果だけを報告する。 * 被験者を少しずつ追加していく度に検定を行い，有意になったら追加するのをやめる。 有意な結果が得られた研究を報告している論文があった。その論文の研究を追試してみたところ，有意な効果は見られなかった。しかし，「追試のやりかたが悪かったからだ」などと批判され，既存の研究に対してネガティブなデータは公表されにくい。これは，出版バイアス（publiation bias）と呼ばれる。 9.2 効果量 上述のように，p値は「帰無仮説のもとで今回のデータ以上にまれな結果が得られる確率」を意味し，効果の大きさそのものを意味する指標ではない。有意だった（あるいは有意でなかった）からといって，差が大きい（ない）とは限らない。 効果の大きさを示す指標は，効果量（effect size）と呼ばれる。 9.2.1 Cohen’s d ここでは，Cohenのd（Cohen’s d）と呼ばれる指標を紹介する。 Cohen’s dは，以下の式で計算される。nはそれぞれの群の標本数，\\(\\bar{X}\\)はそれぞれの群の標本平均，\\(S_{1}^2\\)と\\(S_{2}^2\\)はそれぞれ2群の不偏分散とする。すなわち，\\(S_{P}\\)は2群をプールした上での標準偏差を意味する。 \\[ \\begin{equation} d = \\frac{|\\bar{X_{1}} - \\bar{X_{2}|}}{S_{P}} \\\\ S_{P} = \\sqrt{\\frac{(n_{1}-1)S_{1}^2 + (n_{2}-1)S_{2}^2}{n_{1}+n_{2}}} \\end{equation} \\] つまり，2群の標本平均の差が標準偏差の何倍大きいかを示す指標がdである。 9.2.1.1 例 60人の学生をそれぞれ2群に分け，学習課題を行わせた。課題の前に，実験群にはある訓練を，統制群には関係のない課題を行わせた。実験群と統制群の間で，学習課題の成績(Y)を比較する。 set.seed(1234) #実験群：平均60，標準偏差20の正規分布から，30個サンプルする。 Exp = round(rnorm(n=30, mean = 60, sd = 20), 0) #平均55，標準偏差20の正規分布から，30個サンプルする。 Cntr = round(rnorm(n=30, mean = 55, sd = 20), 0) sample_data = data.frame(Y = c(Exp, Cntr), X = c(rep(&quot;Exp&quot;, 30), rep(&quot;Cntr&quot;, 30))) それぞれ，群別に平均値と標準偏差を見てみる。 dplyr::group_by(sample_data, X) %&gt;% dplyr::summarise(mean = mean(Y), var = var(Y), sd = sd(Y), n = length(Y)) ## # A tibble: 2 x 5 ## X mean var sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Cntr 44 367. 19.2 30 ## 2 Exp 54.1 326. 18.0 30 t検定で2群の平均値の差が統計的に有意かを検討する。 t.test(data=sample_data, Y~X, paired = F) ## ## Welch Two Sample t-test ## ## data: Y by X ## t = -2.0947, df = 57.793, p-value = 0.0406 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.687396 -0.445937 ## sample estimates: ## mean in group Cntr mean in group Exp ## 44.00000 54.06667 効果量\\(d\\)を求める（上記の式に当てはまる数値を入れる）。 SP = sqrt(((30 - 1)*367 + (30 - 1)*326 ) / (30 + 30)) d = abs(44 - 54.1) / SP #abs()は絶対値を求める関数 d ## [1] 0.5518631 Cohen (1968)によると，効果量の評価は「小さい」，「中くらい」，「大きい」の目安がある。 だいたい，d = 0.2 が小さい，d = 0.5が中くらい，d = 0.8が大きい。 Cohen’s dは2つの標本の平均値の差の大きさを示す指標である。例えば，効果量0.2は以下のようになる。 curve(dnorm(x), lwd=2, xlim=c(-3,3), xlab=&quot;&quot;, ylab=&quot;&quot;, frame.plot= FALSE, yaxt=&quot;n&quot;, yaxs=&quot;i&quot;) curve(dnorm(x,mean=0.2), lwd=2, add=TRUE) segments(0, 0, 0, dnorm(0)) segments(0.2, 0, 0.2, dnorm(0)) 9.2.1.2 その他の効果量 Cohen’s d以外にも，効果量の指標が提案されている。詳しくは，大久保・岡田(2012)「伝えるための心理統計学」の第3章を参照。 9.3 検定力 9.3.1 第Ⅱ種の過誤 第Ⅱ種の過誤（\\(\\beta\\)）とは，「帰無仮説が偽なのに，帰無仮説を採択してしまう」誤りのことであった。 検定力とは，正しい判断の確率，つまり「帰無仮説が偽であるときに，正しく帰無仮説を棄却する確率」であった。 * 検定力は\\(1 - \\beta\\)で求められる（全体の確率から第２種の過誤を犯す確率を引いたもの） 検定力は有意水準（α=0.05）と違って特に基準が定まっているわけではないが，.80を水準として設定するのが良いとされている（5回に1回の誤りは許す:β=0.2）。 後述するように，統計的検定の検定力は標本数にも依存する。標本数が大きければ検定力は上がる（わずかな差でも”有意差”として検出してしまう）。標本数が少なければ検定力は下がる。 9.4 有意水準，効果量，検定力，標本数の関係 有意水準，効果量，検定力，標本数はそれぞれ関わり合っている。 * 標本数が増えれば有意な結果が出やすくなるというのは既に見たとおりである。 いたずらに標本数を増やせば意味のない差も検出されてしまう。無駄な労力にもなる。 これら4つのパラメータのうち，研究者が左右できるのは「標本数」だけである。 * 効果量は研究で明らかにしたいものそのもの。測定しなければわからない（事前に知ることはできない。先行研究からこれくらいだろうと予想することはできる）。 研究の前にあらかじめ「有意水準」，「効果量（の予想）」，「検定力」を決めておけば，取るべき標本数が定まる（事前の検定力分析）。また，データの取得後に，「有意水準」，「効果量」，「標本数」から，そのデータに対する「検定力」を調べることができる（事後の検定力分析）。 Rでは，pwrパッケージにある関数を用いて検定力の分析をすることができる。 #事前の検定力分析（標本数の設計） #2群間の差をt検定で検定する場合 #各群の標本数(n)，効果量（d: Cohen&#39;s d），有意水準（sig.level），検定力（power）のどれか３つを入れると，入れなかったものの結果が出力される。 pwr::pwr.t.test(d=0.5, power=0.8, sig.level=0.05, n=NULL) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group #事後の検定力分析 #２群それぞれの標本数(n1, n2)，効果量(d), 有意水準(sig.level)を入れると，検定力が求められる。 pwr::pwr.t2n.test(n1=10, n2=10, d=0.8, sig.level=0.05, power=NULL) ## ## t test power calculation ## ## n1 = 10 ## n2 = 10 ## d = 0.8 ## sig.level = 0.05 ## power = 0.3950692 ## alternative = two.sided 9.5 信頼区間 95%信頼区間(95% confidence interval)は，95%の確率で母数が含まれる範囲のことを言う。 例えば，母集団からランダムに20個の標本を抽出し，平均を求める。 set.seed(1234) sample_data2 = rnorm(n=20, mean=0, sd=1) sample_data2 ## [1] -1.20706575 0.27742924 1.08444118 -2.34569770 0.42912469 0.50605589 ## [7] -0.57473996 -0.54663186 -0.56445200 -0.89003783 -0.47719270 -0.99838644 ## [13] -0.77625389 0.06445882 0.95949406 -0.11028549 -0.51100951 -0.91119542 ## [19] -0.83717168 2.41583518 mean(sample_data2) ## [1] -0.2506641 平均は-0.25であった。この20個のデータから，母集団の平均を推定する。 標本から得られた値をもとに母集団のパラメータを評価する手続きは，推定と呼ばれる。 もちろん，正確な母集団の平均値を当てることは難しい。そこで，母集団の平均値が入るだろうと予測される範囲を推定する。これが，信頼区間である。 9.5.1 信頼区間の求め方 例えば平均値の信頼区間は以下のように求める。 \\[ CI = \\bar{X} + ME\\\\ ME = SE \\times \\pm t_{95\\%} \\] MEは誤差範囲(margin of errors)である。つまり，標本から得られた値について正及び負の方向に誤差を加えた値が信頼区間の上限及び下限値となる。 誤差範囲には標準誤差（SE）を用いる。 データが正規分布に従うのならば，誤差範囲はt分布を使って求めるのが一般的。\\(t_{95\\%}\\)は，t分布の95％点に対応する（t分布の下位または上位2.5%に対応するtの値）。 #conf.levelの値を変えれば，信頼区間の範囲を任意で設定できる（デフォルトで0.95） t.test(sample_data2, conf.level = 0.95) ## ## One Sample t-test ## ## data: sample_data2 ## t = -1.1057, df = 19, p-value = 0.2826 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.7251406 0.2238125 ## sample estimates: ## mean of x ## -0.2506641 平均値の信頼区間ならば，Rではt.test()でも求めることができる。 9.5.2 信頼区間と統計的帰無仮説検定の関係 信頼区間は統計的帰無仮説検定と表裏一体である。 例えば「母集団の平均値は0ではない」という帰無仮説を評価する。すなわち，母集団の平均値の95％信頼区間にゼロが含まれていなければ，帰無仮説は棄却されることになる。 標本数を増やせば（標準）誤差が小さくなるので，信頼区間の範囲も狭くなる。 9.5.3 平均値以外の信頼区間の求め方（＊） 割合について求める場合は，Rならばbinom.test()で求められる。 #100人中，30人がある意見に賛成した場合。母集団の賛成率の信頼区間は？ #conf.levelの値を変えれば，信頼区間の範囲を任意で設定できる（デフォルトで0.95） binom.test(x=30, n=100, conf.level = 0.95) ## ## Exact binomial test ## ## data: 30 and 100 ## number of successes = 30, number of trials = 100, p-value = 7.85e-05 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.2124064 0.3998147 ## sample estimates: ## probability of success ## 0.3 もちろん，他の統計量についても，同じく信頼区間を評価することは可能。相関係数，回帰係数，効果量などについても求められる（詳しくは，大久保・岡田, 2012）。 信頼区間を評価することで，その推定値の正確さや範囲を評価することができる。 信頼区間を報告するときは一般的に，\\(CI = [-0.56, 0.36]\\)といったように表現する。 最近の心理学においても，信頼区間の報告が求められるようになっている。グラフのエラーバーの範囲にも信頼区間を載せることも多い（95%水準で有意差があるのかがわかりやすくなる） 練習問題 問１ 二群間で平均値をt検定で比較する場合，効果量が小さい，中くらい，大きいとき（それぞれ，\\(d=0.2, d=0.5, d=0.8\\)），それぞれで一群あたり何名程度の参加者を取ればよいか？有意水準を0.05, 検定力を0.8とした上で求めよ。 ヒント：Rのpwrパッケージのpwr.t.test()を使えば求まる。 問２ 小問1 以下のプログラムを読み込む。 あるサプリメントにダイエットの効果があるかを検討するために，10名を対象に実験を行った。それぞれの参加者にサプリメントを投与する前(before)と投与した後（after）で体重を測定した（架空のデータである）。 before = c(37.93, 52.77, 60.84, 26.54, 54.29, 55.06, 44.25, 44.53, 44.36, 41.1) after = c(57.84, 50.02, 53.36, 65.97, 79.39, 63.35, 57.33, 51.33, 52.44, 101.24) Value = c(before, after) Treatment = c(rep(&quot;before&quot;, 10), rep(&quot;after&quot;, 10)) Subject = c(c(1:10), c(1:10)) sample_1 = data.frame(Subject = Subject, Treatment = Treatment, Value = Value) sample_1 ## Subject Treatment Value ## 1 1 before 37.93 ## 2 2 before 52.77 ## 3 3 before 60.84 ## 4 4 before 26.54 ## 5 5 before 54.29 ## 6 6 before 55.06 ## 7 7 before 44.25 ## 8 8 before 44.53 ## 9 9 before 44.36 ## 10 10 before 41.10 ## 11 1 after 57.84 ## 12 2 after 50.02 ## 13 3 after 53.36 ## 14 4 after 65.97 ## 15 5 after 79.39 ## 16 6 after 63.35 ## 17 7 after 57.33 ## 18 8 after 51.33 ## 19 9 after 52.44 ## 20 10 after 101.24 対応のあるt検定で投与前と投与後の体重の比較をし，帰無仮説が棄却されるかについて結論を述べよ。 ヒント：対応のあるt検定のやりかたについては，第６回の資料を参照。 小問２ 以下のデータは，小問１のデータを横に並べ替えたものである。参加者１人につき１行，投与前（before）と投与後（after）も数値が入力されている。 sample_2 = data.frame(Subject = c(1:10), before = before, after = after) sample_2 ## Subject before after ## 1 1 37.93 57.84 ## 2 2 52.77 50.02 ## 3 3 60.84 53.36 ## 4 4 26.54 65.97 ## 5 5 54.29 79.39 ## 6 6 55.06 63.35 ## 7 7 44.25 57.33 ## 8 8 44.53 51.33 ## 9 9 44.36 52.44 ## 10 10 41.10 101.24 投与前と投与後の体重の差を求めて，体重の差の95%信頼区間を求めよ。 ヒント：投与前と投与後の差の変数を作る。つまり，beforeからafterを引き（逆でも可），diffという新しい変数を作る。新しく作った変数をt.test()に入れれば95%信頼区間が求まる。 参考文献 大久保街亜・岡田謙介 (2012). 伝えるための心理統計 勁草書房 "],
["08-regression.html", "Chapter 10 回帰分析 10.1 回帰分析 10.2 重回帰分析 10.3 ダミー変数（独立変数が質的変数の場合） 練習問題", " Chapter 10 回帰分析 回帰分析の基礎について学ぶ。 * 回帰分析 * 最小二乗法 * 重回帰分析 install.packages(&quot;tidyverse&quot;) install.packages(&quot;car&quot;) library(tidyverse) library(car) 10.1 回帰分析 以下のプログラムを読み込み，サンプルデータを作る。 #架空のデータを作る set.seed(1234) N = 100 a = 10 b1 = 3 b2 = 0.5 x1 = rnorm(n = N) x2 = rnorm(n = N) e = rnorm(n = N, sd = 5) y = b1*x1 + b2*x2 + a + e sample_data = data.frame(x1 = x1, x2 = x2, y= y) head(sample_data) #サンプルデータの上数行を表示 ## x1 x2 y ## 1 -1.2070657 0.41452353 9.012199 ## 2 0.2774292 -0.47471847 14.078772 ## 3 1.0844412 0.06599349 14.213890 ## 4 -2.3456977 -0.50247778 6.215336 ## 5 0.4291247 -0.82599859 12.432780 ## 6 0.5060559 0.16698928 15.403974 qplot(data=sample_data, x1, y) #x1とyの散布図を示す 以下のことが知りたい。 新たに測定を行ったとき，x1 = 0.1 の値が得られた。この値から，y がどのような値になるのか？（新たなデータから，yを予測する） x1 が 1増えたら，y がどれくらい増えるのか？（x1の効果の強さを知りたい） 回帰分析(regresion analysis)は，「データの予測」と「効果の測定」を目的として行う。 回帰分析とは，以下の式により独立変数の値から従属変数の値を予測する解析である。以下のような式は，「回帰式」あるいは「線形予測子」と呼ばれる。 \\[ \\begin{equation} \\hat{y} = bx+a \\end{equation} \\] xを独立変数，yを従属変数とする。\\(\\hat{y}\\)は，yの予測値とする。傾きbと切片aをデータから求める。 実測値であるyと予測値である\\(\\hat{y}\\)の差が最も小さくなるときの，bとaを求める。 回帰分析はシンプルな直線の式から結果を予測する解析である。 切片aは，独立変数xがゼロのときの従属変数の予測値を表現している。 傾きは，回帰係数（regression coefficient）と呼ばれる。 回帰係数は独立変数が従属変数にもたらす効果の強さを意味する。つまり，効果量の一種ともいえる。 なお，回帰分析において独立変数を「説明変数」，従属変数を「目的変数，応答変数，ないしは被説明変数」という場合もある。 10.1.1 回帰分析の結果の解釈 回帰分析をする場合，Rではlm()を使う。 result = lm(data = sample_data, y ~ x1) #結果を別の変数で保存しておき，summary()関数で詳細な結果を出すことができる summary(result) ## ## Call: ## lm(formula = y ~ x1, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.3526 -3.3001 0.5401 2.6685 13.2098 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8529 0.4945 21.95 &lt; 2e-16 *** ## x1 3.3780 0.4889 6.91 4.94e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.886 on 98 degrees of freedom ## Multiple R-squared: 0.3276, Adjusted R-squared: 0.3207 ## F-statistic: 47.74 on 1 and 98 DF, p-value: 4.935e-10 係数（Coefficients）の結果を見る。 Interceptは切片を意味する。x1が独立変数の傾きを意味する。 Estimateが推定された切片または傾きの値である。 Std.Errorは推定された係数の標準誤差である。 t value及びPrは係数の有意性検定の結果を示している（それぞれt値，p値）。ここでは，「係数がゼロである」という帰無仮説を検定している。p値が極端に低い場合は，「求めた係数の値は有意にゼロから乖離している」と結論付けることができる。 傾きが意味することは，独立変数が1単位増えたら従属変数がどう変化するかということである。 係数がプラスならば，係数の値が増えると従属変数の値が増える関係にあることを意味する。 係数がマイナスならば，係数が値が増えると従属変数が減る関係にあることを意味する。 さっきの散布図に回帰分析から求めた回帰直線を引いてみよう。 p = qplot(data = sample_data, x1, y) p = p + stat_smooth(method = &quot;lm&quot;, se = FALSE) p ## `geom_smooth()` using formula &#39;y ~ x&#39; 概ねデータに重なるかたちで直線が引かれている。 他にも出力に「Multiple R-Squared」とあるが，これは「決定係数」と呼ばれるもので，データの全ての分散のうち，今回の回帰分析によって説明できている割合を意味する。0から1の範囲を取り，1に近いほど今回の回帰分析の結果がデータを良く説明できていることを意味する。 \\[ \\begin{equation} R^2 = \\frac{\\sum^n_{i=1}(\\hat{y}_i-\\bar{y})^2}{\\sum^n_{i=1}(y_i-\\bar{y})^2} \\end{equation} \\] \\(y\\)は実測値，\\(\\bar{y}\\)は実測値yの平均値，\\(\\hat{y}\\)は回帰分析で求めたyの予測値を意味する。 10.1.2 最小二乗法（＊） では，回帰分析では傾きと切片の値をどう求めているのだろうか？ 実測値と予測値との差は残差（Residual value）と呼ばれる。残差が最も小さくなるときの傾き及び切片を，yを予測する上で最適な傾き及び切片の値として採用する（残差が最小となるときの式が，データをうまく説明できていると解釈できる）。 このような回帰式の切片及び傾きの求め方を，「最小二乗法」という。 ある条件のもとで結果を最小あるいは最大にする手法のことを，数学では最適化と呼ぶ。最小二乗法とは最適化手法の一種である。 当然ながら手計算では困難である。普通はコンピュータを使って計算する。Rには最適化を行うための関数optim()が用意されている。 あくまで参考までに，最小二乗法をoptim()で行ったプログラムを以下に示す。 y = sample_data$y x1 = sample_data$x1 residual = function(para){ b = para[1] a = para[2] hat_y = b*x1 + a #x1とパラメータa, bから，予測値hat_yを計算する sum((y - hat_y)^2) #予測値と実測値との差の二乗の合計値を求めたものをresidualという変数で保存 } optim(par = c(1, 1), residual) #特に何も指定しなければ，residualという変数を最小化せよという命令となる（par=c(1, 1)は，bとaを計算するときの初期値を1, 1にせよという命令。あまり深く考えなくて良い）。出力結果の「par」に，傾き(b)と切片(a)の推定結果が出る。lm()関数を使って計算した値と近似しているはず。 ## $par ## [1] 3.379037 10.852593 ## ## $value ## [1] 2339.32 ## ## $counts ## function gradient ## 67 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 最小二乗法以外にも，「最尤法（さいゆうほう）」という最適化手法でも同じ回帰係数を求めることができる。最尤法については，一般化線形モデルの回で説明する。 10.2 重回帰分析 独立変数が複数の場合の回帰分析は，重回帰分析(multivariant regression analysis)と呼ばれる。 独立変数が一つの場合は，単回帰分析と呼んで区別することもある。 例えば，独立変数が2つの場合の回帰式は以下のようになる。 \\[ \\begin{equation} \\hat{y} = b_{1}x_{1}+b_{2}x_{2} + a \\end{equation} \\] result_2 = lm(data=sample_data, y ~ x1 + x2) summary(result_2) ## ## Call: ## lm(formula = y ~ x1 + x2, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.812 on 97 degrees of freedom ## Multiple R-squared: 0.3544, Adjusted R-squared: 0.3411 ## F-statistic: 26.63 on 2 and 97 DF, p-value: 6.049e-10 例えば変数 X1 の回帰係数は，他の独立変数の効果が同じとした場合に X1 が1単位増えた時の y の変化量を意味している。 このように他の変数の影響を統制（control）した上で独立変数の効果を検討することができるのが，回帰分析のメリットの一つである。 10.2.1 多重共線性（＊） 独立変数同士が強く相関していると，回帰分析の結果は信用できない恐れがある。このような問題は「多重共線性（multicollinearity）」と呼ばれる。例えば，\\(x_{1}\\)と\\(x_{2}\\)が相関している，つまりほぼ\\(x_{1}=x_{2}\\)と言える場合，傾きの値もほぼ\\(b_{1}=b_{2}\\)となる。この場合，傾きの組み合わせが複数ありえるため（\\(b_{1}=1, b_{2}=2\\)や\\(b_{1}=2, b_{2}=1\\)），パラメータの値を推定することが出来ない。 独立変数同士の相関が0.8を超えている場合は，多重共線性を疑った方が良い。 多重共線性を疑う基準として，VIF(Variance inflation factor)と呼ばれる指標もある。VIFが10を超えていたら，多重共線性を疑った方が良いとされている。 \\[ \\begin{equation} VIF = \\frac{1}{1-R^2} \\end{equation} \\] 上の式の\\(R^2\\)は，ある独立変数を従属変数としたときの他の独立変数による重回帰分析での決定係数を意味する。 carパッケージのvif()を使うと，VIFを計算してくれる。 car::vif(result_2) #lm()関数で計算した回帰分析の結果を入れれば良い。 ## x1 x2 ## 1.000645 1.000645 10.2.2 決定係数 先に説明したように，決定係数は回帰分析によって説明されるデータの分散の割合，すなわち「回帰分析の精度の良さ」を示す指標として認識されている。 しかし，決定係数は基本的に独立変数を多く入れれば大きい値を取る傾向にある（意味のない変数を入れたとしても，データの分散を多少は説明できるようになる）。 この問題を解消するために，決定係数とは別に「情報量基準」という指標が用いられている。詳しくは「一般化線形モデル」の回で説明する。 10.3 ダミー変数（独立変数が質的変数の場合） 上の例は独立変数が量的変数の場合を用いたが，質的変数（性別，学生か否かなど）の場合でも可能である。解析の際には，変数を0か1の変数に変換する必要がある。ダミー変数（dummy variable）と呼ばれる。 #サンプルデータの作成 set.seed(1234) y = rnorm(100) x = round(runif(100),0) sampledata_2 = data.frame(y = y,x = x) head(sampledata_2) #sampledata_2の上数行だけを表示する。 ## y x ## 1 -1.2070657 1 ## 2 0.2774292 1 ## 3 1.0844412 0 ## 4 -2.3456977 1 ## 5 0.4291247 1 ## 6 0.5060559 1 result_3 = lm(data = sampledata_2, y ~ x) summary(result_3) ## ## Call: ## lm(formula = y ~ x, data = sampledata_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0686 -0.7651 -0.1993 0.6510 2.6929 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02642 0.14456 -0.183 0.855 ## x -0.25066 0.20047 -1.250 0.214 ## ## Residual standard error: 1.002 on 98 degrees of freedom ## Multiple R-squared: 0.0157, Adjusted R-squared: 0.005658 ## F-statistic: 1.563 on 1 and 98 DF, p-value: 0.2142 独立変数が一つで二値の場合，回帰係数の有意性検定の結果は，t検定を行った場合と一致する。つまり，独立変数が二値の単回帰分析でやっていることは，２群間の平均値の差の検定と同じである。その理由は，一般化線形モデルの回で説明する。 t.test(data = sampledata_2, y ~ x, paired = F, var.equal = T) #等分散を仮定した場合の検定。 ## ## Two Sample t-test ## ## data: y by x ## t = 1.2503, df = 98, p-value = 0.2142 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1471737 0.6484883 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.02641995 -0.27707724 練習問題 問１，問２いずれも宿題とする。なお，数値は小数点第３位まで報告せよ（小数点第４位以下は四捨五入）。 問１ Rにはattitudeというデータが入っている。ある金融機関の30部門で従業員に行ったアンケート調査の結果である。各部署ごとに，７つの質問項目について好意的な評価をした人の割合が示されている。 ちなみに，データのcomplaints, privileges, learning, raises, critical, advanceはそれぞれ，「従業員の不満への対応」，「特権を許さない」，「学習の機会」，「能力に応じた昇給」，「批判的すぎる」，「昇進」を意味する。 head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 ratingを従属変数，その他の変数（complaints, privileges, learning, raises, critical, advance）を独立変数とした重回帰分析を行い， 小問１ それぞれの独立変数の傾きの係数及び検定の結果を報告せよ（係数の値とp値だけでよい）。 ヒント：一般的に回帰係数の推定結果を論文などで報告するときは，(b=x.xxx, p=.xxx)とする。bは回帰係数を意味する。 小問２ 5%水準で有意な効果を持った独立変数を挙げ，それらの独立変数の増減によって従属変数がどう変化する傾向にあるかを報告せよ。 小問３ learningが１単位増えると，ratingはいくら変化するかを述べよ。 問２ 以下のプログラムを読み込み，サンプルデータ dat_q2 を作成する。 このデータには，A県とB県それぞれで10人の生徒を選んで学力テストを行った結果を示している（架空の調査である）。Prefectureが県，Value がテストの成績を意味する。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(55, 50, 52, 61, 70, 59, 55, 51, 52, 84) Value = c(A, B) Prefecture = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) dat_q2 = data.frame(Prefecture = Prefecture, Value = Value) dat_q2 ## Prefecture Value ## 1 A 38 ## 2 A 53 ## 3 A 61 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 55 ## 12 B 50 ## 13 B 52 ## 14 B 61 ## 15 B 70 ## 16 B 59 ## 17 B 55 ## 18 B 51 ## 19 B 52 ## 20 B 84 A県=1, B県=0のダミー変数を作り，そのダミー変数を独立変数，Value を従属変数とした回帰分析を行い， 小問１ A県=1を意味するダミー変数の傾きの推定値とp値を報告せよ。 ヒント：このダミー変数を作る場合には，以下のようにプログラムを書くと良い（今後のためにも覚えておくと良い）。 dat_q2$A = ifelse(dat_q2$Prefecture == &quot;A&quot;, 1, 0) #「もしPrefectureがAならば1，それ以外ならば0とする，新しい変数Aを作れ」という命令 ヒント：実はダミー変数を作らなくても，質的変数（i.e., Prefecture）を独立変数としてそのまま入れてもRは自動でダミー変数を作って計算してくれる。 小問２ t検定（等分散を仮定する）で，A県とB県の間で Value の平均値の比較を行い，A県とB県の間の平均値の差が5%水準で有意かどうかを述べよ（t値，自由度，p値も報告すること）。 ヒント：対応のあるt検定か対応のないt検定，どちらが適切か？ "],
["09-linear_model.html", "Chapter 11 線形モデル 11.1 一般化線形モデル 11.2 一般化線形モデルと統計解析との関係 11.3 Rでの一般化線形モデルの方法 11.4 線形モデルに含まれる統計解析 練習問題", " Chapter 11 線形モデル 線形モデルについて学ぶ。 一般化線形モデル 線形モデル これまで学んできたいくつかの分析（t検定，分散分析，回帰分析）が，「線形モデル」という一つの枠組みで捉えることができることを理解する。 まず，一般化線形モデルの前に，線形モデルを学ぶ。線形モデルは，一般化線形モデルに含まれるものである。今後学ぶもの関係図を示すと，以下のようになる。 Figure: 一般化線形混合モデル，一般化線形モデル，線形モデルの関係図 11.1 一般化線形モデル 一般化線形モデルとは，データを線形の式で従属変数と独立変数の関係で表現したモデルのことをいう。 一般化線形モデルを理解するには，「線形予測子」，「誤差分布」，「リンク関数」の３つの要素を知っておく必要がある。 11.1.1 線形予測子 従属変数と独立変数を表した式。いわゆる，「回帰式」である。 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] xを独立変数（説明変数），yを従属変数（応答変数または目的変数）と呼ぶ。独立変数は1個でなくても構わない。また，独立変数は質的変数でも量的変数でも構わない（その両方でもOK）。 線形予測子から，従属変数の予測値(\\(\\hat{y}\\))を推定する。 11.1.2 誤差分布 従属変数の実際の値(\\(y\\))と従属変数の予測値(\\(\\hat{y}\\))の誤差が従う確率分布のことをいう。 11.1.2.1 確率分布の復習 確率分布の種類として，正規分布，二項分布，ポアソン分布などがあった。変数の種類によって，ある変数が従うだろうと仮定する確率分布が異なることを学んだ。 例えば，連続量ならば正規分布，二値の値ならば二項分布，カウントデータ（非負の離散値）ならばポアソン分布というように，変数の特徴によって仮定する確率分布が異なる。 11.1.3 リンク関数 線形予測子と従属変数との関係をリンクさせる関数。 恒等リンク（線形予測子を変形せずにそのまま利用すること），logリンク，logitリンク関数などがある（誤差分布によってどのリンク関数を指定するかは大体定まっている）。 恒等リンクの場合 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] logがリンク関数の場合 \\[ \\begin{equation} \\log\\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] logitがリンク関数の場合 \\[ \\begin{equation} \\log\\frac{\\hat{y}}{1-\\hat{y}} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] 詳しくは次回以降で触れるが，線形予測子そのままだと，推定されるyの予測値は，-∞から∞の値をとり得る。これだと，従属変数の種類によっては不自然な場合もある（例えば従属変数が1個，2個といった頻度なのに-1個といった負の値が推定されてしまう，確率を推定したいのに，1.5や-1など0から1に収まらない値が推定されてしまうなど）。 そのため，リンク関数によって，推定される従属変数の予測値を正の値に限定したり（対数を求める），0から1の範囲に限定する（ロジットを取る）事を行う。 誤差分布とリンク関数については，次回で詳しく扱う。とりあえず今回は，一般化線形モデルはこれら3つの要素で構成されていると言うことを理解しておく。 11.2 一般化線形モデルと統計解析との関係 誤差分布やリンク関数の組み合わせによって，様々な解析を表現することが可能になる。 重回帰分析：誤差分布が正規分布，リンク関数には何も指定しない（線形式をそのまま使う） ロジスティック回帰：誤差分布が二項分布，リンク関数はロジット ポアソン回帰：誤差分布がポアソン分布，リンク関数はlog 対数線形分析：誤差分布がポアソン分布，リンク関数はlog（ポアソン回帰と同じだが，説明変数がカテゴリカルのみの場合） つまり，一般化線形モデルとは，ある特定の分析手法を指す言葉ではなく，「あらゆる統計解析を共通の枠組みから包括的に理解する統計解析の考え方」である。誤差分布やリンク関数をカスタマイズすることで，あらゆるデータの解析に対応させることができるといったイメージである。 今回は，「従属変数の誤差分布を正規分布，リンク関数には何も指定しない場合」に，どのような解析ができるかについて見ていく。 要は前回学んだ回帰分析の式をそのまま扱う。新しく学ぶことはなにもない。今回の目的は，「回帰分析の式により，t検定など他の分析も行うことができる」ことを理解することである。 「従属変数の誤差分布を正規分布，リンク関数には何も指定しない」場合の統計モデルのことを，「線形モデル(linear model)」と表現することもある。 正確には，「一般線形モデル（general linear model）」と呼ばれる事が多い。ただ，これだと一般”化”線形モデル（generalized linear model）と区別がつきにくいので，ここでは”線形モデル”と表現する。 11.3 Rでの一般化線形モデルの方法 Rには，一般化線形モデルで解析を行うためのglm()が用意されている。 前回の回帰分析の練習との時に用いたデータを使い，glm()で重回帰分析を行ってみる。 set.seed(1234) N = 100 a = 10 b1 = 3 b2 = 0.5 x1 = rnorm(n = N) x2 = rnorm(n = N) e = rnorm(n = N, sd = 5) y = b1*x1 + b2*x2 + a + e sample_data = data.frame(x1 = x1, x2 = x2, y = y) head(sample_data) #サンプルデータの上数行を表示 ## x1 x2 y ## 1 -1.2070657 0.41452353 9.012199 ## 2 0.2774292 -0.47471847 14.078772 ## 3 1.0844412 0.06599349 14.213890 ## 4 -2.3456977 -0.50247778 6.215336 ## 5 0.4291247 -0.82599859 12.432780 ## 6 0.5060559 0.16698928 15.403974 qplot(data = sample_data, x1, y) #x1とyの散布図を示す 今回のデータは正規分布に従うという前提を置くことにする。 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} \\\\ y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] まず，独立変数と切片及び傾きのパラメータから従属変数yの予測値を求める。予測値と実測値との間には誤差が生じると考えられる。その誤差は予測値を平均，\\(\\sigma\\)を標準偏差とする正規分布に従うだろうと考える。 result_glm = glm(data = sample_data, y ~ x1 + x2, family = gaussian(link = &quot;identity&quot;)) summary(result_glm) ## ## Call: ## glm(formula = y ~ x1 + x2, family = gaussian(link = &quot;identity&quot;), ## data = sample_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 23.15325) ## ## Null deviance: 3479.0 on 99 degrees of freedom ## Residual deviance: 2245.9 on 97 degrees of freedom ## AIC: 602.96 ## ## Number of Fisher Scoring iterations: 2 familyで指定しているのが誤差分布である。ここでは，gaussian（正規分布はガウス分布とも呼ばれる）としている。 linkで指定しているのが，リンク関数である。ここでは，identity（恒等リンク。要は何も変換しないで線形式そのまま使うという意味）。 なお，family及びlinkを特に何も指定しなければ，デフォルトでそれぞれgausian, identityとなる。 前回は重回帰分析を行うための関数はlm()であると学んだ。lm()を使った結果と比べてみよう。 result_lm = lm(data = sample_data, y ~ x1 + x2) summary(result_lm) ## ## Call: ## lm(formula = y ~ x1 + x2, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.812 on 97 degrees of freedom ## Multiple R-squared: 0.3544, Adjusted R-squared: 0.3411 ## F-statistic: 26.63 on 2 and 97 DF, p-value: 6.049e-10 全く同じ結果が出力されている。 lmはlinear modelの略で，lm()は線形モデル（誤差分布が正規分布で，リンク関数が恒等リンクの場合の一般化線形モデル）の解析を行うための関数である。 11.4 線形モデルに含まれる統計解析 線形モデルでできる解析は，重回帰分析に限らない。lm()の結果は説明変数がどのような種類かによって，いわゆる 独立変数が一つ以上で，独立変数が質的か量的変数かは問わない → 回帰分析 独立変数が一つで，独立変数が二値[0, 1]の変数 → t検定 独立変数が一つ以上で，独立変数がすべて質的変数 → 分散分析 更に共変量を加える → 共分散分析 と呼ばれる統計解析と対応する。 11.4.1 重回帰分析 既に学んだので省略。 11.4.2 t検定 二群間の差の検定を，対応のないt検定を行った場合と群をダミー変数とした単回帰分析の結果は一致する。 set.seed(1234) y = rnorm(100) x = round(runif(100),0) dat_t = data.frame(y = y, x = x) t.test(data = dat_t, y ~ x, paired = F, var.equal = T) #等分散を仮定する ## ## Two Sample t-test ## ## data: y by x ## t = 1.2503, df = 98, p-value = 0.2142 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1471737 0.6484883 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.02641995 -0.27707724 summary(lm(data = dat_t, y ~ x)) ## ## Call: ## lm(formula = y ~ x, data = dat_t) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0686 -0.7651 -0.1993 0.6510 2.6929 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02642 0.14456 -0.183 0.855 ## x -0.25066 0.20047 -1.250 0.214 ## ## Residual standard error: 1.002 on 98 degrees of freedom ## Multiple R-squared: 0.0157, Adjusted R-squared: 0.005658 ## F-statistic: 1.563 on 1 and 98 DF, p-value: 0.2142 回帰分析の傾きの検定は，「傾きがゼロである」という帰無仮説の検定をしている。 傾きの係数が意味することは，独立変数xが1単位増えたときのyの変化量であった。傾きの検定は，「x=0 から x=1 に変化することによって， y が上昇（下降）するか（傾きがゼロではないか）」を検定している。 すなわち，「x=0とx=1の間にyの値に差があるかを検定している」のと同じである。 * t検定も，測定値の誤差が正規分布に従うという前提が置かれている。 このように，2群間の対応のないt検定も線形モデルの中に含まれる。 ただし，対応のあるt検定の場合は線形モデルでは表現できない。これについては，「一般化線形混合モデル」の回で扱う。 11.4.3 分散分析 分散分析は，独立変数が質的変数の場合（ダミー変数に変換する必要あり）の線形モデルである。 Rで分散分析をしたい場合は，lm()とanova()を使えば良い。各変数の主効果・交互作用効果について，F統計量が出力される。 11.4.3.1 1要因の分散分析 #サンプルデータ set.seed(1234) y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;) #要因1：A,かBかC dat = data.frame(y = y, x1 = x1) result_anova = aov(data = dat, y ~ x1) summary(result_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 2 3.972 1.9861 3.374 0.0806 . ## Residuals 9 5.298 0.5886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 result_anova = lm(data = dat, y ~ x1) anova(result_anova) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 2 3.9721 1.98607 3.3741 0.08064 . ## Residuals 9 5.2976 0.58863 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(result_anova) ## ## Call: ## lm(formula = y ~ x1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0913 -0.4933 0.2020 0.5015 1.0775 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.2544 0.3836 -3.270 0.00968 ** ## x1B 1.1751 0.5425 2.166 0.05849 . ## x1C 1.2613 0.5425 2.325 0.04512 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7672 on 9 degrees of freedom ## Multiple R-squared: 0.4285, Adjusted R-squared: 0.3015 ## F-statistic: 3.374 on 2 and 9 DF, p-value: 0.08064 lm()は変数が文字列でも，自動でダミー変数化してくれる。今回の結果で言うと，X1BとX1Cという変数が自動で作られており，要因1がAのときはX1B=0かつX1C=0，BのときはX1B=1かつX1C=0，CのときはX1B=0かつX1C=1となっている（ダミー変数の個数は「水準の数-1」）。 線形モデルの結果のsummaryを見てみると，Bのダミー変数の係数が正でかつ有意であった。このことから，条件Bにおいてyの値が顕著に高い傾向にあることがわかる。 ただし，これでわかるのはB条件が従属変数に及ぼす効果の強さのみである。BとAの間，あるいはBとCの間に有意な差があるかはわからない。条件間の差に興味があるのならば，多重比較を行う必要がある。 このように，単に各独立変数が従属変数に及ぼす効果を見たいときは，線形モデルの方が結果を直感的に理解しやすい。 ただし，自動で変数名がつけられて混乱するので，自分でダミー変数を作り直したほうが良い。 11.4.3.2 2要因以上の分散分析 独立変数の主効果だけではなく交互作用も考える。分散分析は独立変数（要因）の数が増えると複雑になるが，線形モデルならばモデルを立てるのも，結果を解釈するのもわかりやすい。 #サンプルデータ: 2x2の2要因配置 set.seed(1234) y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;) #要因1：A,かB x2 = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;) #要因2:MかF dat_anova = data.frame(y = y, x1 = x1, x2 = x2) #ダミー変数を作る dat_anova$x1_A = ifelse(dat_anova$x1 == &quot;A&quot;, 1, 0) dat_anova$x2_M = ifelse(dat_anova$x2 == &quot;M&quot;, 1, 0) dat_anova ## y x1 x2 x1_A x2_M ## 1 -1.2070657 A M 1 1 ## 2 0.2774292 B M 0 1 ## 3 1.0844412 A M 1 1 ## 4 -2.3456977 B M 0 1 ## 5 0.4291247 A M 1 1 ## 6 0.5060559 B M 0 1 ## 7 -0.5747400 A F 1 0 ## 8 -0.5466319 B F 0 0 ## 9 -0.5644520 A F 1 0 ## 10 -0.8900378 B F 0 0 ## 11 -0.4771927 A F 1 0 ## 12 -0.9983864 C F 0 0 result_anova2 = lm(data = dat_anova, y ~ x1_A + x2_M + x1_A:x2_M) #lm(data=dat_anova, y ~ x1_A*x2_M) #これでも同じ summary(result_anova2) ## ## Call: ## lm(formula = y ~ x1_A + x2_M + x1_A:x2_M, data = dat_anova) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.82496 -0.10544 0.01797 0.44476 1.02679 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.8117 0.5746 -1.413 0.195 ## x1_A 0.2729 0.8126 0.336 0.746 ## x2_M 0.2909 0.8126 0.358 0.730 ## x1_A:x2_M 0.3500 1.1493 0.305 0.768 ## ## Residual standard error: 0.9953 on 8 degrees of freedom ## Multiple R-squared: 0.1451, Adjusted R-squared: -0.1755 ## F-statistic: 0.4526 on 3 and 8 DF, p-value: 0.7226 anova(result_anova2) #分散分析表が出力される ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1_A 1 0.6018 0.60184 0.6076 0.4581 ## x2_M 1 0.6513 0.65134 0.6575 0.4409 ## x1_A:x2_M 1 0.0919 0.09188 0.0928 0.7685 ## Residuals 8 7.9247 0.99059 交互作用は独立変数同士をかけ合わせた項で表現される。 Rのlm()ならば，x1_A:x2_Mが交互作用項を意味する。x1_A*x2_Mとすれば，主効果と交互作用効果の項の両方が自動で入る。 ただし，ここで行っているb分散分析は，要因がどれも「対応なし」の場合である。「対応あり」の要因が含まれる分散分析は，lm関数ではできない。このような場合どうするかは，「一般化線形混合モデル」の回で扱う。 11.4.4 共分散分析 例えば，ある大学Aと大学Bで，体重測定を行ったとする。その結果，大学Aの学生の方が大学Bの学生よりも平均体重が大きかった。しかし，もしかしたら大学Aには身長が高い学生が多く，それにより体重にも違いが生じてしまったのかもしれない（体重と身長は正の相関関係にある）。 共分散分析（ANCOVA: analysis of covariance）は，データに影響を及ぼす別の変数の影響を統制した上で平均値の群間比較を行う分散分析である。要因に影響を及ぼす別の変数のことを，共変量と呼ぶ。 要は重回帰分析である。重回帰分析の利点の１つとして，他の変数の影響を統制した上での，ある独立変数が従属変数に及ぼす効果を見れる点があると学んだ。共分散分析では，共変量の効果を統制した上で，注目する独立変数の効果を見ている。 #サンプルデータ # 2x2の2要因配置 y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;) #要因1：A,かB x2 = rnorm(12) #共変量 dat_ancova = data.frame(y=y, x1=x1, x2=x2) result_ancova = lm(data = dat_ancova, y ~ x1 + x2) summary(result_ancova) ## ## Call: ## lm(formula = y ~ x1 + x2, data = dat_ancova) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0912 -0.5103 -0.1350 0.2195 2.1292 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1923 0.4395 -0.438 0.673 ## x1B 0.5891 0.6970 0.845 0.423 ## x1C 0.9224 1.2062 0.765 0.466 ## x2 0.2316 0.4456 0.520 0.617 ## ## Residual standard error: 1.047 on 8 degrees of freedom ## Multiple R-squared: 0.1074, Adjusted R-squared: -0.2273 ## F-statistic: 0.3208 on 3 and 8 DF, p-value: 0.8104 次回は更に，誤差分布及びリンク関数を別の物に変えた時に，どのような解析ができるのかについて検討する。 練習問題 問１は宿題にしない。問２を宿題とする。小数点第３位まで報告すること（小数点第４位以下は四捨五入）。 問１ 一般化線形モデルを構成する「線形予測子」，「誤差分布」，「リンク関数」それぞれを説明せよ。 問２ 以下のプログラムを読み込み，サンプルデータを作成する。 Height = c(149.9, 164.8, 172.8, 138.5, 166.3, 167.1, 156.3, 156.5, 156.4, 153.1, 145.2, 140, 142.2, 150.6, 159.6, 148.9, 144.9, 140.9, 141.6, 174.2) Class = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;,10)) A = c(rep(1, 10), rep(0,10)) Weight = c(56.3, 50.1, 50.6, 59.6, 48.1, 40.5, 60.7, 44.8, 54.8, 45.6, 66, 50.2, 47.9, 50, 38.7, 43.3, 33.2, 41.6, 52.1, 50.3) sample_data = data.frame(Height = Height, Class = Class, A = A, Weight = Weight) sample_data ## Height Class A Weight ## 1 149.9 A 1 56.3 ## 2 164.8 A 1 50.1 ## 3 172.8 A 1 50.6 ## 4 138.5 A 1 59.6 ## 5 166.3 A 1 48.1 ## 6 167.1 A 1 40.5 ## 7 156.3 A 1 60.7 ## 8 156.5 A 1 44.8 ## 9 156.4 A 1 54.8 ## 10 153.1 A 1 45.6 ## 11 145.2 B 0 66.0 ## 12 140.0 B 0 50.2 ## 13 142.2 B 0 47.9 ## 14 150.6 B 0 50.0 ## 15 159.6 B 0 38.7 ## 16 148.9 B 0 43.3 ## 17 144.9 B 0 33.2 ## 18 140.9 B 0 41.6 ## 19 141.6 B 0 52.1 ## 20 174.2 B 0 50.3 ある学校でAクラスとBクラスの二つのクラスで，生徒の身長と体重の測定を行ったとする。それぞれ，身長（Height），クラス(Class: A or B)，体重（Weight）が保存されている。更に，クラスAか否かを意味するダミー変数（A: 1=クラスA, 0=クラスB）もある。 このデータを使って，一般化線形モデルで身長を従属変数，体重とクラスを独立変数とした分析を行え。ただし，従属変数の誤差分布は正規分布に従うとし，リンク関数は恒等リンク関数とすること。 ヒント：つまり，「重回帰分析を行え」ということ。 結果を報告せよ（独立変数の係数及びp値を報告すること）。また，分析の結果から，独立変数の増減が従属変数にどう影響を及ぼす傾向にあるかを述べよ。 "],
["10-glm.html", "Chapter 12 一般化線形モデル 12.1 一般化線形モデル（再掲） 12.2 今回の予定 12.3 ロジスティック回帰 12.4 最尤法（＊） 12.5 ポアソン回帰分析（＊） 12.6 モデルの予測力 練習問題 参考文献", " Chapter 12 一般化線形モデル 誤差分布が正規分布以外の場合の「一般化線形モデル」について学ぶ。 ロジスティック回帰 最尤法 ポアソン回帰 install.packages(&quot;tidyverse&quot;, &quot;MASS&quot;) library(tidyverse) library(MASS) 12.1 一般化線形モデル（再掲） 一般化線形モデルとは，線形の式で従属変数と独立変数の関係を表した統計モデルのことをいう。一般化線形モデルを理解するには，以下の３つの要素を知っておく必要がある。 12.1.1 線形予測子 従属変数と独立変数を表した式。いわゆる，「回帰式」である。 \\[ \\hat{y} = \\beta_{0} + \\beta_{1} x \\tag{1}\\\\ \\] xを独立変数（説明変数），yを従属変数（応答変数または目的変数）と呼ぶ。独立変数は1個でなくても構わない。また，独立変数は質的変数でも量的変数でも構わない（その両方でもOK）。 線形予測子から，従属変数の予測値(\\(\\hat{y}\\))を推定する。 12.1.2 誤差分布 従属変数の実際の値(\\(y\\))と従属変数の予測値(\\(\\hat{y}\\))の誤差が従う確率分布のことをいう。 確率分布の復習 正規分布，二項分布，ポアソン分布などがあった。変数の種類によって，ある変数が従うだろうと仮定する確率分布が異なることを学んだ。 例えば，連続量ならば正規分布，二値の値ならば二項分布，カウントデータ（非負の離散値）ならばポアソン分布というように，変数の特徴によって仮定する確率分布が異なる。 12.1.3 リンク関数 線形予測子と従属変数との関係をリンクさせる関数。 恒等リンク（線形予測子を変形せずにそのまま利用すること），logリンク，logitリンク関数などがある（誤差分布によってどのリンク関数を指定するかは大体定まっている）。 12.1.4 一般化線形モデルと統計解析との関係 誤差分布やリンク関数の組み合わせによって，様々な解析を表現することが可能になる。 重回帰分析：誤差分布が正規分布，リンク関数には何も指定しない（線形式をそのまま使う） ロジスティック回帰：誤差分布が二項分布，リンク関数はロジット ポアソン回帰：誤差分布がポアソン分布，リンク関数はlog 対数線形分析：誤差分布がポアソン分布，リンク関数はlog（ポアソン回帰と同じだが，説明変数がカテゴリカルのみの場合） つまり，一般化線形モデルとはある分析手法を指す言葉というよりも，「あらゆる統計解析を共通の枠組みから包括的に理解する統計解析の考え方」といえる。 12.2 今回の予定 前回は，誤差分布が正規分布である場合の解析（回帰分析，t検定，分散分析など）を扱った。今回は，誤差分布として正規分布以外を仮定する場合の解析について扱う。 Figure: 一般化線形混合モデル，一般化線形モデル，線形モデルの関係図 12.3 ロジスティック回帰 ロジスティック回帰は，「ある事象が生じる確率を予測する回帰分析」である。誤差分布が二項分布に従う場合において，二項分布のパラメータq（ある事象が生じる確率）を独立変数から推定する回帰分析である。 12.3.1 二項分布 \\[ P(r) = {}_n\\mathrm{C}_rq^{r}(1-q)^{(n-r)} \\tag{2} \\] qはある事象が生じる確率（投げたコインが表の確率など），nがすべての試行数，rがある事象が生じた回数を意味する。 n=1のときは，「ベルヌーイ分布」と呼ばれる（例えば誰か1人を選んだときに，その人が病気であるか否かなど）。 二項分布は，生じる事象が2つのカテゴリに分けられる場合に当てはまる確率分布である。コインを投げたときに出る面が「表か裏」か，学生の中から選んだ人の性別が「男か女」か，ある意見について「賛成か反対」かなど。このような事象が生じる確率は，理論的には二項分布に従う。 12.3.2 例題 例えば次のようなデータを考えてみる。MASSパッケージに入っている生検の結果のサンプルデータを用いる。 library(MASS) b &lt;- biopsy b$classn[b$class==&quot;benign&quot;] &lt;- 0 b$classn[b$class==&quot;malignant&quot;] &lt;- 1 head(b) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class classn ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 V1は整数の変数，classnは1ならば癌，0ならば癌ではないことを意味する変数とする。 まず，独立変数V1と従属変数classnとの関係をプロットする。 p1 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + xlab(&quot;V1&quot;) + ylab(&quot;classn&quot;) p1 これに回帰分析のモデルを当てはめる。 fit_lm = lm(data=b, classn ~ V1) summary(fit_lm) ## ## Call: ## lm(formula = classn ~ V1, data = b) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77804 -0.17331 -0.01994 0.06859 1.06859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.189535 0.023395 -8.102 2.43e-15 *** ## V1 0.120947 0.004467 27.078 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3323 on 697 degrees of freedom ## Multiple R-squared: 0.5127, Adjusted R-squared: 0.512 ## F-statistic: 733.2 on 1 and 697 DF, p-value: &lt; 2.2e-16 p2 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + stat_smooth(method=lm, se=F, size=2) + xlab(&quot;x&quot;) + ylab(&quot;q&quot;) p2 ## `geom_smooth()` using formula &#39;y ~ x&#39; 推定された回帰式をプロット上に当てはめる。 がんにかかるリスク（確率）を推定したいが，例えばxが10を超えると，従属変数の予測値として1以上の値が推定されてしまう。xが2.5を下回ったときも，0未満の数値が推定されてしまう。従属変数は0か1しか取らない変数なのに，それぞれを超える値が予測されてしまう。これは確率の推定としては不都合である。 そこで，この直線を以下のように変換してみたらどうだろうか。 p3 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + stat_smooth(method=glm, method.args = list(family = &quot;binomial&quot;),se=F, size=2) + xlab(&quot;x&quot;) + ylab(&quot;q&quot;) p3 ## `geom_smooth()` using formula &#39;y ~ x&#39; この予測線は，以下の式で作成されたものである。 \\[ q_{i} = \\frac{1}{1+\\exp[-(\\beta_{0} + \\beta_{1}x_{i})]} \\tag{3} \\] qは「y=1が生じる確率」を意味する。\\(1/(1+\\exp(-y))\\)は，ロジスティック関数と呼ばれる。 xがどのような値をとっても，\\(q_{i}\\)は\\(0\\leq q \\leq1\\)となる。 更に，式(3)は，以下の式(4)に変換することができる（理由は付録参照）。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] 左辺のことを，ロジット関数（logit function）という。つまり，線形の式をロジット関数でリンクさせたものがロジスティック回帰である（リンク関数を”ロジット”とする理由）。 まとめると，ロジスティック回帰は誤差分布が二項分布，リンク関数をロジット関数とした一般化線形モデルのことをいう。 12.3.3 まとめ ロジスティック回帰は，独立変数がある事象が生じる確率に及ぼす影響を推定したいときに使う回帰分析である。従属変数が0か1を取る変数の場合には，ロジスティック回帰を行う。 一般化線形モデルの誤差分布を二項分布，リンク関数をロジット関数としたものが，ロジスティック回帰に相当する。 12.3.4 Rでのロジスティック回帰 glm()関数で行う。familyにbinomial, linkにlogitを設定すれば良い。先程のサンプルデータbを使って，glm()関数でロジスティック回帰を行ってみる。 fit_logistic = glm(data=b, classn ~ V1, family = binomial(link=&quot;logit&quot;)) summary(fit_logistic) ## ## Call: ## glm(formula = classn ~ V1, family = binomial(link = &quot;logit&quot;), ## data = b) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## V1 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 (link=“logit”)は省略しても良い。familyにbinomialを設定すれば，リンク関数はデフォルトでlogitとなる。 結果の見方は，重回帰と同じである。 [Coefficients]の部分を見る。Estimateが係数。更にその効果のp値が表示される。係数の推定値の解釈は重回帰分析のときと同じで，プラスならば従属変数が1の値，マイナスならば従属変数が0の値を取りやすいことを意味する。 12.4 最尤法（＊） 線形回帰のときは，切片及び傾きの値は最小二乗法で求めると学んだ。ロジスティック回帰の場合はどうなのだろうか？ 一般化線形モデルでは，最尤法（さいゆうほう）と呼ばれる手法を通してパラメータの推定を行う。 表が出る確率が未知のコインを何回か投げて，表が出る確率\\(\\theta\\)（シータ）を推定するとする。 1回目は，表が出た。この実験結果が生じる確率は\\(\\theta\\)である。 2回目は，裏が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)\\)である。 3回目は，表が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)*\\theta\\)である。 その後，4回目は裏，5回目は裏だったとする。この実験結果が生じる確率は以下のように表すことができる。 \\[ L = (1-\\theta)^3 \\theta^2 \\tag{5} \\] Lのことを尤度(likelihood)と呼ぶ（”ゆうど”と読む。”いぬど”ではない）。 尤度とは「もっともらしさ」を示すものである。イメージとしては，「今回の実験結果が得られる確率」である。今回の観測データに対して最も当てはまりが良くなる，すなわち最も尤度の高いパラメータを求めるのが，最尤法と呼ばれる手法である。 掛け算を扱う尤度は計算が困難なので，対数化して足し算にする。対数化した尤度を対数尤度と呼ぶ。対数尤度が最大となるパラメータを求めるのが，最尤法である。 \\[ \\log L = \\log(1-\\theta)+\\log(1-\\theta)+\\log(1-\\theta)+\\log(\\theta)+\\log(\\theta) \\tag{6} \\] パラメータ\\(\\theta\\)と対数尤度\\(\\log L\\)との関係を以下に示す。 対数尤度が最も大きくなるのは，\\(\\theta= 0.40\\) のときである（2/5と一致）。 一般化線形モデルでは，最尤法で対数尤度の高くなる切片及び傾きを推定する（上の例の\\(\\theta\\)を線形予測子に置き換えて同様の計算をする）。 なお，回帰分析で切片及び傾きを求めるときに最小二乗法を使うと前回説明したが，最尤法でも同じ値が推定される（証明は省略）。 12.5 ポアソン回帰分析（＊） 従属変数がカウントデータ（非負の整数。1個，2個,3個といった個数など）の場合は，理論的にはポアソン回帰分析をするのが適切である。 確率分布はポアソン分布(poisson)，リンク関数はlogを指定する。 12.5.1 ポアソン分布（＊） \\[ P(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!} \\tag{7}\\\\ \\] yは0以上の整数（0, 1, 2, 3, …）とする。P(y)はyが生じる確率。 ポアソン分布のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，平均と分散が等しい分布である。 12.5.2 ポアソン回帰（＊） \\[ \\log\\lambda_{i}=\\beta_{0}+\\beta_{1}x_{i} \\tag{8}\\\\ \\] \\[ \\lambda_{i}=\\exp(\\beta_{0}+\\beta_{1}x_{i}) \\tag{9}\\\\ \\] ポアソン回帰では，誤差分布をポアソン分布，リンク関数を対数（log）に設定する。 12.5.3 例題（＊） 嶋田・阿部(2017)「Rで学ぶ統計学入門」 p. 167の演習問題（改題）。 鳥種Aの雄はときどき高い柱にとまって，自分の縄張りを見張る行動を示す。鳥の体重(wt)と見張り行動の発生数（mihari）を調べた。体重が見張り行動に及ぼす影響を調べる。 # サンプルデータ（嶋田・阿部(2017) p. 167より） mihari = c(7, 3, 3, 5, 10, 8, 3, 1, 2, 9, 8) wt = c(110, 88, 80, 95, 120, 103, 97, 84, 91, 114, 107) data_poisson &lt;- data.frame(mihari = mihari, wt = wt) head(data_poisson) ## mihari wt ## 1 7 110 ## 2 3 88 ## 3 3 80 ## 4 5 95 ## 5 10 120 ## 6 8 103 # family にpoisson，linkにlogを指定する。 result_poisson &lt;- glm(data = data_poisson, mihari ~ wt, family = poisson (link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = mihari ~ wt, family = poisson(link = &quot;log&quot;), data = data_poisson) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.06361 -0.55141 0.04139 0.52133 0.97431 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67643 1.17805 -2.272 0.023092 * ## wt 0.04263 0.01112 3.833 0.000127 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 19.8308 on 10 degrees of freedom ## Residual deviance: 4.2671 on 9 degrees of freedom ## AIC: 45.232 ## ## Number of Fisher Scoring iterations: 4 12.6 モデルの予測力 12.6.1 決定係数 回帰分析では，データに対する回帰分析のモデルの予測力を表す指標として，決定係数（R-squared）というものがある。 model_lm = lm(data=iris, Sepal.Length ~ Petal.Width) summary(model_lm) ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.38822 -0.29358 -0.04393 0.26429 1.34521 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.77763 0.07293 65.51 &lt;2e-16 *** ## Petal.Width 0.88858 0.05137 17.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.478 on 148 degrees of freedom ## Multiple R-squared: 0.669, Adjusted R-squared: 0.6668 ## F-statistic: 299.2 on 1 and 148 DF, p-value: &lt; 2.2e-16 summary(model_lm)$r.squared #r.squaredで決定係数のみを取り出すことができる。 ## [1] 0.6690277 これは，回帰式から求めた予測値と実測値の分散が，実際のデータの分散に占める割合を意味する指標である。つまり，回帰分析でどれだけ全データの分散を説明できているかを意味する。 \\[ R^2 = \\sum_{i=1}^{n} \\frac {(y_{i}-\\hat{y}_{i})^2}{(y_{i}-\\bar{y})^2} \\tag{10} \\] ただし，決定係数は単純に，独立変数が増えるほど大きくなる（説明できる分散の量が増える）。 model_lm2 = lm(data=iris, Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width) summary(model_lm2)$r.squared ## [1] 0.8586117 仮に独立変数に影響を及ぼさない変数を含めても，モデルのデータに対する説明力は上昇してしまう。 複雑なモデルは現在のデータによく当てはまるのは，ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし，複雑なモデルは現在のデータに当てはまっても，未知のデータにもうまく当てはまるとは限らない（overfittingと呼ばれる問題）。 良いモデルとは，「予測力が高く，かつできるだけシンプルである」ことである。 決定係数は上記の目的をかなえるのに常に適切な指標であるとは言えない。 12.6.2 赤池情報量基準（AIC） 変数の少なさとモデルの予測力の高さのバランスを取った指標の一つとして，AIC(Akaike inoformation criteria)が知られている。AICは以下の式で計算される。 \\[ AIC = -2 \\log L + 2k \\tag{11}\\\\ \\] \\(\\log L\\)は最大対数尤度，kはパラメータ（独立変数）の数である。 AICの値が低いほど，データに対するモデルの予測力が高いと評価する。 AICは余計なパラメータが多くなる（kが大きくなる）ほど大きい値を取る。つまり，データをうまく予測しつつ，かつパラメータ数を抑えてシンプルなモデルを探ることにかなっている。 Rではglm関数でAICの結果も出力してくれる。 model=glm(data=b, classn ~ V1+V2+V3+V4+V5+V6+V7+V8+V9, family=binomial(link=&quot;logit&quot;)) summary(model) ## ## Call: ## glm(formula = classn ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + ## V9, family = binomial(link = &quot;logit&quot;), data = b) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4841 -0.1153 -0.0619 0.0222 2.4698 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.10394 1.17488 -8.600 &lt; 2e-16 *** ## V1 0.53501 0.14202 3.767 0.000165 *** ## V2 -0.00628 0.20908 -0.030 0.976039 ## V3 0.32271 0.23060 1.399 0.161688 ## V4 0.33064 0.12345 2.678 0.007400 ** ## V5 0.09663 0.15659 0.617 0.537159 ## V6 0.38303 0.09384 4.082 4.47e-05 *** ## V7 0.44719 0.17138 2.609 0.009073 ** ## V8 0.21303 0.11287 1.887 0.059115 . ## V9 0.53484 0.32877 1.627 0.103788 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 884.35 on 682 degrees of freedom ## Residual deviance: 102.89 on 673 degrees of freedom ## (16 observations deleted due to missingness) ## AIC: 122.89 ## ## Number of Fisher Scoring iterations: 8 12.6.3 モデル選択（＊） 最も予測力の高いモデルを探す手続きを「モデル選択（model selection）」という。 MASSパッケージに入っているstepAICという関数を使ってAICが最も低いモデルの探索を行うことができる（stepwise法）。 MASS::stepAIC(glm(data=b, classn ~ V1+V2+V3+V4+V5+V6+V7+V8+V9, family=binomial(link=&quot;logit&quot;))) ## Start: AIC=122.89 ## classn ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## - V2 1 102.89 120.89 ## - V5 1 103.27 121.27 ## - V3 1 104.74 122.74 ## &lt;none&gt; 102.89 122.89 ## - V9 1 106.61 124.61 ## - V8 1 106.66 124.66 ## - V4 1 110.31 128.31 ## - V7 1 110.33 128.33 ## - V1 1 120.72 138.72 ## - V6 1 122.07 140.07 ## ## Step: AIC=120.89 ## classn ~ V1 + V3 + V4 + V5 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## - V5 1 103.27 119.27 ## &lt;none&gt; 102.89 120.89 ## - V9 1 106.66 122.66 ## - V3 1 106.66 122.66 ## - V8 1 106.76 122.76 ## - V4 1 110.64 126.64 ## - V7 1 110.70 126.70 ## - V1 1 121.10 137.10 ## - V6 1 122.07 138.07 ## ## Step: AIC=119.27 ## classn ~ V1 + V3 + V4 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## &lt;none&gt; 103.27 119.27 ## - V9 1 107.14 121.14 ## - V8 1 107.72 121.72 ## - V3 1 107.90 121.90 ## - V7 1 111.69 125.69 ## - V4 1 112.17 126.17 ## - V1 1 121.55 135.55 ## - V6 1 123.15 137.15 ## ## Call: glm(formula = classn ~ V1 + V3 + V4 + V6 + V7 + V8 + V9, family = binomial(link = &quot;logit&quot;), ## data = b) ## ## Coefficients: ## (Intercept) V1 V3 V4 V6 V7 ## -9.9828 0.5340 0.3453 0.3425 0.3883 0.4619 ## V8 V9 ## 0.2261 0.5312 ## ## Degrees of Freedom: 682 Total (i.e. Null); 675 Residual ## (16 observations deleted due to missingness) ## Null Deviance: 884.4 ## Residual Deviance: 103.3 AIC: 119.3 しかし，データに含まれるすべての変数を対象にしてstepwiseで最適なモデルを探して，出てきた結果を鵜呑みにするのは，あまり良い方法とは言えない。解析の前に研究者自身が仮説を立て，関連するであろう変数をモデルに入れる作業を繰り返してモデルを選択するのが仮説検証型の研究としてふさわしいメソッドである。 他にもモデル選択に使われる指標として，BIC（ベイズ情報量基準）などもある。 練習問題 宿題にはしない。 問１ 以下のプログラムを実行し，サンプルデータを作成する。 変数の意味は以下の通りである。 Disease: ある病気にかかっているか（1=かかっている，0=かかっていない） BMI: BMI（肥満度を表す指標） Exercise: 1週間あたりの運動時間（単位：時間） Sleep: 1日の睡眠時間（単位：時間） Disease = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) BMI = c(15, 16, 16, 18, 19, 20, 21, 22, 22, 23, 23, 23, 24, 24, 24, 30, 31, 31, 33, 34, 34, 34, 35, 36, 40, 40, 40, 41, 43, 43) Exercise = c(2, 1, 1, 2, 0, 3, 1, 1, 4, 4, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 0, 2, 2, 3, 0, 4) Sleep = c(7, 4, 5, 4, 4, 6, 5, 6, 4, 6, 4, 7, 4, 7, 4, 6, 5, 4, 5, 6, 7, 5, 4, 6, 4, 7, 5, 5, 4, 7) data_q01 &lt;- data.frame(Disease = Disease, BMI = BMI, Exercise = Exercise, Sleep = Sleep) 小問１ Diseaseを従属変数，BMI，Exercise，Sleepを独立変数としたロジスティック回帰を行い，結果について報告せよ。 ヒント: 重回帰のときと同じく，回帰係数とp値を報告すれば良い（b=x.xxx, p = .xxx）。有意な効果があった場合は，その係数の増減によって従属変数がどう影響するかを述べる。 小問２ BMIのみを独立変数とした場合(model 1)，BMIとExerciseを独立変数とした場合(model 2)，BMI, Exercise, 及びSleepを独立変数とした場合(model 3)について，いずれのモデルの方がデータに対する説明力が高いかを，それぞれのモデルのAICを報告した上で述べよ。 参考文献 久保拓弥 (2012) データ解析のための統計モデリング入門 岩波書店 嶋田正和・阿部真人 (2017) Rで学ぶ統計学入門 東京化学同人 "],
["11-glmm.html", "Chapter 13 一般化線形混合モデル 13.1 個人差や集団差の影響 13.2 一般化線形混合モデル 13.3 Rでの一般化線形混合モデル 13.4 一般化線型混合モデルの拡張（ロジスティック回帰） 練習問題 参考文献", " Chapter 13 一般化線形混合モデル 一般化線形混合モデルについて学ぶ。 個人差や集団差の影響 一般化線形混合モデルとは？ Rでの実践 install.packages(&quot;tidyverse&quot;) install.packages(&quot;lme4&quot;) install.packages(&quot;lmerTest&quot;) library(tidyverse) library(lme4) library(lmerTest) 一般化線形モデル（genelarized linear model: GLM）は，回帰分析やロジスティック回帰分析など，様々な統計分析を行うことのできる統計モデルであった。 一般化線形混合モデル（generalized linear mixed model: GLMM）は，一般化線形モデルを更に拡張させたものである。 13.1 個人差や集団差の影響 以下では，Rにデフォルトで入っている iris データを例として使う。irisデータには，3種類のあやめの種（Speicies: setosa, versicolor, virginica）ごとに，がくの長さ（Sepal.Length）やがくの幅（Sepal.Width）などのデータが入っている。 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa まず，がくの長さ（Sepal.Length）とがくの幅（Sepal.Width）の関係を散布図で示してみよう。 graph_1 = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point(size = 3) + theme_bw() graph_1 がくの幅を従属変数，がくの長さを独立変数とした回帰分析をしてみる。 iris_lm = lm(data = iris, Sepal.Width ~ Sepal.Length) summary(iris_lm) ## ## Call: ## lm(formula = Sepal.Width ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1095 -0.2454 -0.0167 0.2763 1.3338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.41895 0.25356 13.48 &lt;2e-16 *** ## Sepal.Length -0.06188 0.04297 -1.44 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4343 on 148 degrees of freedom ## Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 ## F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 がくの長さ（Sepal.Length）は，がくの幅に対して負の影響を持っていることがわかる。回帰式の直線を引くと，以下のようになる。 graph_lm = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point(size = 3) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() graph_lm では，先程の散布図を種（Species）ごとに色をわけて示してみる。 graph_2 = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species, shape = Species)) + geom_point(size = 3) + theme_bw() graph_2 種を無視して検討したところ，がくの長さとがくの幅の間には負の相関関係があるように思えたが，実際にはどの種でもがくの長さが大きくなるほど，がくの幅が大きくなる関係にあるように見える。 このあやめのデータのように，いくつかのデータが同じグループに属している構造の場合，グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には，統計的独立性が保証されていないためである。つまり，同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。 独立とは，各データが他のデータに影響されないという意味である。これまで学んできた確率分布では，統計的独立性が前提とされている。例えば，コインを数回投げて投げて表が出る回数は二項分布に従うと学んだが，表が出るかどうかは前の試行に影響されることはない（前回表が出たら次も表が出やすいということは起こりえないという前提を置く）。 しかし，上述の同じグループのデータは似ている傾向が強いなど，現実のデータでは独立性を前提とすることに無理がある場合がある。 この例に限らず，階層構造を持つデータや繰り返し測定データにも，同じことがいえる。学校ごとに学力テストを行った場合，同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合，その参加者のデータは似たような傾向を取る可能性にある。 個人差や集団差の影響を統制して独立変数が従属変数に持つ効果を検討するための統計モデルが，一般化線形混合モデルである。 一般化線形混合モデルは「階層モデル」と呼ばれることもある。 一般化線形混合モデルでは，独立変数が従属変数に及ぼす効果（固定効果：fixed effect）だけではなく，個人差や集団差を表すランダム効果（random effect）と呼ばれる影響を考慮する。一般化線形モデルは固定効果のみを含むモデルである。固定効果だけではなく，ランダム効果も混ぜたモデルなので，混合モデル(mixed model)と呼ばれる。 13.2 一般化線形混合モデル 繰り返し測定されたデータを扱う。具体例として，以下のようなデータを分析する場合をイメージしてほしい。\\(i\\)がデータを意味する番号（何行目か），\\(j\\)を個人もしくはグループを意味する番号とする。例えば，個人jが\\(x=0\\)の場合と\\(x=1\\)の場合の2回\\(y\\)を測定している，あるいは同じ集団\\(j\\)から2人選ばれて\\(y\\)が測定された，といったケースが当てはまる。 i j y x 1 1 -2.35 0 2 1 0.43 1 3 2 0.51 0 4 2 -0.57 1 5 3 -0.55 0 6 3 -0.56 1 一般化線形モデルの線形予測子（回帰式）は，以下のような数式で表現される。 \\[ y_{i} = \\beta_{0} + \\beta_{1} x_{i} \\] \\(\\beta_{0}\\)が切片，\\(\\beta_{1}\\)が独立変数\\(x\\)に係る傾きを意味する。この式から従属変数の値を予測する。 それに対し，一般化線形混合モデルの線形予測子は以下の式で表現できる。 \\[ y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\alpha_{j} \\\\ \\alpha_{j} \\sim Normal(0, \\sigma_{\\alpha}) \\] 線形予測子に，\\(\\alpha_{j}\\)が加わっている。 つまり，すべての個人に共通して影響する切片\\(\\beta_{0}a\\)と，個人ないしはグループごとに異なる値を取る切片\\(\\alpha_{j}\\)を考慮している。 更に，この切片は平均をゼロ，\\(\\sigma_{\\alpha}\\)を標準偏差とする正規分布から生成されるという仮定を置いている（多くの個人が平均して0の切片を持つが，中には0よりも大きい，ないしは小さい値を切片として持つ個人もいるという仮定）。 これにより，同じグループ（例えば\\(j=1\\)）には同じ効果（\\(\\alpha_{1}\\)）が共通して係ることを表現できる。 一般化線形混合モデルでは，すべてのデータに共通する効果（\\(\\beta_{0}\\)，\\(\\beta_{1}\\)）を固定効果（fixed effect），個体ごとに異なる効果（\\(\\alpha_{j}\\)）をランダム効果と呼ぶ。 ランダム効果は切片に限らない。例えば傾きを\\(\\beta_{1, j}\\)にする，すなわち個人ごとに独立変数に係る効果が異なるという前提を置く場合もある。 しかし，傾きもランダム効果として考慮したモデルの推定は，最尤推定法でも場合によっては解が求まらない場合がある。多くの場合，個人差の影響（ランダム効果）は切片のみを考慮したモデルで表現されることが多い。 しかし最近は，ベイズ統計手法によってランダム効果として傾きを考慮したモデルも扱われている（この講義で扱う内容を超えるので，詳細は省く）。 13.3 Rでの一般化線形混合モデル Rで一般化線形混合モデルで解析を行うためには，外部パッケージが必要になる。様々なパッケージがあるが，lme4パッケージが有名である（lmerTestも必要）。以下では，lme4パッケージに含まれるlmer()を使った解析の例を示す。 model_lmm = lmer(data= iris, Sepal.Length ~ Sepal.Width + (1|Species)) #(1|Species)を加える summary(model_lmm) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.4062 0.6683 3.4050 5.097 0.0107 * ## Sepal.Width 0.7972 0.1062 146.6648 7.506 5.45e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 出力結果を見てみると，Fixed effectsという部分がある。ここに，固定効果の結果が表示されている。見方は，一般化線形モデルのときと同じである。回帰式の切片(intercept)と独立変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず，すべての個体共通に係る独立変数の効果）。 ランダム効果の推定結果が，Random effectsという部分に表示されている。 上述のサンプルプログラムでは，回帰式に個体やグループを意味する変数を(1|個体やグループを意味する変数)というかたちで加えた。これは，lmer()及びglmer()特有の書き方で，「ランダム効果を切片として入れよ」という命令である。 ランダム効果の出力結果に，Std.Dev.が表示されている。これが，さきほどの式の\\(\\sigma_{\\alpha}\\)の推定結果を意味している。 がくの幅（Sepal.Width)の回帰係数（Estimate）を見ると，lm()での推定結果とは逆に，プラスになっている。やはり，グループの違いを統制すると，実際にはがくの幅が大きくなるほど，がくの長さも大きくなる関係にあることが，lmer()による推定結果からわかる。 lmer()では，デフォルトでは係数のp値は表示されない。p値もみたいのならば，lmerTest()パッケージをインストールしておく必要がある。 13.3.1 対応のある要因を含む分散分析 以下に，サンプルデータを作成する。 要因１（Factor_1）はAとBの２つの水準，要因２（Factor_2）はX，Y，Zの３つの水準をもつとする。IDは参加者を識別する番号とする。 ある実験で，参加者はAかBのどれかの群に割り振られ，その中でX, Y, Zの3つの実験条件に参加したとする。つまり，要因１は参加者間要因，要因２は参加者内要因として配置されたとする。 Score = c(6,4,5,3,2,10,8,10,8,9,11,12,12,10,10,5,4,2,2,2,7,6,5,4,3,12,8,5,6,4) Factor_1 = c(rep(&quot;A&quot;, 15), rep(&quot;B&quot;, 15)) Factor_2 = rep(c(rep(&quot;X&quot;, 5),rep(&quot;Y&quot;, 5), rep(&quot;Z&quot;, 5)),2) ID = c(1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,6,7,8,9,10,6,7,8,9,10) dat_anova = data.frame(Score = Score, Factor_1 = Factor_1, Factor_2 = Factor_2, ID = ID) dat_anova = dat_anova %&gt;% arrange(ID) dat_anova ## Score Factor_1 Factor_2 ID ## 1 6 A X 1 ## 2 10 A Y 1 ## 3 11 A Z 1 ## 4 4 A X 2 ## 5 8 A Y 2 ## 6 12 A Z 2 ## 7 5 A X 3 ## 8 10 A Y 3 ## 9 12 A Z 3 ## 10 3 A X 4 ## 11 8 A Y 4 ## 12 10 A Z 4 ## 13 2 A X 5 ## 14 9 A Y 5 ## 15 10 A Z 5 ## 16 5 B X 6 ## 17 7 B Y 6 ## 18 12 B Z 6 ## 19 4 B X 7 ## 20 6 B Y 7 ## 21 8 B Z 7 ## 22 2 B X 8 ## 23 5 B Y 8 ## 24 5 B Z 8 ## 25 2 B X 9 ## 26 4 B Y 9 ## 27 6 B Z 9 ## 28 2 B X 10 ## 29 3 B Y 10 ## 30 4 B Z 10 dat_anova %&gt;% group_by(Factor_1, Factor_2) %&gt;% summarise(Mean = mean(Score), SD = sd(Score), N = length(Score)) #平均値と標準偏差を出力 ## # A tibble: 6 x 5 ## # Groups: Factor_1 [2] ## Factor_1 Factor_2 Mean SD N ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A X 4 1.58 5 ## 2 A Y 9 1 5 ## 3 A Z 11 1 5 ## 4 B X 3 1.41 5 ## 5 B Y 5 1.58 5 ## 6 B Z 7 3.16 5 こういった参加者内要因と参加者間要因を含む複雑な要因配置の分析も，一般化線形混合モデルならば簡単にモデルを立てることができる。 すなわち，従属変数をScore，独立変数をFactor_1とFactor_2の主効果と交互作用効果，参加者をランダム効果とした一般化線形混合モデルを考える（誤差分布は正規分布，リンク関数は恒等リンク）。 result = lmer(data = dat_anova, Score ~ Factor_1 * Factor_2 + (1|ID)) #２つの変数を*でつなげると，主効果と交互作用効果の組み合わせを全て独立変数として投入してくれる。 summary(result) #線形混合モデルの結果。Factor_1及びFactor_2について，自動でダミー変数を作ってくれる。 ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Score ~ Factor_1 * Factor_2 + (1 | ID) ## Data: dat_anova ## ## REML criterion at convergence: 93.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.2667 -0.2333 -0.1333 0.1333 2.4000 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 2.167 1.472 ## Residual 1.000 1.000 ## Number of obs: 30, groups: ID, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 4.0000 0.7958 12.3948 5.026 0.000268 *** ## Factor_1B -1.0000 1.1255 12.3948 -0.889 0.391177 ## Factor_2Y 5.0000 0.6325 16.0000 7.906 6.47e-07 *** ## Factor_2Z 7.0000 0.6325 16.0000 11.068 6.58e-09 *** ## Factor_1B:Factor_2Y -3.0000 0.8944 16.0000 -3.354 0.004032 ** ## Factor_1B:Factor_2Z -3.0000 0.8944 16.0000 -3.354 0.004032 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Fct_1B Fct_2Y Fct_2Z F_1B:F_2Y ## Factor_1B -0.707 ## Factor_2Y -0.397 0.281 ## Factor_2Z -0.397 0.281 0.500 ## Fct_1B:F_2Y 0.281 -0.397 -0.707 -0.354 ## Fct_1B:F_2Z 0.281 -0.397 -0.354 -0.707 0.500 anova(result) #分散分析表を出力したいときは，anova()を使う。主効果と交互作用効果の検定結果が出る。 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Factor_1 9 9.0 1 8 9.0 0.017072 * ## Factor_2 155 77.5 2 16 77.5 5.875e-09 *** ## Factor_1:Factor_2 15 7.5 2 16 7.5 0.005036 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.4 一般化線型混合モデルの拡張（ロジスティック回帰） もちろん，回帰分析に限らず，ロジスティック回帰などでもランダム効果を含めることができる。 lme4のglmer()で，誤差分布として正規分布以外のものを指定した一般化線型混合モデルを行うことができる。以下では，ランダム効果を加えたロジスティック回帰分析の例を示す。 まず，サンプルデータを作る。 x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0) y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) ID = c(rep(&quot;a&quot;,5),rep(&quot;b&quot;,5),rep(&quot;c&quot;,5)) data_sample = data.frame(ID, x1, y1) data_sample ## ID x1 y1 ## 1 a 1.0 0 ## 2 a 2.0 0 ## 3 a 3.0 1 ## 4 a 4.2 1 ## 5 a 5.1 1 ## 6 b 3.1 0 ## 7 b 4.2 0 ## 8 b 5.0 0 ## 9 b 6.1 0 ## 10 b 7.0 1 ## 11 c 5.3 0 ## 12 c 6.0 1 ## 13 c 7.0 1 ## 14 c 8.1 1 ## 15 c 9.0 1 x1を独立変数（量的変数），y1を従属変数（0か1のいずれかを取る），IDが個体を示す変数とする。1つの個体からx1を変えて5回，y1が計測がされたデータをイメージしてほしい。 まずは，通常のロジスティック回帰分析の復習である。Rでは，glm()でロジスティック回帰分析を行うことができた。 オプションとして，誤差分布として二項分布（binomial）, リンク関数としてロジット（logit）を指定する。 model_logistic = glm(data = data_sample, y1 ~ x1, family = binomial(link=&quot;logit&quot;)) summary(model_logistic) ## ## Call: ## glm(formula = y1 ~ x1, family = binomial(link = &quot;logit&quot;), data = data_sample) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5695 -0.8681 0.3298 0.7439 1.7326 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.3155 1.9890 -1.667 0.0955 . ## x1 0.6889 0.3796 1.815 0.0696 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 20.728 on 14 degrees of freedom ## Residual deviance: 15.577 on 13 degrees of freedom ## AIC: 19.577 ## ## Number of Fisher Scoring iterations: 4 次に，個体差を統制する。lme4パッケージのglmer()を使う。 回帰式に個体を識別する変数（ID）を加える。以下のように，(1|ID)というかたちで入れる。あとのオプションの指定などは，glm()のときと同じである。 model_logistic_glmm = glmer(data = data_sample, y1 ~ x1 + (1|ID), family = binomial(link=&quot;logit&quot;)) summary(model_logistic_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: y1 ~ x1 + (1 | ID) ## Data: data_sample ## ## AIC BIC logLik deviance df.resid ## 14.3 16.4 -4.2 8.3 12 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.01928 0.00000 0.00000 0.00000 0.04031 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 67795 260.4 ## Number of obs: 15, groups: ID, 3 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -220.575 27.154 -8.123 4.54e-16 *** ## x1 38.996 4.804 8.117 4.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x1 -0.987 練習問題 問１ carパッケージに入っているカナダにおける職業の威信度に関する調査データPrestigeを使う。102業種に関する調査結果が入っている。 library(car) head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof prestigeを従属変数，education, income及び womenを独立変数，typeをランダム効果（切片）とした一般化線型混合モデルを行え。ただし，誤差分布として正規分布，リンク関数は恒等リンクを用いるものとする。 分析の結果，有意な効果を持った独立変数を挙げ，結論を述べよ（その独立変数が大きくなるほど，従属変数がどう変化するか）。 なお，変数の意味は以下の通りである。 prestige：職業威信度（値が高いほど威信度が高い） education：在職者の平均教育年数 income：平均所得（単位はドル） women：女性の割合 type：職業のカテゴリ（bc=ブルーカラー，wc=ホワイトカラー，prof=専門職） ヒント：ランダム効果を考慮した単なる回帰分析の場合は，lme4パッケージのlmer()を使えば良い。なお，p値を出力したい場合は，lmerTest()パッケージも必要になる。 問２ 以下の問題は，嶋田・阿部 (2017)「Rで学ぶ統計学入門」東京化学同人 p.177より抜粋。 以下のデータは，寄生蜂が生んだ子供の性比に関するデータである。motherが母蜂を意味する番号（1〜12。合計12匹），wtが宿主の体重，yが生まれた子供の性別（オス=1, メス=0）を意味する。12匹の母蜂が4匹ずつ子供を生んだ場合，宿主の大きさによってオス・メスの産み分けがされているのかを検討する。 mother = sort(rep(seq(1:12), 4)) wt = c(0.28, 0.31, 0.15, 0.36, 0.21, 0.17, 0.16, 0.41, 0.22, 0.45, 0.22, 0.33, 0.11, 0.24, 0.36, 0.32, 0.51, 0.19, 0.36, 0.28, 0.30, 0.42, 0.11, 0.56, 0.33, 0.25, 0.35, 0.15, 0.35, 0.42, 0.26, 0.45, 0.31, 0.49, 0.31, 0.43, 0.35, 0.27, 0.6, 0.29, 0.35, 0.39, 0.26, 0.15, 0.26, 0.27, 0.51, 0.36) y = c(1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0) data_q2 = data.frame(mother = mother, wt = wt, y = y) data_q2 ## mother wt y ## 1 1 0.28 1 ## 2 1 0.31 1 ## 3 1 0.15 1 ## 4 1 0.36 0 ## 5 2 0.21 1 ## 6 2 0.17 1 ## 7 2 0.16 1 ## 8 2 0.41 0 ## 9 3 0.22 1 ## 10 3 0.45 1 ## 11 3 0.22 1 ## 12 3 0.33 1 ## 13 4 0.11 1 ## 14 4 0.24 1 ## 15 4 0.36 0 ## 16 4 0.32 0 ## 17 5 0.51 0 ## 18 5 0.19 1 ## 19 5 0.36 0 ## 20 5 0.28 0 ## 21 6 0.30 1 ## 22 6 0.42 0 ## 23 6 0.11 1 ## 24 6 0.56 0 ## 25 7 0.33 1 ## 26 7 0.25 0 ## 27 7 0.35 0 ## 28 7 0.15 1 ## 29 8 0.35 0 ## 30 8 0.42 0 ## 31 8 0.26 0 ## 32 8 0.45 0 ## 33 9 0.31 0 ## 34 9 0.49 0 ## 35 9 0.31 0 ## 36 9 0.43 0 ## 37 10 0.35 0 ## 38 10 0.27 0 ## 39 10 0.60 0 ## 40 10 0.29 0 ## 41 11 0.35 1 ## 42 11 0.39 0 ## 43 11 0.26 1 ## 44 11 0.15 1 ## 45 12 0.26 0 ## 46 12 0.27 0 ## 47 12 0.51 0 ## 48 12 0.36 0 生まれた子供の性別（y）を従属変数，宿主の体重(wt)を独立変数，母蜂(mother)をランダム効果（切片）として，一般化線形混合モデルによる分析を行え。そして，寄生蜂が宿主の大きさによってオス・メスの産み分けをしているのかについて，結論を述べよ。 ヒント：ランダム効果を含むロジスティック回帰を行う。誤差分布とリンク関数として何を指定すべきか？yは0か1を取る値なので，二項分布に従うという前提を置く。 参考文献 久保拓弥 (2012). データ解析のための統計モデリング入門 岩波書店 粕谷英一 (2012). 一般化線型モデル 共立出版 嶋田正和・阿部真人(2017). Rで学ぶ統計学入門 東京化学同人 "],
["12-Appendix.html", "A tidyverseとは？ A.1 tidyverseパッケージのインストールとロード A.2 tibble A.3 readr A.4 readxl A.5 dplyr A.6 purrr A.7 ggplot2 A.8 tidyr B 使い方まとめ B.1 tidyverseパッケージのインストールとロード B.2 データの読み込み B.3 結果の集計 B.4 グラフの作成 B.5 新しい変数を作る B.6 データを保存する B.7 もっと知りたい人は C 第10章「一般化線形モデル」の付録 C.1 １．指数と対数 C.2 ２．ロジスティック関数とロジット関数の関係 C.3 ３．ロジスティック回帰の係数の意味 C.4 ４．ポアソン回帰で対数をリンク関数とする理由", " A tidyverseとは？ tidyverseは様々なパッケージを含んだ，パッケージのセットである。Rを使いやすくするための便利なパッケージがまとめて入っている。 データ分析の前に，予めtidyverseをロードしておけば，データの扱いで困ることがない。 ここでは，tidyverseの中に含まれるパッケージについて，よく使うものについて解説をする。 ※この章の内容をすべて教えると授業時間が足りない。tidyverseに収録されているパッケージを学ぶには1 semesterは必要（ここで紹介しているのもほんの一部である！）。ここでは，この先の授業で使うであろうものに絞って紹介する。最後に練習問題ももうけている。 A.1 tidyverseパッケージのインストールとロード install.packages(&quot;tidyverse&quot;) library(tidyverse) 以降のプログラムで関数は「XXXX::YYYY」と表現されているが，「XXXXパッケージに入っているYYYYという関数を使う」という意味である。XXXX::の部分は，基本的に省略しても問題ない。 tidyverse以外のパッケージも読み込んでいる場合，同じ名前の関数を含むパッケージがあるとエラーが生じてしまう。仮にエラーが生じたときは，XXXX::を付けてどのパッケージの関数を使いたいのかを指定しよう。 A.2 tibble tibbleとは，データフレームに代わるものとして開発された，Rの新たなデータ形式である。 これまでRで分析をする際には，データをデータフレーム（data.frame）という形式が使われてきた。 #データフレーム x = c(1, 2, 3) y = c(4, 5, 6) z = data.frame(x, y) z ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 as_tibble()でデータをtibble形式にできる。 z_tibble = as_tibble(z) z_tibble ## # A tibble: 3 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows tibbleは，data.frameよりも可読性を向上させているのが特徴である。コンソールにはデータ全てではなく，最初の10行程度のみ，列も画面に入る範囲のみが表示される。 データをすべてを閲覧したいときは，View()を使おう。 View(dat) 自分でtibble型のデータを作ることも可能。 dat = tibble::tibble(x = c(1, 2, 3), c(4, 5, 6)) dat ## # A tibble: 3 x 2 ## x `c(4, 5, 6)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 次に紹介するreadrパッケージのread_csv関数などでは，読み込んだファイルをデフォルトでtibble型にしてくれる。 A.3 readr 様々な形式のファイルを高速で読み書きことを目標としたパッケージ。 dat = readr::read_csv(&quot;0_sample.csv&quot;) ## Parsed with column specification: ## cols( ## X = col_double(), ## Y = col_double(), ## Gender = col_character() ## ) dat ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F readr::write_excel_csv(dat, &quot;0_sample_2.csv&quot;) read_csv()ならば，csvのファイルをtibble形式で読み込んでくれる。 * read.csv()ではなく，read_csv()なので注意（ドットではなく，アンダースコア）。 ファイルを書き出すための関数も用意されている。 * ここでは，write_excel_csv() を使っている。単にwrite_csv()でも可。 A.4 readxl エクセル形式（xlsx）のファイルを読み込むことができる。 ※ただし，データはできるだけxlsxよりもcsvで保存しておくことを推奨する。 dat = readxl::read_excel(&quot;0_sample.xlsx&quot;) #特にオプションを指定しなければ，1番目に保存されているシートの中身をtibble形式で読み込んでくれる。シートや読み込む範囲を指定したい場合は，ヘルプを参照。 dat A.5 dplyr データを操作するのに特化した関数が用意されている。 * mutate(): 変数の追加 * filter(): 行の抽出 * select(): 変数の抽出 * summarise(): 複数の変数を1つにまとめる * arrange(): データを並べ替える * group_by(): グループにまとめる dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows dat2 = dplyr::mutate(dat, z = Sepal.Length * 2) #新しい変数zを追加する dat2 ## # A tibble: 150 x 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 10.2 ## 2 4.9 3 1.4 0.2 setosa 9.8 ## 3 4.7 3.2 1.3 0.2 setosa 9.4 ## 4 4.6 3.1 1.5 0.2 setosa 9.2 ## 5 5 3.6 1.4 0.2 setosa 10 ## 6 5.4 3.9 1.7 0.4 setosa 10.8 ## 7 4.6 3.4 1.4 0.3 setosa 9.2 ## 8 5 3.4 1.5 0.2 setosa 10 ## 9 4.4 2.9 1.4 0.2 setosa 8.8 ## 10 4.9 3.1 1.5 0.1 setosa 9.8 ## # … with 140 more rows dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) #Speciesのうち，versicolorのみを取り出す。「イコール」は=ではなく，==にするのに注意（計算式と条件式ではイコールの表現が異なる）。 dat2 ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor ## 7 6.3 3.3 4.7 1.6 versicolor ## 8 4.9 2.4 3.3 1 versicolor ## 9 6.6 2.9 4.6 1.3 versicolor ## 10 5.2 2.7 3.9 1.4 versicolor ## # … with 40 more rows dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) #データから，Sepal.LengthとPetal.Lengthの列を取り出す。 dat2 ## # A tibble: 150 x 2 ## Sepal.Length Petal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5 1.4 ## 6 5.4 1.7 ## 7 4.6 1.4 ## 8 5 1.5 ## 9 4.4 1.4 ## 10 4.9 1.5 ## # … with 140 more rows dat2 = dplyr::summarise(dat, M = mean(Sepal.Length), SD = sd(Sepal.Length)) #Sepal.Lengthの平均を求め，Mという変数で保存する。同じく，SDという変数を新たに作る。 dat2 ## # A tibble: 1 x 2 ## M SD ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 0.828 dat2 = dplyr::arrange(dat, Sepal.Length) #Sepal.Lengthを小さい順に並び替える。 dat2 ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 4.3 3 1.1 0.1 setosa ## 2 4.4 2.9 1.4 0.2 setosa ## 3 4.4 3 1.3 0.2 setosa ## 4 4.4 3.2 1.3 0.2 setosa ## 5 4.5 2.3 1.3 0.3 setosa ## 6 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 4.6 3.6 1 0.2 setosa ## 9 4.6 3.2 1.4 0.2 setosa ## 10 4.7 3.2 1.3 0.2 setosa ## # … with 140 more rows dat2 = dplyr::group_by(dat, Species) #Speciesをグループとしてまとめる。新しくGroupsというものが作られている。（使い方については後述） dat2 ## # A tibble: 150 x 5 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows A.6 purrr パイプ処理が使える。パイプ（%&gt;%）で関数をつなげることで，1行のプログラムで複数の処理をすることができる。 ※これはとても便利なので覚えよう。例えば，練習問題に載せている「グループ別に平均などを算出する」などをするには，パイプ処理を知らないと難しい。 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows dat2 = dat %&gt;% dplyr::filter(Species == &quot;setosa&quot;) %&gt;% dplyr::select(Sepal.Width, Sepal.Length) %&gt;% dplyr::arrange(Sepal.Width) #Speciesがsetosaだけの行を抽出し，更にSepal.WidthとSepal.Lengthの列を取り出し，Sepal.Widthの小さい順に並び替える。これを1行で実行 dat2 ## # A tibble: 50 x 2 ## Sepal.Width Sepal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.3 4.5 ## 2 2.9 4.4 ## 3 3 4.9 ## 4 3 4.8 ## 5 3 4.3 ## 6 3 5 ## 7 3 4.4 ## 8 3 4.8 ## 9 3.1 4.6 ## 10 3.1 4.9 ## # … with 40 more rows dat2 = dat %&gt;% dplyr::group_by(Species) %&gt;% dplyr::summarise(M = mean(Sepal.Width), SD = sd(Sepal.Width), N = length(Sepal.Width)) #Speciesでグループにし，グループごとにSepal.Widthの平均，標準偏差，N数を出力する。 dat2 ## # A tibble: 3 x 4 ## Species M SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 A.7 ggplot2 ※ggplot2の使い方はかなり「玄人」向けである。一度で理解しきれなくても，普通である。Excelでグラフを作るよりもはるかに便利では有るので，今はすべて理解しきれなくても，卒論やゼミでグラフを作る必要に駆られたときにトライしてみてほしい。 グラフの作成に特化したパッケージである。Rにも標準でグラフィック関数が用意されているが，あまりきれいではないし，グラフの編集もやりにくい。 p = plot(iris$Sepal.Length, iris$Sepal.Width) #Rで標準で入っているplot関数で，散布図などを作ることができる p ## NULL ggplot2ならば，見やすいグラフを出力してくれる。また，軸の名前など，ユーザーが事由に図をカスタマイズできる。 p = ggplot2::ggplot() p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length, shape = Species)) p = p + labs(x = &quot;Length of Sepal&quot;, y = &quot;Length of Petal&quot;) p A.7.1 ggplot2の使い方の基本 基本は，1) geom_xxxxで出力したいグラフの種類を指定し，2) データの名前を指定し，3) aes()にx軸とy軸に指定したい変数を入力する。 p = ggplot2::ggplot() #←まず，ggplot()関数を実行する p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length, shape = Species)) #geom_xxxxで，グラフの種類を指定する。data=でデータの名前，aes()の中に，x軸の変数，y軸の変数を入力する。他にも，形(shape)，色（fill）などで区別したい変数があれば指定する。 p A.7.1.1 グラフの種類 geom_xxxxでグラフの種類を指定することができる。以下に，作ることができるグラフを紹介する。 A.7.1.1.1 散布図 geom_pointで作成可能。 p = ggplot2::ggplot() p = p + geom_point(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 重なって見えにくい場合は，geom_jitterを使うとランダムのズレつくって表示してくれる。 p = ggplot2::ggplot() p = p + geom_jitter(data=mpg, aes(x=cty, y=hwy, shape = fl)) p A.7.1.1.2 棒グラフ geom_barで作成可能。 なお，エラーバーを表示できるgeom_errorbarもある。 #サンプルデータをつくる: あやめの種ごとのがくの幅の平均 iris_mean = data.frame( Sepecies = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), Mean_Width = c(3.43, 2.77, 2.97), SE_Width = c(0.0536, 0.0444, 0.456) ) iris_mean ## Sepecies Mean_Width SE_Width ## 1 setosa 3.43 0.0536 ## 2 versicolor 2.77 0.0444 ## 3 virginica 2.97 0.4560 p = ggplot() p = p + geom_bar(data = iris_mean, aes(x=Sepecies, y=Mean_Width), stat = &quot;identity&quot;) #stat=&quot;identity&quot;は，”データの集計などは必要なく数値どおりの値を出力してください”という命令 p = p + geom_errorbar(data = iris_mean, aes(x= Sepecies, ymax = Mean_Width + SE_Width, ymin = Mean_Width - SE_Width)) p 棒グラフはデータの範囲について，かなりの情報を削ぎ落としてしまう。なので，棒グラフではなく箱ひげ図を使うほうが良い。 A.7.1.1.3 ヒストグラム geom_histogramで作成可能。x軸の変数のみを指定する。 p = ggplot() p = p + geom_histogram(data = cars, aes(x=dist)) p A.7.1.1.4 箱ひげ図 geom_boxplotで作成可能。y軸に分布を示したい変数を入れる。x軸にグループを入れれば，グループごとに分布の違いを比較することができる。 最小値，第一分位点，中央値，第三分位点，最大値を示す（外れ値は点で示される）。 p = ggplot() p = p + geom_boxplot(data = InsectSprays, aes(x=spray, y=count)) p A.7.1.1.5 ヴァイオリンプロット 分布を示すグラフとして，ヴァイオリンプロットと呼ばれるグラフがある。 geom_violineで作成可能。 p = ggplot() p = p + geom_violin(data = InsectSprays, aes(x=spray, y=count)) p A.7.1.1.6 折れ線グラフ geom_lineで作成可能。時系列などの変化を見るときには適切。 #サンプルデータをつくる: 10日間の気温の変化 temperature &lt;- data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) p = ggplot() p = p + geom_line(data = temperature, aes(x=Days, y=Celsius)) p A.7.1.2 オプション A.7.1.2.1 ラベル labs()で，グラフの軸のタイトルなどを変えることができる。 p = ggplot2::ggplot() p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length, shape = Species)) p = p + labs(x = &quot;title of x axis&quot;, y = &quot;title of y axis&quot;, title = &quot;title of the plot&quot;) p A.7.1.2.2 ファセット facet_grid()で，グループごとにプロットを分けることができる。 p = ggplot2::ggplot() p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length)) p = p + facet_grid(cols = vars(Species)) p p = ggplot2::ggplot() p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length)) p = p + facet_grid(rows = vars(Species)) p A.7.1.2.3 テーマ デフォルトのテーマはプロットの背景が灰色だが，テーマを変えることができる。theme_bw()がおすすめ。 p = ggplot2::ggplot() p = p + geom_point(data=iris, aes(x=Sepal.Length, y=Petal.Length, shape = Species)) p p = p + theme_bw() #balack and white p = p + theme_classic() #classic A.7.1.3 もっと簡単な方法 gglot2パッケージのqplot()でも手っ取り早くグラフを作成することが出来る（quick plotの略）。とりあえずデータの傾向を見たいだけなら，qplot()を使うのが良い。x軸もしくはy軸に指定したい変数を入れればできるので，これくらいは覚えておくと良い。 x軸とy軸に表示する変数を2つ入れれば散布図，x軸の変数1つだけならヒストグラムがデフォルトで表示される。geomでグラフの種類を指定することも可能。 ggplot2::qplot(data = iris, x = Sepal.Length, y = Petal.Length) ggplot2::qplot(data = ChickWeight, x = weight) ggplot2::qplot(data = InsectSprays, x = spray, y = count, geom=&quot;boxplot&quot;) A.7.1.4 グラフの保存 ggsave()を使う。 ggplot2::ggsave(plot = p, filename = &quot;hoge.png&quot;, dpi = 300) A.8 tidyr tidyrパッケージに入っているgather()とspread()を使うと，データのレイアウトを変えることができる。 A.8.1 wide型とlong型の区別 まず，データのレイアウトには，wide型とlong型の二種類があることを理解しよう。 以下のデータを例として説明する。A, B, Cの3人の参加者が，X, Y, Z条件の３つの条件で実験課題を行ったとする。 それぞれの条件での課題の成績（数値），参加者の性別，年齢をデータとして入力する。 dat_wide = tibble(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_wide ## # A tibble: 3 x 6 ## Subject X Y Z Gender Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 このように，１行につき１人の参加者の情報を入力するやり方が一つある（例えばExcelでデータを入力する際も，恐らくこのレイアウトの方が入力しやすいだろう）。このようなデータのレイアウトをwide型という。 同じデータでも，以下のようなレイアウトで表現する場合もある。 dat_long = tibble(Subject = sort(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 3)), Condition = rep(c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;), 3), Score = c(6,9,7,2,3,5,7,4,7), Gender = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;, &quot;F&quot;), Age = sort(rep(c(18,19,20), 3))) #サンプルデータを作る（ここでは手入力でlong型のデータに変換することの面倒さを知ってもらうために，あえて複雑なプログラムでサンプルデータを作っている。プログラムの意味を理解する必要はない。） dat_long ## # A tibble: 9 x 5 ## Subject Condition Score Gender Age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 実験成績ごとに１行ずつでデータが作られている。すなわち，同じ参加者１人につき３行のデータがある。このようなデータの方をlong型と呼ぶ。 A.8.2 どのデータ型にすべきか？ この講義で使う分析では，原則としてデータはlong型で入力されたものを使う。 Rで用いる関数の多くは，long型でデータが入っていることを想定としているためである。 A.8.3 データレイアウトの変換 ※以下のプログラムを理解するのははじめのうちは難しい。必要になったときにヘルプなども参照し，試行錯誤をしながら適切なデータ変換を試みてほしい。 wide型をlong型に変換するには，tidyrパッケージのgather()を使う。 dat_wide #wide型のデータ ## # A tibble: 3 x 6 ## Subject X Y Z Gender Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 dat_long2 = dat_wide %&gt;% tidyr::gather(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, key = Condition, value = Score) #並び替える変数を指定する。そして，key変数（ここでは条件），value変数（ここでは成績）の名前を指定する。 dat_long2 ## # A tibble: 9 x 5 ## Subject Gender Age Condition Score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A M 18 X 6 ## 2 B F 19 X 2 ## 3 C F 20 X 7 ## 4 A M 18 Y 9 ## 5 B F 19 Y 3 ## 6 C F 20 Y 4 ## 7 A M 18 Z 7 ## 8 B F 19 Z 5 ## 9 C F 20 Z 7 逆に，long型をwide型に変換するには，tidyrパッケージのspread()を使う。 dat_long #wide型のデータ ## # A tibble: 9 x 5 ## Subject Condition Score Gender Age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 dat_wide2 = dat_long %&gt;% tidyr::spread(key = Condition, value = Score) #key変数（ここでは条件），value変数（ここでは成績）とする変数を指定する。 dat_long2 ## # A tibble: 9 x 5 ## Subject Gender Age Condition Score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A M 18 X 6 ## 2 B F 19 X 2 ## 3 C F 20 X 7 ## 4 A M 18 Y 9 ## 5 B F 19 Y 3 ## 6 C F 20 Y 4 ## 7 A M 18 Z 7 ## 8 B F 19 Z 5 ## 9 C F 20 Z 7 B 使い方まとめ 「心理学データ解析法」で使う手順に沿って，よく使うものだけ確認しよう。 B.1 tidyverseパッケージのインストールとロード 帝京大学のコンピュータルームのマシンは電源をいれるたびに設定がリセットされてしまうので，毎回はじめにこれを行う。 install.packages(&quot;tidyverse&quot;) library(tidyverse) B.2 データの読み込み read_csv()でcsvファイルを読み込む。tibble形式でデータが表示される。 dat = read_csv(&quot;0_sample.csv&quot;) dat ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F 今日の課題は以下だったとする。 性別（Gender）ごとに，変数X及びYの平均値と標準偏差を計算する。 変数XとYの散布図を出す。 新しい変数Z=2X+Yを作って，データに付け足す。 新しい変数を加えたデータを名前をつけて保存する。 B.3 結果の集計 group_byでグループにごとに出したい変数（i.e., Gender）を指定し， summariseでまとめる。 パイプ(%&gt;%)を使えば1つのプログラムで実行可能。 dat %&gt;% group_by(Gender) %&gt;% summarise(Mean_X = mean(X), SD_X = sd(X), Mean_Y = mean(Y), SD_Y = sd(Y)) ## # A tibble: 2 x 5 ## Gender Mean_X SD_X Mean_Y SD_Y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 5.5 2.12 3.5 0.707 ## 2 M 4.67 2.08 5 1.73 B.4 グラフの作成 ggplot2パッケージを使う。qplot()で十分。 qplot(data=dat, x=X, y=Y) B.5 新しい変数を作る mutate()を使う。 dat2 = dat %&gt;% mutate(Z=X+2*Y) dat2 ## # A tibble: 5 x 4 ## X Y Gender Z ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 6 M 16 ## 2 7 3 F 13 ## 3 7 6 M 19 ## 4 3 3 M 9 ## 5 4 4 F 12 #以下でも可（昔ながらの表記） dat2 = dat dat2$Z = dat2$X + 2*dat2$Y dat2 ## # A tibble: 5 x 4 ## X Y Gender Z ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 6 M 16 ## 2 7 3 F 13 ## 3 7 6 M 19 ## 4 3 3 M 9 ## 5 4 4 F 12 B.6 データを保存する write_excel_csv()を使う。 write_excel_csv(dat2, &quot;0_sample_2.csv&quot;) B.7 もっと知りたい人は tidyverseの公式ページに，チートシートが公開されている。 C 第10章「一般化線形モデル」の付録 以下の説明は，高校数学の知識があればわかる内容となっている（はず）。この付録を読まなくても，ロジスティック回帰の理解に差し障りはない（はず）。 C.1 １．指数と対数 指数と対数は，以下の関係にある。 \\[ y = e^x \\\\ \\log_{e}y = x \\\\ \\] \\(e\\)とはネイピア数と呼ばれるもので（自然対数の底とも呼ばれる），\\(e=\\) 2.7182818の一定の値を取る（円周率\\(\\pi\\)と同様に，数学において扱われる重要な定数である）。 一般的に，ネイピア数は指数関数の底（\\(f(x)=a^x\\)の\\(a\\)に当たる部分）の底としてよく用いられる。 なお，\\(e^x\\)は，\\(\\exp(x)\\)とも表記する。以降の記述でも，\\(\\exp(x)\\)の方を使う。 また，\\(e\\)を底とする対数（自然対数）は，\\(\\log_{e}y\\)は単に\\(\\log y\\)と底を省略して表記される。 すなわち，まとめるとネイピア数を底とする指数及び対数の関係は以下のようになる。 \\[ y = \\exp(x) \\\\ \\log y = x \\\\ \\] Rでは\\(\\exp(x)\\)の値を求める関数exp()が用意されている。また，\\(\\log x\\)もlog()関数で求めることができる。 exp(1) #x=1のとき，つまりネイピア数eが出力される ## [1] 2.718282 exp(-1) #xが負の値のとき ## [1] 0.3678794 exp(-1000) #xが負の値のとき ## [1] 0 対数と指数は逆関数の関係にある。 log(exp(100)) ## [1] 100 exp(log(100)) ## [1] 100 C.1.1 指数関数 以下が，\\(y=e^x\\)のグラフである。 x = seq(-3, 3, 0.01) y = exp(x) exp_graph = data.frame(y=y, x=x) ggplot(data = exp_graph, aes(x=x, y=y)) + geom_line() このように，指数関数は，xが負の値をとってもyは0に漸近するが0にはならないという性質がある。 C.1.2 対数関数 以下が，\\(y=\\log x\\)のグラフである。 x = seq(0, 3, 0.01) y = log(x) log_graph = data.frame(y=y, x=x) ggplot(data = log_graph, aes(x=x, y=y)) + geom_line() C.2 ２．ロジスティック関数とロジット関数の関係 以下のロジスティック関数（式(3)）は， \\[ q_{i} = \\frac{1}{1+\\exp[-(\\beta_{0} + \\beta_{1}x_{i})]} \\tag{3} \\] ロジット関数（式(4)）にも変形できる。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] なぜか。式(3)から式(4)へ整理してみる（\\(y_{i} = \\beta_{0} + \\beta_{1} x_{i}\\)とする）。 \\[ q_{i} = \\frac{1}{1+\\exp(-y_{i})} \\\\ q_{i}(1+\\exp(-y_{i})) =1 \\\\ q_{i} \\biggl\\{ 1+\\frac{1}{\\exp(y_{i})} \\biggl\\} = 1 \\\\ q_{i}+\\frac{1}{\\exp(y_{i})}q_{i} = 1 \\\\ q_{i}\\exp(y_{i})+q_{i} = \\exp(y_{i}) \\\\ q_{i} = \\exp(y_{i}) - q_{i}\\exp(y_{i}) \\\\ q_{i} = \\exp(y_{i})(1 - q_{i}) \\\\ \\frac{q_{i}}{1-q_{i}} = \\exp(y_{i})\\\\ \\log \\frac{q_{i}}{1-q_{i}} = y_{i}\\\\ \\] このように，ロジット関数はロジスティック関数へ変形することが可能。逆に，ロジスティック関数からロジット関数へ変形することも可能。ロジット関数はロジスティック関数の逆関数であり，ロジスティック関数はロジット関数の逆関数である。 C.3 ３．ロジスティック回帰の係数の意味 ロジスティック回帰で推定した回帰式で求める予測値yそのものは，確率を意味しない。 例えば，本文の例題で求めた係数をもとに，独立変数V1=1を代入してみると， y = -5.16017 + 0.93546 * 1 y ## [1] -4.22471 マイナスの値である。推定したいのは確率なのに，0から1の範囲に収まらない。 なぜならば，線形予測子から推定されるのは本文の式(4)で示したように，正確には\\(\\log q_{i}/(1-q_{i})\\)だからである。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] \\(q_{i}/(1-q_{i})\\)はオッズ(odds)と呼ばれるもので，ある事象が生じる確率と生じない確率の比を意味する（\\(q_{i} = 0.5\\)ならばオッズは1，\\(q_{i} = 0.9\\)ならばオッズは9になる）。 線形予測子から推定されるのは，オッズの対数である（対数オッズ）。 また，式(4)は，以下の式(4’)に変形できる。 \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(\\beta_{0} + \\beta_{1} x_{i}) \\tag{4&#39;}\\\\ \\] \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(\\beta_{0}) \\exp(\\beta_{1} x_{i}) \\\\ \\] 例題で推定された切片及び傾きのパラメータを入れると， \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94 x_{i}) \\\\ \\] 独立変数\\(x_{i}\\)が一単位増えた場合， \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94 (x_{i}+1)) \\\\ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94x_{i})\\exp(0.94) \\\\ \\] つまり，オッズ\\(q_{i}/(1-q_{i})\\)は，\\(\\exp(0.94)\\)倍になる。\\(\\exp(0.94)\\)は2.56であるので，2.56倍である。 exp(0.94) ## [1] 2.559981 すなわち，ロジスティック回帰の係数（の指数）が意味することは，1単位増えたときにオッズが何倍になるかである（オッズ比という。「XXXをすると，病気のリスクがしないときよりもX倍になる」というイメージ）。 係数が正だとオッズ比は1を超える値を取り，負だと0以上1未満の値を取る。 exp(1) ## [1] 2.718282 exp(-1) ## [1] 0.3678794 線形予測子から，確率\\(q_{i}\\)を直接求めるならば，式(3)を使えば良い。 1/(1+exp(-1*y)) ## [1] 0.01441864 C.4 ４．ポアソン回帰で対数をリンク関数とする理由 ポアソン分布を誤差分布とするとき，リンク関数は対数(log)を用いた。 \\(\\lambda\\)はポアソン分布の平均値を意味する。 \\[ \\log\\lambda = \\beta_{0} + \\beta_{1} x \\\\ \\] これを変形すると， \\[ \\exp(\\log \\lambda) = \\exp(\\beta_{0} + \\beta_{1} x) \\\\ \\lambda = \\exp(\\beta_{0} + \\beta_{1} x) \\\\ \\] となる。 線形予測子が負の値であっても，exp(線形予測子)は正の値を取る（指数関数の式を参照）。ポアソン分布のパラメータ\\(\\lambda\\)は\\(\\lambda&gt;0\\)でなければならないが，対数をリンク関数とすることでその前提が満たされる。 "]
]
