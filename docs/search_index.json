[["index.html", "心理データ応用解析法 Chapter 1 はじめに 1.1 テキストの構成", " 心理データ応用解析法 帝京大学文学部心理学科 堀田結孝 2025-09-28 Chapter 1 はじめに このテキストでは、Rを使いながら心理学における応用的なデータ解析を学んでいく。 心理学を専攻していて基礎統計学を学修済みの学生を対象としている。また、Rを全く使ったことがない人も想定して、Rのインストールの仕方から基本的な使い方も解説する。 このテキストでは、t検定や分散分析など心理統計で学んだ手法を統計モデルという一つの枠組みで包括的に理解することを目的とする。具体的には線形モデル、一般化線形モデル、マルチレベルモデルを順に学んでいき、様々な種類のデータに対して柔軟に解析を行う技術を身に着けていく。 1.1 テキストの構成 このテキストでは、R及びRStudioを用いてデータ解析の練習をしていく。 第2章から第5章にかけては、Rの使い方について扱っている。 第2章ではRのインストール、Rでのプログラムの書き方、パッケージのインストールの仕方など、Rを使ってデータ解析をする上で基本的なことについて、初めてRを使う人向けに解説している。第3章では、平均値や標準偏差、相関などの基礎統計量をRで計算する方法について解説する。第4章と第5章では、Rでデータを加工する方法やグラフの作成などについて解説する。 第6章以降で、データ解析について学んでいく。 まずは、心理統計でも学んだ内容の復習も交え、統計モデルを用いる上で必要な知識を順に学んでいく。第6章では、確率分布について学ぶ。第7章では、統計的仮説検定とそれが抱える問題を学ぶ。第8章では、t検定、χ二乗検定など、心理統計で学んだ解析をRで行う方法について解説する。 第9章からは、統計モデルについて学んでいく。第9章では線形モデルを学ぶ。この章では、心理統計における回帰分析の復習を通して、t検定、分散分析などの解析法が線形モデルという一つの枠組みで扱えることを学んでいく。第10章では、線形モデルで解析を行う上での注意点について触れる。 第11章と第12章では、線形モデルの拡張である一般化線形モデルを学ぶ。質的変数など、正規分布に従わないデータを解析する手法を学んでいく。第11章では代表的な解析法としてロジスティック回帰とポアソン回帰について、第12章ではその他の一般化線形モデルについて学ぶ。一般化線形モデルを用いることで、様々な種類のデータに対して柔軟な解析が可能となることを理解していく。 第13章では、更に一般化線形モデルを拡張させたマルチレベルモデルを学ぶ。同じ集団や個人から得られたデータなど、個人差や集団差を含むデータを解析する手法について学ぶ。 第14章と第15章にかけては、ベイズ統計など、より応用的な解析について学ぶ。 1.1.1 テキストの読み方 各章の最後に、確認問題を設ける。 "],["01-intro.html", "Chapter 2 Rの使い方 2.1 Rのインストール 2.2 RStudioのインストール 2.3 プログラムの書き方 2.4 パッケージ 2.5 ヘルプ 2.6 Rを終わらせる", " Chapter 2 Rの使い方 はじめてRを使う人向けに、下準備について解説する。 R 及び RStudio のインストール （このテキストの手順でうまくいかない場合は、「R インストール」などでWeb検索してみよう） パッケージ Rの終わらせ方 2.1 Rのインストール インストールは、R-ProjectのWebページから可能。自分の OS にあったインストーラを選ぶ。 インストーラを実行したら、あとは指示に従ってインストールをすすめる。 2.2 RStudioのインストール RStudio とは、R の使いやすさを向上させる目的で開発されているアプリケーションである。R をインストールしたら、RStudio もインストールしておくこともすすめる。このテキストでも、RStudio を使って解析することを前提に説明する。 インストールは、RStudioのWebページからできる。 R と Rstudio の両方をインストールできたら、RStudio の方を開く。 以下のような画面が表示される。 注意: ここまでの手順で「R もしくは RStudio をインストールできない」あるいは「R 及び RStudio はインストールできたが、起動できない」という人は、以下の可能性を考えてみてほしい。 OS が Windows の場合、管理者権限のあるアカウントで R 及び RStudio をインストールする必要がある。インストールする際は、管理者権限として実行しよう。 同じく OS が Windows の場合、アカウント名にマルチバイト文字（全角文字）を含んでいると R が正常に起動しない。例えば、マシンにログインする時の名前を「ほげ」など全角文字（日本語）にしてしまっていると、うまくいかない。この場合は面倒ではあるが、「既にあるアカウントの名前を半角英数に変更する（hoge など）」、あるいは「もう一つ別の半角のアカウントを作る」といった方法で対処してみよう。 R に限らず、ファイルやフォルダ名に全角文字が含まれていると障害になる場合がある。ファイル名やフォルダ名には、なるべく日本語（全角文字）は使わない習慣を身に着けよう。 2.3 プログラムの書き方 RStudioの画面構成について確認する。 2.3.1 コンソール コンソール（Console）という部分にプログラムを入力すると、結果が出力される。 ためしに、コンソールの&gt;の部分に、以下のプログラムを入力して、Enter を押してみよう。 このテキストでは以下のように、背景が灰色の箇所にプログラムとその出力結果（行頭に##が付いている部分）が示されている。 1 + 1 ## [1] 2 同じコンソールに、答えである2が出力されたはずである。 このように、コンソールに直接プログラムを入力すると、結果を返してくれる。 2.3.2 スクリプト コンソールに入力したプログラムや出力結果は、R を閉じると消えてしまう。これでは復習できないので、プログラムは別のファイルに残しておいた方が良い。 プログラムを書き込んだテキストのことを「スクリプト (Script)」と呼ぶ。プログラムはなるべく、スクリプトに残しておく習慣をつけよう。 「File」から「New Script」を選ぶ。何も書かれていないファイル（R Editor）が開かれる。 名前をつけて保存する。「File」から「Save as..」を選び、名前をつけて保存する。拡張子が「.R」のファイルとして保存される。 スクリプトに、試しに以下のプログラムを入力してみよう。 1 + 1 プログラムを選択し、Ctrl と Enter を同時に押して実行する（「Run line or selection」を選んでも可）。すると、「R Console」にプログラムの結果が出力される。 スクリプトファイルを開きたいときは、RStudio を立ち上げて、「File」から「Open File」を選び、スクリプトのファイルを選ぶ。 初心者が戸惑いやすい点について説明する。 ためしに、コンソールに以下のプログラムを入力してEnterを押してみよう。 1 + 何も表示されないし、冒頭が&gt;ではなく+が表示される。Enter を押しても元に戻らない。 プログラムが不完全なことが原因である。1 +と中途半端な状態で入力したので、Rはプログラムの続きがあるものと思って入力を待っている状態にある。プログラムの続きを入力すれば、結果が出力される。例えばこの例ならば、1を入力してEnterを押せば、答えである2が出力される。 他にもカッコの閉じ忘れなどでも、同じようなことが生じる。 なお、Esc（エスケープ）キーを押せば、プログラムを中止することができる。困ったときには、Escキーを押そう。 他にも、エラーが生じた場合は、エラーメッセージを読んで、プログラムの書き方に間違いがないかを確認しよう。たいてい、入力間違いなど大したことのないミスが原因である。ちょっとプログラムを間違えたくらいでRが壊れるということは決してないので、冷静に対処しよう。 2.4 パッケージ パッケージとは、R の機能を拡張するためにインターネットからインストールして使うものである。 2.4.1 パッケージのインストール パッケージをインストールする。install.packages()で、インストールしたいパッケージを入力する。 ここではggplot2というパッケージをインストールするのを例として、パッケージのインストール方法について示す。 install.packages(&quot;ggplot2&quot;) もし「Please select a CRAN mirror …」というのが表示されたら、Japan (Tokyo)を選んで「OK」を押す。 パッケージのインストールは、RStudioの右下の「Packages」というタブからも行うことができる。「Install」を選択して、インストールしたいパッケージ名を入力して実行する。 パッケージは世界中で開発され、アップデートもなされている。RStudioならば同じく「Packages」の「Update」を選ぶことでアップデートすることができる。 2.4.2 パッケージのロード 単にインストールしただけではパッケージを使うことができない。使う前にロードする必要がある。library()で、括弧内に使いたいパッケージ名を入力する。 インストールのときとは違って、クオテーションマーク(““)でパッケージ名を囲む必要はない。 library(ggplot2) 一度インストールしておけば、今後は最初にlibrary()でロードするだけで使うことができる。毎回インストールし直す必要はない。 2.5 ヘルプ R の関数やパッケージなど、使い方がわからない場合はhelp()でヘルプを参照することができる（英語）。RStudio ならば、画面右下の「Help」にヘルプが出力される。ヘルプにはプログラムの例も記されている。 help(mean) ?mean #?でもヘルプを表示させることができる。 2.6 Rを終わらせる そのまま閉じてよい。 「Save workspace image?（作業スペースを保存しますか？）」が表示されるが、「いいえ」で良い。 "],["02-data.html", "Chapter 3 データ 3.1 はじめに（パッケージのロード） 3.2 プログラミングの練習 3.3 変数 3.4 変数の型 3.5 データ構造 3.6 欠損値 3.7 データの読み込み 3.8 データの書き出し 3.9 サンプルデータ 3.10 補足", " Chapter 3 データ Rでデータ分析を行う前に、Rでのデータの使い方を理解する。 プログラミングの練習 変数 Rのデータ構造（ベクトル、データフレーム、tibble） 欠損値 外部データの読み込み（csv, xlsx, その他SPSSやStataのファイルなど） データの書き出し サンプルデータ 3.1 はじめに（パッケージのロード） この章ではtibble,readr,havenといったパッケージを使うので、あらかじめロードをしておく（未インストールの場合はインストールする必要あり。パッケージのインストール方法については第2章を参照のこと）。 library(tibble) library(readr) library(haven) 注意： 以降のプログラムでは、パッケージに入っている関数を呼び出す際に「XXXX::YYYY」というかたちでプログラムを書いている。 readr::read_csv(&quot;data.csv&quot;) これらは「XXXXパッケージに入っているYYYYという名前の関数を使う」ということを意味している。XXXX::の部分は基本的に省略しても問題ないが、例えば複数のパッケージをロードしていて、同じ名前の関数が別のパッケージに含まれている場合には、思った通りの結果が表示されない場合もある。外部パッケージの関数を使う場合は、できる限りXXXX::を付けてどのパッケージの関数を使うのかを明示して使うのが良い。 3.2 プログラミングの練習 ここでは、Rで使える識別子(operator)の解説を通して、プログラミングの練習を行う。 3.2.1 コメント文 文頭に#を挿入すると、#から改行まではコメント文として理解され、プログラムが実行されない。スクリプトにメモを残しておきたいときに便利である。 #1 + 1 #この部分は実行されない 3.2.2 算術識別子 +、-、*、/で、四則演算（足し算・引き算・割り算・掛け算）を行える。 1 + 1 #足し算 ## [1] 2 1 - 1 #引き算 ## [1] 0 2 * 3 #掛け算 ## [1] 6 10 / 2 #割り算 ## [1] 5 他にも演算用の識別子として、^、%%、%/%がある。それぞれ、累乗、割り算の余り、割り算の整数部分の計算を結果を返す。 2 ^ 3 #2の3乗 5 %% 3 #5/3の余り 5 %/% 3 #5/3の整数部分 カッコ()を使うと、カッコ内の演算が優先される。 (1 + 3)/2 ## [1] 2 3.2.3 比較識別子 数値の大小関係などを扱うときに用いる。その式が成り立っていればTRUE、成り立っていなければFALSEが出力される。 2 == 1 #2 と 1 は同じか？ ## [1] FALSE 2 != 1 #2 と 1 は同じではないか？ ## [1] TRUE 2 &lt; 1 #2 は 1 よりも小さいか？ ## [1] FALSE 2 &lt;= 1 #2 は 1 以下か？ ## [1] FALSE 2 &gt; 1 #2 は 1 より大きいか？ ## [1] TRUE 2 &gt;= 1 #2 は 1 以上か？ ## [1] TRUE 3.3 変数 数値を変数に代入して扱うことができる。 x = 5 + 8 x ## [1] 13 y = x - 2 y ## [1] 11 =の代わりに&lt;-を使っても良い。 x &lt;- 5 + 8 x ## [1] 13 変数の使い方の注意 Rは小文字と大文字を区別する。たとえば、a（小文字）と入力して実行すると結果が出力されるが、A（大文字）では出力されない。 a = 2 #aに2を代入する。 a - 2 #ゼロが出力されるはず。 A - 2 #Aでは答えが表示されない。Aという変数は作られていないので。 また、数値を全角で入力していないかにも注意すること。全角文字は数値ではなく、文字として認識される。数値は常に半角で入力すること。 x = 2 #半角の2 x = ２ #全角の２ 3.4 変数の型 R では変数の種類として、数値型、文字列、日付、論理型の区別をする。 a = 1 b = &quot;1&quot; c = as.Date(&quot;2020-06-15&quot;) d = TRUE #class()でその変数の型を確認することができる class(a) class(b) class(c) class(d) 3.4.1 数値型(numeric) 数値型として格納した変数は、数値として扱うことができる。数値型の変数同士で、演算（足し算・引き算・掛け算・割り算）を行うことができる。 x = 5 y = 1.2 x + y #数値型同士は演算することができる ## [1] 6.2 数値型には、整数型(integer)と浮動小数点型(double)の区別もある（データ分析においては意識して区別することはあまりない）。 x = 1.5 as.integer(x) #整数型に変換 ## [1] 1 as.double(x) #浮動小数点型に変換 ## [1] 1.5 3.4.2 文字列(character) 文字として扱われる。文字列同士は演算をすることができない。 文字を変数として代入したい場合は、文字をクオテーションマーク\"\"で囲む。 x = &quot;hello&quot; x ## [1] &quot;hello&quot; y = &quot;1&quot; #数値でもクオテーションマークで囲むと文字として扱われる y ## [1] &quot;1&quot; 同じ文字でも、因子型(factor)というものもある。因子型には順序情報を付与することができる。グラフで軸がカテゴリの場合で順序を並び替えたいときや、順序変数を用いた分析の際に活用することがある。 x = c(&quot;Good&quot;, &quot;Very Bad&quot;, &quot;Moderate&quot;, &quot;Very Good&quot;, &quot;Bad&quot;) factor(x, ordered = TRUE, levels = c(&quot;Very Bad&quot;, &quot;Bad&quot;, &quot;Moderate&quot;, &quot;Good&quot;, &quot;Very Good&quot;)) ## [1] Good Very Bad Moderate Very Good Bad ## Levels: Very Bad &lt; Bad &lt; Moderate &lt; Good &lt; Very Good 3.4.3 日付(date) Date型は日付のみを保存し、POSIXct型は日付と時間を保存する。 日付型同士で日数や秒数などの演算をすることができる。 x = as.Date(&quot;2020-06-15&quot;) x_1 = as.POSIXct(&quot;2020-06-14 12:00&quot;) x_2 = as.POSIXct(&quot;2020-06-15 12:00&quot;) x_2- x_1 ## Time difference of 1 days 3.4.4 論理型(logical) TRUEかFALSEの2つの値のどちらかを取る変数の型である。比較や条件式を扱う際に関わってくる。 x = TRUE y = FALSE x ## [1] TRUE y ## [1] FALSE 3.5 データ構造 複数の数値や文字列などをまとめた構造を、データと呼ぶ。R には、データを扱うための形式がいくつか用意されている。 3.5.1 ベクトル 同じ型の要素を集めたものであり、最も単純なデータ型である。c()関数で、ベクトルを作成することができる。 x = c(1, 2, 3, 4, 5) #数値の並びのベクトル x ## [1] 1 2 3 4 5 y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) #文字の並びのベクトル y ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; ベクトル[x]の表記でカッコの中に数値を入れると、そのベクトルの x 番目の要素を取り出せる。 x[2] #ベクトルxの2番目の要素 ## [1] 2 y[3] #ベクトルyの3番目の要素 ## [1] &quot;c&quot; カッコの中に条件式を入れると、その条件に当てはまる要素を取り出せる。 x[x &gt;= 3] #3以上の要素を取り出す ## [1] 3 4 5 y[y == &quot;c&quot;] #3cを取り出す ## [1] &quot;c&quot; y[y != &quot;c&quot;] #3c以外を取り出す ## [1] &quot;a&quot; &quot;b&quot; &quot;d&quot; &quot;e&quot; ベクトルが数値で構成されている場合は、演算をすることもできる。 x * 2 #ベクトル内の全ての要素に2を掛ける ## [1] 2 4 6 8 10 x_2 = c(6, 7, 8, 9, 10) x + x_2 #（ベクトルに格納されている変数の数が同じならば、ベクトル同士で演算ができる） ## [1] 7 9 11 13 15 3.5.2 データフレーム 複数のベクトルを行列でまとめたデータ構造を、R ではデータフレームと呼ぶ。データフレームは頻繁に使うので、構造を覚えよう。 2つのベクトルを作成し、この2つのベクトルからなるデータフレームを作成する。data.frame()は、データフレームを作るための関数である。 x_vec = c(1, 2, 3, 4, 5) y_vec = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) dat = data.frame(x = x_vec, y = y_vec) dat ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e データフレームでの変数の使い方 以下のように、データフレーム$変数名で、データフレームの変数をベクトルとして取り出すができる。 dat$x ## [1] 1 2 3 4 5 データフレームに新たに変数を加えることも出来る。 dat$x_2 = c(6, 7, 8, 9, 10) dat ## x y x_2 ## 1 1 a 6 ## 2 2 b 7 ## 3 3 c 8 ## 4 4 d 9 ## 5 5 e 10 dat$x_3 = dat$x + dat$x_2 dat ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 データフレーム$変数名でデータ内の変数にアクセスする方法は、今後もよく使うので覚えておこう。 データの抽出 データフレーム[行数,列数]のかたちで指定することで、データフレームの行列を取り出すことができる。 dat[1, 2] #1行目, 2列目に該当する部分を抽出 ## [1] &quot;a&quot; dat[1,] #1行目を抽出（列を指定しなければ，データフレームの全ての列が抽出される） ## x y x_2 x_3 ## 1 1 a 6 7 dat[,1] #1列目を抽出（行を指定しなければ，データフレームの全ての行が抽出される） ## [1] 1 2 3 4 5 dat[1:3,] #1行目から3行目を抽出 ## x y x_2 x_3 ## 1 1 a 6 7 ## 2 2 b 7 9 ## 3 3 c 8 11 dat[c(1,3,5),] #1行目, 3行目, 5行目を抽出 ## x y x_2 x_3 ## 1 1 a 6 7 ## 3 3 c 8 11 ## 5 5 e 10 15 カッコ内に条件式を入れると、その条件と一致する部分を取り出せる。 dat[dat$x &gt; 2,] #xが2を超える行を抽出 ## x y x_2 x_3 ## 3 3 c 8 11 ## 4 4 d 9 13 ## 5 5 e 10 15 dat[dat$x &gt; 2 &amp; dat$y != &quot;c&quot;,] #xが2を超え，かつyがc以外の行を抽出 ## x y x_2 x_3 ## 4 4 d 9 13 ## 5 5 e 10 15 3.5.3 tibble 更に、Rにはデータフレームの可読性を向上させたtibbleというデータ形式が用意されている。tibble形式のデータを扱うには、tibbleパッケージが必要となる。 as_tibble()でデータフレームをtibble形式にすることができる。Rにあらかじめ入っているサンプルデータirisを試しにtibble型にしてみよう。 dat_tibble = tibble::as_tibble(iris) dat_tibble ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows tibble形式のデータは、コンソールにデータ全てではなく最初の10行程度のみが表示される。列も画面に入る範囲のみが表示される。データの行数と列数、データの変数の型なども表示される。 tibbleだとデータをすべて閲覧することはできないが、すべて閲覧したい場合はView()を使えばよい。別ウィンドウが開いて、データを閲覧することができる。 View(dat_tibble) 3.6 欠損値 Rでは、欠損値（データが空の部分）はNAで扱う。 先程の例で作ったデータフレームdatに、欠損値を含む変数x_4を入れてみよう。 dat$x_4 &lt;- c(1, 2, NA, 4, 5) dat ## x y x_2 x_3 x_4 ## 1 1 a 6 7 1 ## 2 2 b 7 9 2 ## 3 3 c 8 11 NA ## 4 4 d 9 13 4 ## 5 5 e 10 15 5 dat$x + dat$x_4 ## [1] 2 4 NA 8 10 欠損値を含むベクトルは、計算に用いることができない。例えば、R には平均値を計算するためのmean()という関数がある。しかし、欠損値を含むベクトルの場合は結果が出力されない。 mean(dat$x_4) ## [1] NA 関数によっては、欠損値を含むデータを使うときには欠損値の処理を指定する必要がある。例えば、mean()ならば、オプションとしてna.rm =TRUEを入れると欠損値を除いた上で平均値を計算してくれる。 mean(dat$x_4, na.rm = TRUE) ## [1] 3 3.7 データの読み込み 上述の例では自分でプログラムを書いてデータフレーム等を作成したが、大抵の場合はデータを ファイルなどに保存したデータを読み込んで使うことが多い。以下では、RでCSVやExcelファイルなどを読み込む方法を確認していく。 3.7.1 ワーキングディレクトリの設定 データを読み込む前に、ワーキングディレクトリ (Working directory)について理解する必要がある。ワーキングディレクトリとは、「現在居る場所」のことである。ファイルを読み込む際には、Rにデータがどこに有るかを教える必要がある。Rはワーキングディレクトリを起点にして、読み込むファイルを探す。 試しに、現在のワーキングディレクトリを確認しよう。以下のプログラムをコンソールに入力して実行する。 getwd() 出力された場所が、現在のワーキングディレクトリである。 読み込みたいファイルをデスクトップに保存してある場合を例として理解していこう。すなわち、デスクトップをワーキングディレクトリとして指定する必要がある。 ワーキングディレクトリをデスクトップに変更するには、以下のプログラムを書き込んで実行する。 #Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 #Macの場合 setwd(&quot;~/Desktop&quot;) #正しく設定されたかを確認する getwd() 3.7.2 外部ファイルの読み込み csvファイルの読み込み read.csv()関数で読み込むことが出来る。 dat = read.csv(&quot;data.csv&quot;) #ファイル名をクオテーションで囲んで入れる。ここでは読み込んだデータを「dat」という名前で保存した。 また、readrパッケージのread_csv()関数でもcsvファイルを読み込むことができる。更に、読み込まれたデータはtibble型になる。 read.csv()ではなく、read_csv()なので注意（ドットではなく、アンダースコア）。 library(readr) dat = readr::read_csv(&quot;data.csv&quot;) Excelファイルの読み込み readxlパッケージのread_excel()で、xlsx形式のデータも読み込むことができる。 library(readxl) dat = readxl::read_excel(&quot;data.xlsx&quot;) 特にオプションを指定しなければ、1番目に保存されているシートの中身をtibble形式で読み込んでくれる。読み込みたいシートや読み込む範囲を指定したい場合など、細かい点についてはread_excelのヘルプを参照のこと。 その他の形式 havenパッケージにある関数で、SPSS, Stata, SASなどの統計アプリケーションのファイルも読み込むことができる。 library(haven) dat = haven::read_dta(&quot;data.dta&quot;) dat = haven::read_sav(&quot;data.sav&quot;) dat = haven::read_sas(&quot;data.sas7bdat&quot;) 相対パス 上記の例では、デスクトップ上に読み込みたいファイルを保存し、デスクトップをワーキングディレクトリに指定してデータを読み込んだ。しかし、例えばデスクトップにあるフォルダの中にデータを保存してあってそのファイルを読み込みたい場合、いちいちワーキングディレクトリを設定し直すのは面倒である。 このような場合、相対パスでファイルを指定するのが便利である。 #デスクトップをワーキングディレクトリに指定する ##Windowsの場合 setwd(&quot;C:/Users/ユーザー名/Desktop&quot;) #ユーザー名には設定しているアカウント名を入れる。 ##Macの場合 setwd(&quot;~/Desktop&quot;) #デスクトップにあるDataフォルダの中の「0_sample.csv」を読み込む dat = read.csv(&quot;./Data/0_sample.csv&quot;) .（ピリオド）は、ワーキングディレクトリを意味する。/（スラッシュ）でフォルダの階層を区切ることで、下の層のフォルダにアクセスすることができる。 3.8 データの書き出し 作成したデータを書き出すこともできる。以下に、readrパッケージのwrite_csv()を使ってCSVファイルを書き出す例を示す。 readr::write_excel_csv(dat, &quot;./Data/data_2.csv&quot;) #保存したいデータとパス名を指定する。 3.9 サンプルデータ R には予めサンプルデータがいくつか用意されている。このテキストでもところどころで、R に入っているサンプルデータを使って解析の練習を行う。 iris #有名なフィッシャーのあやめデータ cars #自動車の速度と停止距離との関係 data() #data()で、入っているデータを確認できる 3.10 補足 3.10.1 RStudioの機能でファイルを読み込む 自分でプログラムを書かなくとも、RStudio機能でファイルの読み込みもできる。RStudioの右上のウィンドウにある「Import Dataset」を選ぶ。 読み込むファイルの種類を選ぶ。CSVファイルならば「From Text (base)」を選び、読み込む CSV ファイルを選ぶ。 Name には任意のデータ名を入力する。ファイルの1行目に変数名を入力している場合は Heading は「Yes」を選ぶ。設定ができたら、「Import」を選ぶ。 3.10.2 RStudioの機能でワーキングディレクトリを指定する 直接自分でプログラムを書かなくとも、RStudioの機能でワーキングディレクトリの変更をすることができる。RStudioの右下のウィンドウにある「File」タブにデスクトップを表示し、「Set As Working Directory」を選ぶ。 他にも、メニューバーの「Session」から「Set Working Directory」、「Choose Working Directory」で指定する方法もある。 "],["03-summary.html", "Chapter 4 統計学の基礎の復習 4.1 尺度水準 4.2 基本統計量 4.3 相関(correlation) 4.4 確認問題", " Chapter 4 統計学の基礎の復習 平均値の計算など、心理統計の基礎を復習する。また、R で平均値などの基礎統計量を計算する方法についても解説する。 変数の区別 基本統計 代表値（平均、中央値） 散布度（分散、標準偏差、分位数） 共分散、相関 4.1 尺度水準 まず、統計学における変数の種類の区別について確認する。変数の種類は大きく分けて、数値である量的変数とカテゴリーを意味するカテゴリカル変数（質的変数）の2つに区別できる。 4.1.1 量的変数 数値として扱う変数。計算することができる。量的変数は更に、間隔尺度と比率尺度に区別される。 間隔尺度 データの間隔に意味があるもの。ゼロが何もない状態を意味するものでないもの。例えば、摂氏温度など（0℃以下も-1℃があるように、ゼロは何もない状態を意味しない）。差には意味があるが、比率については意味を持たない。例えば、「10℃と20℃の差は10℃である」とはいえるが、「20℃は10℃の2倍の熱さである」とは言えない。 比率尺度 データの間隔に意味があるもの。ゼロがなにもない状態を意味するもの。例えば、身長、体重、絶対温度など。間隔を比率で表現できる。例えば、「体重100キロの人は体重50キロの人より2倍重い」といえる。 また、量的変数は離散値か連続値かでも区別できる。 離散値 離散値とは、小数の間隔を持たない数値のこと。例えば個数。1個, 2個、3個と数えるが、1.1個, 1.2個などは存在しない。 連続値 連続値とは、小数の間隔を持つ数値のこと。例えば身長。150cmから151cmの間には小数で表現できる数値が連続的に並んでいる。 4.1.2 カテゴリカル変数（質的変数） 分類や種類などを意味するデータ。数量化して計算することはできない。 カテゴリカル変数も更に、いくつかに分類される。ここでは、名義尺度と順序尺度の区別を挙げる。 名義尺度 性別（男、女）、血液型、出身地など。順序関係がないのが特徴である（男性&lt;女性といった関係はない）。 順序尺度 「優、良、可、不可」といった成績、「1. 賛成、2. どちらでもない、3. 反対」といった尺度など。心理学ではよくリッカート尺度（＊）で態度などを測定するが、これも順序尺度である。 ＊「1:そう思わない、2: 思わない、3: どちらでもない, 4: そう思う、5: 強くそう思う」といったように、自分の態度に当てはまる数字を選ばせて態度の強さを測定する方法。 順序尺度には順序関係があるが、間隔は定義されない。例えば、成績には優 &gt; 良 &gt; 可という順序関係があるが、優と良の間と良と可の間の幅は等しいといった定義はできない。また、良+可=優といった計算もできない。つまり、順序尺度は順序関係はあるがカテゴリカル変数である。 4.2 基本統計量 以下では、Rにもともと入っているirisデータをサンプルデータとして使いながら、平均値や中央値などの代表値や分散や標準偏差などの散布度について復習する。 irisと入力して実行すると、データの中身を確認できるが、これだとデータ全てが出力されて見やすくない。head()を使うとデータの変数を含めた上部数行を表示してくれる。また、str()を使うと、簡略化したデータの構成を表示してくれる。これらの関数は今後もよく出てくるのでここで覚えておこう。 head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 4.2.1 代表値 データの代表となる値のこと。平均や中央値などがよく用いられる。 平均値(mean) \\(n\\)個の数値\\(X_{1}, X_{2}, \\cdots, X_{n}\\)からなるデータの平均値\\(\\bar{X}\\)は、以下の数式で求める。 \\[ \\bar{X} = \\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\\\ \\] Rで平均値を求めるには、mean()関数を使う。カッコの中に、平均値を求めたいデータをベクトルの形式で入れる。 データフレームの場合は、「データ名$変数名」でベクトルを指定する。これはよく使う表記なので覚えておこう。 mean(iris$Sepal.Length) ## [1] 5.843333 データに欠損値が含まれている場合は、計算できない。オプションとしてna.rm = TRUEを指定すれば、欠損値を除いて平均値を算出してくれる。 a = c(1, 2, 3, NA, 4, 5) mean(a) ## [1] NA mean(a, na.rm = TRUE) ## [1] 3 中央値(median) データを小さい順から並べた場合、つまり、\\(X_{1} \\le X_{2} \\le ... \\le X_{n}\\)と並べた場合に、中央に位置する値を中央値という。 データの個数を\\(n\\)とした場合、\\(n\\)が奇数の場合は\\(X_{(n+1)/2}\\)、\\(n\\)が偶数の場合は\\((X_{n/2}+X_{n/2+1})/2\\)が中央値となる。 Rで中央値を求めたい場合は、median()関数が使える。mean()と同じく、カッコ内にデータをベクトルの形式で入れる。 median(iris$Sepal.Length) ## [1] 5.8 4.2.2 散布度 データの散らばり具合を示す値のこと。分散、標準偏差などが知られる。 分散(variance) 分散（\\(\\sigma^2\\)）は、以下の式で定義される。すなわち、各変数が平均値から離れている程度を表現したものである。 \\[ \\sigma^2 = \\frac{1}{n-1}\\sum_{k=1}^{n}(X_{k}-\\bar{X})^2\\\\ \\] Rで分散を求める際には、var()関数を使う。 var(iris$Sepal.Length) ## [1] 0.6856935 標準偏差(standard deviation) 分散の平方根が標準偏差（\\(\\sigma\\)）である。 Rで標準偏差を求める際には、sd()関数を使う。 sd(iris$Sepal.Length) ## [1] 0.8280661 分位数(quantile) データを小さい順から大きい順に並べ替えたときに、データを分割する値を分位数という。一般的に、四分位数（25%点、50%点[中央値]、75%点で分割した値）が報告によく使われる。 Rでは、quantile()で分位数を求められる。 quantile(iris$Sepal.Length) #デフォルトだと、0%, 25%, 50%, 75%, 100%が表示される） ## 0% 25% 50% 75% 100% ## 4.3 5.1 5.8 6.4 7.9 quantile(iris$Sepal.Length, probs = c(0.1, 0.3, 0.5, 0.8, 1.0)) #オプションのprobsで、分割する点を任意に指定することができる。 ## 10% 30% 50% 80% 100% ## 4.80 5.27 5.80 6.52 7.90 4.2.3 要約統計量 以上のように、R には代表値や散布度を求めるための関数が標準で入っているが、これらをまとめて計算してくれるsummary()もある。 summary()に変数を入れると、最小値、最大値、平均、四分位数をまとめて算出してくれる。 summary(iris$Sepal.Length) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 4.2.4 グループごとの集計 グループごとに統計量を集計する場合は、tapply関数を使う。tapply()に、最初に統計量を求めたい変数のベクトル、グループを意味するベクトル、求めたい統計量を入れる。 例えば、サンプルデータのirisには、種を意味するSpeciesが入っている。種ごとに平均や標準偏差などを求める場合には、以下のようにプログラムを書く。 tapply(iris$Sepal.Length, iris$Species, mean) #平均 ## setosa versicolor virginica ## 5.006 5.936 6.588 tapply(iris$Sepal.Length, iris$Species, median) #中央値 ## setosa versicolor virginica ## 5.0 5.9 6.5 tapply(iris$Sepal.Length, iris$Species, sd) #標準偏差 ## setosa versicolor virginica ## 0.3524897 0.5161711 0.6358796 tapply(iris$Sepal.Length, iris$Species, length) #サンプル数 ## setosa versicolor virginica ## 50 50 50 4.2.5 表の作り方 カテゴリカル変数の場合、データ全体を把握するために頻度やパーセンテージを知りたいときが多い。 table()関数を使うと、頻度を集計して表にしてくれる。 table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 prop.table()を使うと、パーセンテージを求めてくれる。 tab_iris = table(iris$Species) #まず表を別の名前で保存する prop.table(tab_iris) #カッコの中に、表を入れる ## ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 4.3 相関(correlation) 2変数間の関連のことを相関という。相関の強さは相関係数で示される。 変数\\(x\\)と変数\\(y\\)の相関係数（\\(r\\)）は、以下の式で求められる。 \\[ r = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}} \\] \\(\\sigma_{xy}\\)はxとyの共分散(covariance)で、\\(\\sigma_{xy} = \\sum^n_{i=1}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\)である（\\(\\bar{x}\\)はxの平均）。\\(\\sigma_{x}\\)と\\(\\sigma_{y}\\)はxとyそれぞれの標準偏差である。 相関係数は\\(-1\\)から\\(1\\)までの範囲をとり得る。相関係数の絶対値が大きいほど、相関関係が強いことを意味する。相関係数\\(r\\)が \\(r &gt; 0\\)のときは「正の相関」、つまり一方の変数の量が増えればもう一方の変数も増える関係にあることを意味する。\\(r &lt; 0\\)のときは「負の相関」、つまり一方が増えればもう一方が減るという関係にあることを意味する。 Rで相関係数を求めるには、cor()が使える。カッコの中に、相関係数を求めたい2つの変数を入れれば良い。また、cor.test()を使うと、相関係数の検定などより詳細な結果を示してくれる。 cor(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.1175698 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4403, df = 148, p-value = 0.1519 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269325 0.04351158 ## sample estimates: ## cor ## -0.1175698 なお、共分散はcov()で求められる。 cov(iris$Sepal.Length, iris$Sepal.Width) ## [1] -0.042434 補足: 単に「相関係数」というと、一般的には「ピアソンの積率相関係数」のことを指す場合が多い。上で示した例も、ピアソンの積率相関係数を求めている。以降の章でも、特に断りがない限り「相関係数」は積率相関係数を指すものとする。 相関係数にもいくつか種類があり、例えばスピアマンの順位相関係数がある。順位相関係数は、変数を順位に変換した上で（大きい順から1, 2, …と順位を振る）その順位を使って相関係数を求めたものである。データに極端な値（外れ値）がある場合やデータ数が少ない場合は、外れ値を調整してくれる順位相関係数の方が望ましい。 cor()及びcor.test()のオプションで、method =で相関係数の算出法を指定することができる。 cor(iris$Sepal.Length, iris$Sepal.Width, method = &quot;spearman&quot;) ## [1] -0.1667777 何も指定しなければ、デフォルトでピアソンの積率相関係数を計算してくれる。 4.4 確認問題 irisデータを使って、代表値、散布度、相関係数の求め方を復習する。 問１ Petal.LengthとPetal.Widthそれぞれの平均値と標準偏差を求めよう。 問２ Petal.LengthとPetal.Widthの共分散を求めよう。 ヒント：cov()で2つの変数を入れると求まる。 問３ 問1で求めた標準偏差と、問2で求めた共分散を使って、2変数の間の相関係数を求めよう。 また、cor()でもPetal.LengthとPetal.Widthの相関係数を求め、値が一致することを確かめよう。 ヒント：相関係数の式を確認して、相関係数を求める。 "],["04-data.html", "Chapter 5 データ・ハンドリング 5.1 パッケージのロード 5.2 変数の作成 5.3 データの抽出 5.4 パイプ 5.5 グルーピング 5.6 データの変換 5.7 データの結合 5.8 確認問題", " Chapter 5 データ・ハンドリング データに新しく変数を加えたり、データの形式を変えるなど、より高度で複雑なデータの操作について学んでいく。 この章では、tidyverseというパッケージに入っている関数を解説する。 変数の作成 データの抽出 パイプ グルーピング データの変換 データの結合 データの読み込み 5.1 パッケージのロード まず、この章で使うパッケージのロードをする（初めて使う場合は、マシンに予めパッケージをインストールする必要がある）。パッケージのインストール及びロードについては、第2章で解説している。 library(tidyverse) 注意： 以降のプログラムでは、関数を「XXXX::YYYY」と表現しているが、これらは「XXXXパッケージに入っているYYYYという名前の関数を使う」ということを意味している。XXXX::の部分は基本的に省略しても問題ないが、例えばtidyverseパッケージ以外もロードしていて、同じ名前の関数が別のパッケージに含まれている場合には、思った通りの結果が表示されない場合もある。外部パッケージの関数を使う場合は、できる限りXXXX::を付けた方が無難である。 以降では、Rに標準で入っているirisデータを例として、ファイル操作の練習を行う。以下のプログラムを実行し、irisデータをdatという名前に置き換えて使っていこう。 dat = iris 5.2 変数の作成 dplyrパッケージに入っているmutate()を使うと、新たに変数を追加することができる。 mutate()に、データの名前、新しい変数の順番で入力すると、データの右端に新しい変数を追加してくれる。 dat2 = dplyr::mutate(dat, new_var = Sepal.Length + Petal.Length, hoge = ifelse(Species == &quot;setosa&quot;, 1, 0)) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species new_var hoge ## 1 5.1 3.5 1.4 0.2 setosa 6.5 1 ## 2 4.9 3.0 1.4 0.2 setosa 6.3 1 ## 3 4.7 3.2 1.3 0.2 setosa 6.0 1 ## 4 4.6 3.1 1.5 0.2 setosa 6.1 1 ## 5 5.0 3.6 1.4 0.2 setosa 6.4 1 ## 6 5.4 3.9 1.7 0.4 setosa 7.1 1 上の例では、Sepal.LengthとPetal.Lengthを足したnew_varという名前の新しい変数を作っている。更に、「Sepeciesが”setosa“ならば1, そうでなければ0とする」という条件で新たにhogeという変数を作っている。 5.3 データの抽出 dplyrパッケージに入っているselectや filter関数を使うと、データの中から必要な部分のみを取り出すことができる。 5.3.1 必要な列のみを取り出す（select） select()で、データの名前、取り出したい変数名（複数選択可）の順番で入力すると、指定した変数の列のみを取り出してくれる。 以下には、irisデータからSepal.LengthとPetal.Lengthのみを取り出す場合のプログラム例を示す。 dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) head(dat2) ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 上の例では、データの中からSepal.LengthとPetal.Lengthの列を取り出している。 5.3.2 条件に合う行を取り出す（filter） ある条件に合う行のみを取り出したい場合（例えばデータの中から男性のみを取り出したいなど）、filter()で、データの名前、条件式の順番で入力すると、データの中から条件に合う行のみを取り出してくれる。 以下には、irisデータから、あやめの種類（\"Species\"）のうち\"versicolor\"のみを取り出す場合のプログラム例を示す。 dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) head(dat2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4.0 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor データから条件に合う行だけが取り出される。上の例では、「Speciesがversicolorである」行をdatから取り出している。 「イコール」は=ではなく、==と表記していることに注意。つまり、計算式と論理式ではイコールの表現の仕方が異なる。他の論理式の表現については、第2章で説明しているので確認しておこう。例えば、「Sepal.Lengthが7以上」という条件で取り出したいときは、dplyr::filter(iris, Sepal.Length &gt;= 7)とする。 5.4 パイプ 複数のプログラムをつなげることをパイプ処理という。 例えば、irisデータで「あやめの種類のうち\"setosa\"のみの行を取り出して、更にSpecies、Sepal.Length, Petal.Lengthのみの列を取り出したい」という複数の処理をする場合を例として考える。 先程まで学んだ内容で、以下のように複数のプラグラムを段階的に書けばできなくはないが、プログラムが非常に長くなる（プログラムを分けて書くと途中でミスも生じやすくなる）。 dat1 = dplyr::filter(iris, Species == &quot;setosa&quot;) #まずSpeciesのうち、setosaのみを取り出す。dat1という名前で保存する。 dat2 = dplyr::select(dat1, Species, Sepal.Length, Petal.Length) #別の名前で保存し直したdat1から、Sepal.LengthとPetal.Lengthの列を取り出す。dat2という名前で保存する head(dat2) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 パイプ（|&gt;）を使えば、このプログラムを1文で書くことができる。 dat2 = iris |&gt; dplyr::filter(Species == &quot;setosa&quot;) |&gt; dplyr::select(Species, Sepal.Length, Petal.Length) head(dat2) ## Species Sepal.Length Petal.Length ## 1 setosa 5.1 1.4 ## 2 setosa 4.9 1.4 ## 3 setosa 4.7 1.3 ## 4 setosa 4.6 1.5 ## 5 setosa 5.0 1.4 ## 6 setosa 5.4 1.7 |&gt;はパイプと呼ばれ、プログラムを渡していく関数である。irisデータをfilterに渡し、その結果をselectに渡している。 5.5 グルーピング パイプを利用することで、グループごとに統計量（平均値や標準偏差など）を算出することができる。 irisデータを例として、グループごとに平均や標準偏差を計算する方法を覚えよう。 あやめの種類ごとに、がくの長さの平均値と標準偏差を算出してみる。 先ほど学んだパイプ処理（|&gt;）に加え、dplyrパッケージのgroup_byとsummarise関数を利用する。 iris |&gt; dplyr::group_by(Species) |&gt; dplyr::summarise(Mean = mean(Sepal.Width, na.rm = TRUE), SD = sd(Sepal.Width, na.rm = TRUE), N = n()) ## # A tibble: 3 × 4 ## Species Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 group_by()はグループ変数を作成する関数である。データの中でグループとして使いたい変数を括弧内に指定する。 summarise()は、複数の関数を実行させる関数である。この例では、mean()、sd(), n()（dplyrパッケージにあるデータ個数を集計する関数）の3つの関数を実行し、それぞれの結果をMean, SD, Nという別の名前で保存している。 5.6 データの変換 tidyrパッケージに入っているpivot_longer()とpivot_wider()を使うと、データの並び替えなどをすることができる。 5.6.1 wide型とlong型の区別 まず、データのレイアウトには、wide型とlong型の二種類があることを理解しよう。 以下のデータを例として説明する。A, B, Cの3人の参加者が、X, Y, Z条件の３つの条件で実験課題を行ったとする。 まずは、以下のプログラムを実行してサンプルデータを作成しよう。 dat_wide = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) #サンプルデータを作る dat_wide ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 このようにデータの入力方法として、１行につき１人の参加者の情報を入力するやり方がある（実験や調査でデータを入力する際も、このレイアウトの方が入力しやすいだろう）。このようなデータのレイアウトをwide型という。 同じデータを、以下のようなレイアウトで表現することもできる。同じく、以下のプログラムを実行して、サンプルデータを作ろう。 dat_long = data.frame(Subject = sort(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 3)), Condition = rep(c(&quot;X&quot;,&quot;Y&quot;,&quot;Z&quot;), 3), Score = c(6,9,7,2,3,5,7,4,7), Gender = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;,&quot;F&quot;, &quot;F&quot;), Age = sort(rep(c(18,19,20), 3))) dat_long ## Subject Condition Score Gender Age ## 1 A X 6 M 18 ## 2 A Y 9 M 18 ## 3 A Z 7 M 18 ## 4 B X 2 F 19 ## 5 B Y 3 F 19 ## 6 B Z 5 F 19 ## 7 C X 7 F 20 ## 8 C Y 4 F 20 ## 9 C Z 7 F 20 実験成績ごとに１行ずつでデータが作られている。すなわち、同じ参加者1人につき3行のデータがある。このようなデータの方をlong型と呼ぶ。 どのデータ型にすべきか？ R でデータ解析に用いる関数のほとんどは、「1つの観測値（observation）につき1行」が原則、つまりデータがlong型であることを前提として作られている。このテキストで学ぶデータ解析も、基本的に分析で使うデータはlong型を前提とする。 その一方、データを入力するときなど、実務的にはwide型が扱いやすいということもあるだろう。データ入力は研究者の都合に応じてやりやすい方法で用意するとして、解析をする際に適切なデータ形式に変換するすべを身に着けておこう。 5.6.2 wide型からlong型に変換 tidyrパッケージのpivot_longer()を使う。 dat_long2 = dat_wide |&gt; tidyr::pivot_longer(cols = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), names_to = &quot;Condition&quot;, values_to = &quot;Score&quot;) dat_long2 ## # A tibble: 9 × 5 ## Subject Gender Age Condition Score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A M 18 X 6 ## 2 A M 18 Y 9 ## 3 A M 18 Z 7 ## 4 B F 19 X 2 ## 5 B F 19 Y 3 ## 6 B F 19 Z 5 ## 7 C F 20 X 7 ## 8 C F 20 Y 4 ## 9 C F 20 Z 7 cols =で、並べ替える変数を指定する。names_to =で新しく作られるグループを意味する列の名前、values_to =で値を意味する列の名前を指定する。 5.6.3 long型からwide型に変換 tidyrパッケージのpivot_wider()を使う。 dat_wide2 = dat_long |&gt; tidyr::pivot_wider(names_from = Condition, values_from = Score) dat_wide2 ## # A tibble: 3 × 6 ## Subject Gender Age X Y Z ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A M 18 6 9 7 ## 2 B F 19 2 3 5 ## 3 C F 20 7 4 7 names_from =で横並びにしたときの値のラベル名、values_from =で横並びの対象となる値を指定する。 5.7 データの結合 複数のデータを結合したい場合は、dplyrパッケージのjoin関数を使うとよい。join関数には、left_join, full_joinなど、いくつかの種類が用意されている。 サンプルデータを使いながら、手順について説明する。まず、以下のプログラムを実行して、サンプルデータを作ろう。 dat_sample = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), X = c(6,2,7), Y = c(9,3,4), Z = c(7,5,7), Gender = c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;), Age = c(18, 19, 20)) dat_sample ## Subject X Y Z Gender Age ## 1 A 6 9 7 M 18 ## 2 B 2 3 5 F 19 ## 3 C 7 4 7 F 20 実験で3人の参加者A, B, Cについて、X, Y, Zのデータを取ったとする。 更に、2人の参加者（AとB）に追加で実験を行い、Wのデータを取ったとする。 dat_sample2 = data.frame(Subject = c(&quot;A&quot;,&quot;B&quot;), W = c(8,3)) dat_sample2 ## Subject W ## 1 A 8 ## 2 B 3 dat_sampleとdat_sample2のデータを結合して、一つのデータにしたい。 full_join()で結合したい2つのデータ、更に結合する際にキーとなる変数（2つのデータに共通して存在する変数）をby=で指定すると2つのデータを結合してくれる。 なお、by=を省くと、自動で2つのデータに共通する変数を見つけて、それを手がかりに結合してくれる。 dat_sample3 = dplyr::full_join(dat_sample, dat_sample2, by = &quot;Subject&quot;) dat_sample3 ## Subject X Y Z Gender Age W ## 1 A 6 9 7 M 18 8 ## 2 B 2 3 5 F 19 3 ## 3 C 7 4 7 F 20 NA full_join()だと、2つのデータをすべてつなげてくれる。データが含まれていない部分は、欠損になる（data_sample2に参加者Cのデータはないので、欠損になっている）。 left_join()だと、left_join()で左側に入力したデータを含む部分のみをつなげてくれる。 dat_sample3 = dplyr::left_join(dat_sample2, dat_sample, by = &quot;Subject&quot;) dat_sample3 ## Subject W X Y Z Gender Age ## 1 A 8 6 9 7 M 18 ## 2 B 3 2 3 5 F 19 5.8 確認問題 問１ irisデータから、1)Speciesがversicolorである行を選び、2) Species, Petal.Length及びPetal.Widthの列を取り出し、3) Petal.LengthとPetal.Widthを足し合わせた変数hogeを作るという一連の処理を、パイプ処理を使ってプログラム1行でやってみよう。 問２ irisデータから、1)Speciesがvirginica以外の行を選び、2) Species, Petal.Length及びPetal.Widthの列を取り出す処理を、パイプ処理を使ってプログラム1行でやってみよう。 ヒント：Rでは、!=が「○○ではない」を意味する論理式である（第2章参照）。 問３ irisデータで、Species別にPetal.Lengthの平均値、標準偏差を求めよう。 ヒント：group_by()とsummarise()の使い方をおさらいする。 "],["05-graph.html", "Chapter 6 データ・ビジュアライゼーション 6.1 Rのグラフィック関数 6.2 ggplot2パッケージ 6.3 オプション 6.4 応用的なグラフ 6.5 その他の機能 6.6 確認問題", " Chapter 6 データ・ビジュアライゼーション データをグラフで表現する方法について学ぶ。 データの傾向をグラフによって表現することを可視化(visualization)という。この章では、Rに標準で入っているグラフィック関数を確認しつつ、可視化に特化したggplot2パッケージを使った可視化の方法について学んでいく。更に、ggplot2以外の可視化用パッケージについても触れていく。 6.1 Rのグラフィック関数 Rには標準でグラフを作成するための関数がいくつか用意されている。 6.1.1 plot plotはグラフを作る基本となる関数である。 #サンプルデータをつくる sample = data.frame( x = 1:5, y = c(2, 5, 7, 11, 9) ) sample ## x y ## 1 1 2 ## 2 2 5 ## 3 3 7 ## 4 4 11 ## 5 5 9 plot(x = sample$x, y = sample$y) plot()にx軸とy軸にプロットするデータをベクトルのかたちで入れると、図を作成してくれる。 更に、plot()関数では、自分の好みに合わせてグラフの体裁を変えることもできるオプションも用意されている。 オプションの指定 以下に例として、様々なオプションを変更したグラフを示す。 plot(x = sample$x, y = sample$y, type = &quot;b&quot;, #type: グラフの種類 pch = 2, #点の種類 cex = 1.5, #点の大きさ lty = &quot;dotted&quot;, #線の種類 lwd = 2, #線の太さ col = &quot;blue&quot;, #色 xlim = c(0,6), #x軸の範囲（最小値, 最大値） ylim = c(0,15), #y軸の範囲（最小値, 最大値） las = 1, #軸の文字の配置 main = &quot;Graph&quot;, #グラフのタイトル xlab = &quot;x value&quot;, #x軸のラベル ylab = &quot;y value&quot; #y軸のラベル ) type: グラフの種類を変える（“p” = 点（デフォルト）、 “l” = 線、 “b” = 点と線の両方、“n” = 何も描画しない、など）。 pch: 点の種類を指定（0 = 四角、1 = 丸（デフォルト）、2 = 三角、など）。 cex: 点の大きさを指定。数値を入力する。 lty: 線の種類を指定（“solid” = 実線（デフォルト）、“dotted” = ドット、“dashed” = ダッシュ、など） lwd: 線の太さの指定。数値を入力する。 col: 色を変更する。“red”, “blue”など色の名前を指定する。 xlim, ylim: それぞれx軸とy軸の最小値と最大値を指定。 las: 軸の文字の配置を指定（1 = x軸もy軸も水平方向）。 main, xlab, ylab: それぞれグラフのラベルを指定。 他にもどのようなオプションを指定できるか確認したい場合は、ヘルプを参照。?parsと入力するとヘルプが表示される。 6.1.2 その他のグラフ 以下では、Rで標準でに入っているサンプルデータirisで様々なグラフを作ってみよう。 head(iris) #データの上数行を表示して中身を確認する。 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa 散布図 plot()で作成する。 plot(x = iris$Sepal.Length, y = iris$Sepal.Width) ヒストグラム hist()で作成する。 hist(iris$Sepal.Length, breaks = 20, #breaksでビン（棒）の数を指定できる col = &quot;red&quot;, main = &quot;Histgram&quot;, xlab = &quot;Sepal Length&quot;, ylab = &quot;Frequency&quot;) #その他のオプションも指定可能 箱ひげ図 boxplot()で作成する。 boxplot(x = iris$Sepal.Length, main = &quot;Boxplot&quot;, col = &quot;red&quot;, xlab = &quot;Sepal Length&quot;) #その他のオプションも指定可能 boxplot(data = iris, Sepal.Length ~ Species, main = &quot;Boxplot&quot;, col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), xlab = &quot;Sepal Length&quot;) #グループごとに示す場合 グラフを重ねる 同じグラフの中でグループ別にデータを表示したい場合は、例えば以下のようにグラフを重ねて描画していくかたちで表現することができる。 以下に例として、irisデータのSpeciesごとに色や点の形を変えて散布図を示してみる。 まず、グラフの領域を作成して、その上にグループ別にpointやlinesなどで点や線を重ねていくかたちで描画していく。 #該当する部分を取り出してベクトルに格納する。 Sepal.Length_setosa &lt;- iris$Sepal.Length[iris$Species == &quot;setosa&quot;] Sepal.Width_setosa &lt;- iris$Sepal.Width[iris$Species == &quot;setosa&quot;] Sepal.Length_versicolor &lt;- iris$Sepal.Length[iris$Species == &quot;versicolor&quot;] Sepal.Width_versicolor &lt;- iris$Sepal.Width[iris$Species == &quot;versicolor&quot;] Sepal.Length_virginica &lt;- iris$Sepal.Length[iris$Species == &quot;virginica&quot;] Sepal.Width_virginica &lt;- iris$Sepal.Width[iris$Species == &quot;virginica&quot;] plot(0, 0, type = &quot;n&quot;, xlim = c(3.5, 8.5), ylim=c(0, 6), xlab = &quot;Sepal Length&quot;, ylab = &quot;Sepal Width&quot;)#まず、空のプロットを作る（軸の範囲やラベルを指定する） points(x = Sepal.Length_setosa, y = Sepal.Width_setosa, col = &quot;red&quot;, pch = 0) points(x = Sepal.Length_versicolor, y = Sepal.Width_versicolor, col = &quot;blue&quot;, pch = 1) points(x = Sepal.Length_virginica, y = Sepal.Width_virginica, col = &quot;orange&quot;, pch = 2) legend(&quot;topleft&quot;, legend = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), pch = c(0, 1, 2), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;orange&quot;)) #legendで凡例を重ねることもできる。 6.1.3 図の保存 #png形式で保存する場合 png(&quot;plot.png&quot;, height = 400, width = 400) #ファイル名を指定する。画像サイズも任意に指定できる（特に指定しなくても出力してくれる）。 plot(x = iris$Sepal.Length, y = iris$Petal.Length) #グラフを作る dev.off() #描画デバイスを閉じる RStudio ならば、右下の「Plots」ウィンドウの「Export」から保存することも可能。 6.2 ggplot2パッケージ 手っ取り早くデータの分布や相関を確認したい場合は、plotなどの関数を使えば十分である。plotなどでもオプションを駆使すれば研究報告向きのグラフを作れないこともないが、プログラムやオプションの指定の方法も複雑で、作成できるグラフの種類も少ない。 これに対し、様々なグラフを作る機能に特化したggplot2というパッケージがある。以降で、ggplot2の基本的な使い方について解説する。 まずは、ggplot2パッケージをロードする。 library(ggplot2) 早速、ggplot2を使ってグラフを作ってみよう。以下のプログラムを実行してみよう。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) p プログラムの解説： ggplot()：初期設定。「ggplot2を使ってグラフを書きます」という意味。必ず書く。カッコの中には何も入れなくて良い。 geom_xxxx()：グラフの種類の指定。必ず書く。xxxxには、グラフの種類を入力する。この例では、散布図を書くのでgeom_pointを指定した。更に、カッコの中に必要な設定を記す。 data =でグラフを描画するデータを指定する。 更に、aes()のカッコの中に描画に必要な要素を指定する。x=とy=でそれぞれ、 x軸とy軸に指定したい変数を指定する。点の大きさ、色、線の種類など、グラフの種類によって指定できる要素がある。 オプション：上での例では、labs()でx軸やy軸のラベルを指定している。他にも、軸の値の範囲、軸のラベル、グラフの色の設定などをオプションで指定することができる。オプションを加えなくてもグラフは出力される。 とりあえず、これらだけ知っておけばggplot2でグラフを作れる。 6.2.1 散布図 geom_pointで作成できる。 p = ggplot2::ggplot() + geom_point(data=mpg, aes(x=cty, y=hwy, shape = fl)) p この例では、shape =でグループを指定して、グループごとに点のかたちを変えた散布図を作成した。 点が重なって見えにくい場合は、geom_jitterを使うとランダムのズレを生成して表示してくれる。 p = ggplot2::ggplot() + geom_jitter(data=mpg, aes(x=cty, y=hwy, shape = fl)) p 6.2.2 ヒストグラム geom_histogramで作成する。 p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length)) #xに、横軸にしたい変数を入れる。 p p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, fill = Species)) #種類ごとに色の塗りつぶしを変えたい場合は、fillに指定する。 p p = ggplot2::ggplot() + geom_histogram(data = iris, aes(x=Sepal.Length, color = Species)) #colorだと周りの線の色を変える。 p 6.2.3 箱ひげ図 geom_boxplotで作成する。 最小値、第一分位点、中央値、第三分位点、最大値を示す（外れ値は点で示される）。 p = ggplot2::ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot2::ggplot() + geom_boxplot(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 6.2.4 バイオリンプロット データの分布を表現したグラフ。 geom_violinで作成する。 p = ggplot2::ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count)) p p = ggplot2::ggplot() + geom_violin(data = InsectSprays, aes(x=spray, y=count, fill = spray)) p 6.2.5 折れ線グラフ geom_line()を使う。geom_line()だけだと線のみだが、geom_point()で作ったグラフを重ねることで、点もつけることができる。このように、複数のグラフを重ねて描画することもできる。 #サンプルデータをつくる: 10日間の気温の変化 temperature = data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) temperature ## Days Celsius ## 1 1 17.2 ## 2 2 17.5 ## 3 3 18.1 ## 4 4 18.8 ## 5 5 19.0 ## 6 6 19.2 ## 7 7 19.7 ## 8 8 20.2 ## 9 9 20.5 ## 10 10 20.1 p = ggplot2::ggplot() + geom_line(data = temperature, aes(x=Days, y=Celsius)) + geom_point(data = temperature, aes(x=Days, y=Celsius)) p 6.2.6 エラーバーつきのグラフ geom_errorbar()でエラーバーをつけることができる。 あるいは、geom_pointrange()でも作れる。 #サンプルデータをつくる sample_dat = data.frame(Condition=c(&quot;A&quot;, &quot;B&quot; ,&quot;C&quot;), mean=c(2, 5, 8), lower=c(1.1, 4.2, 7.5), upper=c(3.0, 6.8, 9.1)) #meanが平均、lowerとupperにそれぞれ下限値と上限値。 p = ggplot2::ggplot() + geom_point(data = sample_dat, aes(x = Condition, y = mean)) + geom_errorbar(data = sample_dat, aes(x = Condition, ymax = upper, ymin = lower), width = 0.1) #まず、geom_pointで平均を点で示したグラフを作成する。そのグラフに、ymaxとyminにそれぞれ上限値と下限値を指定したエラーバーのグラフを重ねる（widthでエラーバーの横の長さを指定できる）。 p p2 = ggplot2::ggplot() + geom_pointrange(data = sample_dat, aes(x = Condition, y = mean, ymax = upper, ymin = lower)) #geom_pointrangeならば、点とエラーバーの両方を一括して指定できる。 p2 6.3 オプション 6.3.1 ファセット（Facet） グループごとにグラフを分けたい場合は、ファセット（facet）を利用すると良い。facet_wrap()を使う。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + facet_wrap(vars(Species)) p 6.3.2 ラベル（labs） x軸やy軸のラベルを変えたいときは、labsを使うと良い。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Length of sepal&quot;, y = &quot;Length of petal&quot;) p 6.3.3 テーマ（Theme） theme()で、フォントの大きさや色などグラフのテーマも細かく変えることができる。具体的にどの部分を変えられるかは、theme()のヘルプで確認してほしい。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme(axis.title.x = element_text(size = 20)) #x軸のフォントサイズを変える p + theme(panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) #背景や枠線の色を変える 手っ取り早く一括でテーマを変えたい場合は、予め用意されている既存のテーマを選ぶと良い。theme_bw(), theme_classic()などが用意されている。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) p + theme_bw() p + theme_gray() p + theme_classic(base_size = 20) #base_sizeで軸や軸ラベルなどのバランスを考慮した上でフォントサイズを整えることができる 6.3.4 図の保存 ggplot2パッケージのggsave()で保存できる。plotに保存した図を、filenameにファイル名を指定すると、ワーキングディレクトリに作成した図が保存される。 p = ggplot2::ggplot() + geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + labs(x = &quot;Sepal Length&quot;, y = &quot;Petal Length&quot;) + theme_bw() ggplot2::ggsave(plot = p, filename = &quot;plot.png&quot;) #ファイル名を指定する。拡張子も忘れずにつける。 ggplot2::ggsave(plot = p, filename = &quot;plot_2.png&quot;, dpi = 300) #解像度（dpi）を指定可能。 ggplot2::ggsave(plot = p, filename = &quot;plot_3.png&quot;, width = 8, height = 5) #幅(width)や高さ(height)を指定可能（単位はインチ）。 6.4 応用的なグラフ その他のパッケージとの併用で、更に応用的なグラフの作成も可能である。 6.4.1 Rain-cloud plot 箱ひげ図、密度曲線、ドットプロットをあわせて表示したもの。データの分布がよりわかりやすい。 以下は、ggplot2とggdistパッケージを使って作図した例である。 library(ggdist) ggplot2::ggplot()+ ggdist::stat_halfeye(data = iris, aes(x = Species, y = Sepal.Length, fill = Species), width = 0.5,.width = 0, point_colour = NA, position = position_nudge(x = 0.2)) + geom_boxplot(data = iris, aes(x = Species, y = Sepal.Length, fill = Species), width=0.2, outlier.color = NA, position = position_nudge(x = 0)) + geom_jitter(data = iris, aes(x = Species, y = Sepal.Length), width = 0.05,alpha = 0.5) + coord_flip() + labs(y = &quot;Sepal Length&quot;, x = &quot;Species&quot;) + theme_bw() 6.4.2 Ridge plot 密度曲線をグループ別に示したもの。 以下は、ggplot2とggridgesパッケージを使って作図した例である。 library(ggridges) ggplot2::ggplot()+ ggridges::geom_density_ridges(data = iris, aes(x = Sepal.Length, y = Species, fill = Species), alpha = 0.5, scale = 1) + labs(x = &quot;Sepal Length&quot;, y = &quot;Species&quot;) + theme_bw() 6.4.3 Alluvial plot 複数のカテゴリの分類をフローの形式で表現したグラフ。ggalluvialパッケージを使って作図する。 library(ggalluvial) UCBAdmissions_2 = data.frame(UCBAdmissions) #サンプルデータUCBAdmissionsを使ってみる。データフレーム形式に修正する。 head(UCBAdmissions_2) #カテゴリ別に頻度を集計したデータフレームを用意する。 ## Admit Gender Dept Freq ## 1 Admitted Male A 512 ## 2 Rejected Male A 313 ## 3 Admitted Female A 89 ## 4 Rejected Female A 19 ## 5 Admitted Male B 353 ## 6 Rejected Male B 207 ggplot2::ggplot(data = UCBAdmissions_2, aes(axis1 = Gender, axis2 = Dept, axis3 = Admit, y = Freq)) + ggalluvial::geom_alluvium(aes(fill = Admit)) + ggalluvial::geom_stratum() + ggplot2::geom_text(stat = &quot;stratum&quot;, aes(label = after_stat(stratum))) + scale_x_discrete(limits = c(&quot;Gender&quot;, &quot;Dept&quot;, &quot;Admit&quot;)) + labs(x = &quot;&quot;, y = &quot;Frequency&quot;) + theme_bw() 6.5 その他の機能 ここで紹介しているのはggplot2の一部の機能にすぎない。このテキストで説明しきれないことについては、関数のヘルプやRStudioのサイトにあるggplot2のCheat sheetを見てみよう（日本語訳もある）。 https://rstudio.com/resources/cheatsheets/ 6.6 確認問題 Rに標準で入っているmtcarsデータを使って、グラフを作成しよう。 問１ Rで標準で入っているhist()を使って、変数mpgのヒストグラムを作ろう。 問２ Rで標準で入っているplot()を使って、変数mpgと変数wtの散布図を作ろう。 問３ ggplot()で、変数mpgと変数wtの散布図を作ろう。グラフには以下の点を反映させること。 x軸をmpg、y軸をwtとする。 x軸のラベルは「Miles/gallon」、y軸のラベルは「Weight」とする。 テーマはtheme_classic()にする。 "],["06-probability_distribution.html", "Chapter 7 確率分布 7.1 確率変数と確率分布 7.2 二項分布 7.3 正規分布 7.4 ポアソン分布 7.5 Rで使える確率分布関数 7.6 正規分布と他の確率変数との関係 7.7 確率分布の表現の仕方 7.8 その他の確率分布 7.9 確認問題", " Chapter 7 確率分布 前章まではRの使い方について扱ってきたが、この章以降から本格的に統計解析を学んでいく。まずは、基礎統計学の復習として、「確率分布」を学んでいく。確率分布は統計学の基礎であると同時に、今後学んでいく統計モデルを理解する上でとても重要な知識である。 確率変数と確率分布 一様分布 二項分布 正規分布 ポアソン分布 Rの確率分布関数 中心極限定理 その他の確率分布 準備として、以下のプログラムを実行しよう。 この章でもggplot2パッケージを使うので、ロードしておく。 library(ggplot2) 更に、乱数の種を指定する。set.seed(1)と入力して実行する。このプログラムを実行しておくと、このテキストに書かれているプログラムの結果と同じ結果を再現できる。 set.seed(1) 7.1 確率変数と確率分布 まず、サイコロを例として、確率分布とは何かについて理解する。 サイコロを1個投げるとする。それぞれの目が出る確率は1/6である。それぞれの目を\\(X\\)（1, 2, 3, 4, 5, 6）、それぞれの目が出る確率を\\(P(X)\\)とする。\\(X\\)と\\(P(X)\\)を以下の表で示す。 このとき、Xを確率変数と呼ぶ。確率変数とは、その値と対応する確率が存在する変数のことをいう。表のように、確率変数とその変数が取り得る確率の分布を確率分布という。 確率分布は大きく分けて、確率変数が離散値である離散型分布と連続値である連続型分布に分かれる。 7.1.1 離散型分布 確率変数が離散値（小数の値を取らないもの）である場合の確率分布である。例えば、以下のようなものがある。 （離散型）一様分布 二項分布 ベルヌーイ分布 ポアソン分布 負の二項分布 多項分布 7.1.2 連続型分布 確率変数が連続値（小数の値が存在するもの）である確率分布である。例えば、以下のようなものがある。 （連続型）一様分布 正規分布 指数分布 対数正規分布 t分布 χ二乗分布 コーシー分布 ガンマ分布 今後の章でも扱う確率分布として、二項分布（ベルヌーイ分布）、正規分布、ポアソン分布について学んでいく。 7.2 二項分布 コインを\\(n\\)回投げる。表が出る確率を\\(q\\)とすると、裏が出る確率は\\((1-q)\\)である。\\(n\\)回中表が\\(x\\)回出る確率\\(P(x)\\)は、以下の式で求められる。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] \\(x\\)を確率変数とした場合、上記の式の確率に従う確率分布を二項分布(binomial distribution)という。 つまり二項分布は、2つのカテゴリで表現されるある事象が何回生じるかの確率を表している。コイン（表か裏）を何回か投げたときに表が出る回数、学生の中から何人か選んだときの男の人数など。これらのような事象が生じる確率は、理論的には二項分布に従う。 例えば、コインの表が出る確率\\(q\\)を0.5とする。10回投げたときに表が6回出る確率を計算してみよう。Rならば、dbinom()関数を使えば計算できる（この関数の意味については、また後で説明する）。 #xは確率変数（コインの例でいうと表が出た回数）、sizeは試行回数（コインの例でいうとコインを投げた回数）、 dbinom(x=6, size=10, prob=0.5) ## [1] 0.2050781 #上の式n, x, pに実際に値を入れて計算する。dbinom()関数を使った場合と結果が一致することを確認しよう。 choose(10, 6) * 0.5^6 * (1 - 0.5) ^4 ## [1] 0.2050781 7.2.1 二項分布の期待値と分散 表が出る回数\\(x\\)が0〜10の全てについて、それぞれが生じる確率を計算すると以下のようになる。 dbinom(x=0:10, size=10, prob=0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 グラフにすると以下のようになる。横軸をx, 縦軸をP(x)とする。 d_plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) p = ggplot2::ggplot() + ggplot2::geom_bar(data = d_plot, aes(x = factor(x), y = p), stat=&quot;identity&quot;) + labs(y= &quot;P(x)&quot;, x= &quot;x&quot;) p グラフからもわかるように、表が出る確率が0.5のコインを10回投げたときに、最も出やすいのは10回中5回であることがわかる。10回中8回以上はほとんどまれであることがわかる。 この図からも、確率分布には最も出やすい変数（平均値。確率分布の場合は「期待値」と呼ぶ）と分散が存在することがわかる。 二項分布の期待値\\(E(x)\\)と分散\\(Var(x)\\)は、以下の式から計算できる。 \\[ E(x) = nq\\\\ Var(x) = nq(1-q) \\] この式から、表が出る確率が0.5（つまり、\\(q=0.5\\)）として、10回投げた場合(つまり, \\(n=10\\))における、表が出る回数の期待値と分散を計算してみよう。 #E(x) = nq 10*0.5 ## [1] 5 #Var(x) = nq(1-q) 10*0.5*(1-0.5) ## [1] 2.5 7.2.2 ベルヌーイ分布 二項分布で\\(n=1\\)のときは、ベルヌーイ分布(Bernoulli distribution)と呼ぶ。\\(x\\)は1か0を取る変数で、\\(x=1\\)のときの確率を\\(q\\)とすると、\\(x=1\\)及び\\(x=0\\)となる確率は以下の式で表される。 \\[ P(x=1) = q\\\\ P(x=0) = 1 - q\\\\ \\] 例： コインを一回だけ投げたときに、表が出るあるいは裏が出る確率。 サイコロを一回だけ振ったときに、1が出るあるいは1以外が出る確率。 dbinom(x=0:1, size=1, prob=0.5) #表が出る確率0.5のコインを投げた時に、表が出ない時の確率と表が出る時の確率が出力される。 ## [1] 0.5 0.5 dbinom(x=0:1, size=1, prob=1/6)#ある目が出る確率1/6のサイコロを振った時に、その目以外が出る確率とその目が出る確率が出力される。 ## [1] 0.8333333 0.1666667 d_plot = data.frame(x=0:1, p=dbinom(x=0:1, size=1, prob=0.5)) p = ggplot2::ggplot() + ggplot2::geom_bar(data = d_plot, aes(x = factor(x), y = p), stat=&quot;identity&quot;) + ylim(c(0, 1)) + labs(y= &quot;P(x)&quot;, x= &quot;x&quot;) p 7.3 正規分布 7.3.1 正規分布の基礎 統計学で用いられる確率分布の中でも有名なのは、正規分布(normal distribution)である。正規分布は、平均\\(\\mu\\)、標準偏差\\(\\sigma\\)を持つ確率分布で、釣鐘型（ベル・カーブ）の分布を描く。 平均\\(\\mu\\)、標準偏差\\(\\sigma\\)とする正規分布の確率密度関数\\(f(x)\\)は、以下の式から計算される（「確率密度関数」とは何かは、後で説明する）。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] 正規分布の式を覚える必要はない。式の中に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)が含まれていることだけ覚えておこう。また、正規分布はガウス分布（gaussian distribution）と呼ばれることもある。 試しに、平均0、標準偏差1の正規分布のグラフを作ってみよう。以下のプログラムではdnorm()という新しく出てきた関数もあるが、使い方については後で解説する。 x = seq(-3, 3, 0.05) # -3から3まで0.05刻みで数字の連続を作る y = dnorm(x=x, mean=0, sd=1) #平均0、標準偏差1の正規分布の確率を算出する dat_norm = data.frame(x = x, y = y) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_norm, aes(x = x, y = y)) p 7.3.2 確率密度関数 正規分布のグラフは確率分布を表しているのではなく、確率密度関数であることにも注意してほしい。先程の二項分布では、縦軸は、横軸の値（確率変数）が生じる「確率」そのものを意味していた。しかし、正規分布のグラフの縦軸は、横軸の値が生じる確率を意味しない。確率密度関数は、グラフの面積が確率を表す。 例えば、x = 0のときのyの値を確認しよう。 dnorm(x = 0, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 ## [1] 0.3989423 dnorm()は、確率密度関数の縦軸の値を出力する。x = 0のときの縦軸の値は0.4であり、先程のグラフからもわかるように、縦軸の値と一致している。しかし、x = 0が生じる確率は0.4ではない。確率密度関数の場合、縦軸の値そのものは何の意味も持たない。 二項分布のような離散型の変数である場合に対して、正規分布のように変数が連続型（小数点を含む値）であるものは、特定の値の確率を定義することができない。例えば、身長が170.5cmである確率を求めるにしても、170cmから171cmまでの間に,170.001cm, 170.002cmと無限の値が広がっている。170.5cmぴったりである確率を求めることはできず、連続型の変数において特定の値が生じる確率はゼロということになる。したがって、連続型の変数の確率については、個々の確率を求めるのではなく、確率密度関数によって区間の確率を求める方法が取られる。 x &lt; 0の範囲の面積を求めよう。正規分布の左半分なので、確率は0.5である。 pnorm(q = 0, mean = 0, sd = 1) #-∞からqの値までの範囲を求めてくれる。 ## [1] 0.5 7.3.3 正規分布の平均値と標準偏差 以下が、平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の値をそれぞれ変えた場合の正規分布である。赤が平均0で標準偏差1、青が平均1で標準偏差1, 黒が平均0で標準偏差2である。平均と標準偏差によって、正規分布の形状が変化することを理解しよう。 x = seq(-4, 4, 0.05) y_1 = dnorm(x=x, mean=0, sd=1) y_2 = dnorm(x=x, mean=1, sd=1) y_3 = dnorm(x=x, mean=0, sd=2) dat_norm_1 = data.frame(x = x, y = y_1) dat_norm_2 = data.frame(x = x, y = y_2) dat_norm_3 = data.frame(x = x, y = y_3) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_norm_1, aes(x = x, y = y), color=&quot;red&quot;)+ ggplot2::geom_line(data = dat_norm_2, aes(x = x, y = y), color= &quot;blue&quot;)+ ggplot2::geom_line(data = dat_norm_3, aes(x = x, y = y), color=&quot;black&quot;) p 7.4 ポアソン分布 xは0以上の整数（0, 1, 2, 3, …）とする。xがポアソン分布に従う場合、xの値それぞれが生じる確率は以下のように表す。 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ \\] ポアソン分布 (Poisson distribution)の期待値（平均）は\\(\\lambda\\)、分散は\\(\\lambda\\)である。つまり、ポアソン分布は平均と分散が等しい分布である。 以下に、\\(\\lambda\\)をそれぞれ変えた場合のポアソン分布を示す。 pois_1 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=1), lambda=1) pois_2 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=2), lambda=2) pois_3 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=3), lambda=3) pois = rbind(pois_1, pois_2, pois_3) ggplot() + ggplot2::geom_bar(data = pois, aes(x=factor(x), y=p, fill=factor(lambda)), stat=&quot;identity&quot;, color = &quot;black&quot;, position = &quot;dodge&quot;) + labs(y= &quot;P(x)&quot;, x = &quot;x&quot;, fill = &quot;lambda&quot;) 一定の期間中にランダムで生じる事象はポアソン分布に従う。具体的な例としては、1日の間に届くメールの件数、営業時間中に来る客の数など。 二項分布の試行数\\(n\\)が十分大きくかつ確率\\(q\\)が小さい場合は、平均を\\(np\\)とするポアソン分布に近似する。つまり、めったに起こらない事象はポアソン分布に従う。例えば、1年間の間に生じる交通事故の件数など（365日それぞれの日に0.1%で生じる場合など）。歴史的に有名な例として、「ドイツ軍で1年間で馬に蹴られて死亡した兵士の数」がポアソン分布に従うといったものがある。 二項分布と同じく、ポアソン分布の確率変数は離散値である。離散値は、1個、2個、3個と数える個数のような整数の値をいう（1.1個といった小数の値が存在しないもの）。それに対し、正規分布の確率変数は連続量である。 7.5 Rで使える確率分布関数 ここまで、dbinom、dnorm, dpois, pnormなど、確率分布を扱う関数がいくつか出てきた。これらは、Rに標準で入っている関数である。 Rには確率分布から乱数を生成したり、確率変数の確率を求めることができる関数が実装されている。関数は、確率分布それぞれにrXXXX, qXXXX, pXXXX, dXXXXといった4種類の関数が用意されている（ XXXXには確率分布の種類が入る）。 rXXXXは、乱数(random number)を出力する。 rnorm(n = 10, mean = 0, sd = 1) #平均0、標準偏差1に従う正規分布から乱数を10個生成する ## [1] -0.6264538 0.1836433 -0.8356286 1.5952808 0.3295078 -0.8204684 ## [7] 0.4874291 0.7383247 0.5757814 -0.3053884 dXXXXは、確率変数xが生じる確率密度（確率密度関数の縦軸の値）を出力する。 dnorm(x = 0.5, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、x=0.5のときの確率密度を求める（確率密度の値であって、確率そのものではないので注意） ## [1] 0.3520653 qXXXXは、確率点(quantile)を出力する。ある確率を取る時のx軸の値を出力してくれる。 qnorm(p = 0.5, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、p ≤ 0.5の確率となるときの確率変数の値を求める ## [1] 0 pXXXXは、累積確率を出力する。 pnorm(q = 1, mean = 0, sd = 1) # 平均0、標準偏差1に従う正規分布 で、x ≤ 1の確率を求める ## [1] 0.8413447 他の確率分布についても同様に、4種類の関数が用意されている。 #乱数を作る関数 runif(n=100, min = 0, max = 1) #一様分布からの乱数 rbinom(n=100, size = 10, prob = 0.5) #二項分布からの乱数 rpois(n=100, lambda = 3) #ポアソン分布からの乱数 7.6 正規分布と他の確率変数との関係 なぜ正規分布は「正規（normal）」な分布なのか？よく言われるのは、「現実世界の様々な分布が正規分布に従う」からという説明である。 しかし、現実には正規分布に従わないデータの方が多い。体重や身長なども、実際には正規分布を描くことは少ない。年収なども釣鐘型の分布にならない。また、正規分布は確率変数が連続量の場合の確率分布である。心理学ではアンケートへの回答得点などを分析したりするが、離散値や順序尺度（すなわちカテゴリカル変数）が正規分布に従うという前提を置くのはそもそも適切ではない。 では、正規分布がよく使われる理由は何か。もっとらしい理由は、数学的な扱いやすさである。なぜならば、元の変数がどのような確率分布に従っていたとしても、変数を足し合わせた結果は正規分布に従うという都合の良い性質があるからである。この性質は、中心極限定理と呼ばれるものである。中心極限定理により、正規分布は身近に現れる「正規な分布」となり得るのである。 7.6.1 中心極限定理 中心極限定理とは、「母集団が平均及び標準偏差を持つ確率分布であるならば、たとえ母集団が正規分布でなくても、母集団から標本を無作為抽出して平均値を計算することを何回も繰り返すとその分布は正規分布に近づく」という定理である。 シミュレーションで中心極限定理を実感してみよう。6面のサイコロを100回振る実験を行うとする。 以下のプログラムを実行してみよう。runif()は一様分布から乱数を生成する関数である（一様分布の詳細については、以降の「その他の確率分布」を参照）。round()は値を丸める関数で、以下では小数点以下の値を丸めて整数の値を出力するようにしている。 X = round(runif(n = 100, min = 1, max = 6),0) mean(X) ## [1] 3.5 それぞれの目が出る確率は1/6で一定である。すなわち、サイコロが出る目は一様分布に従う（つまり、元の分布は正規分布ではない）。一様分布の平均値は、最大値をa, 最小値をbとすると、(a+b)/2。つまり、サイコロの例の場合の平均値は理論的には(1+6)/2=3.5となる。 サイコロを100回振って平均値を求める。この平均値を求めるのを、1,000回繰り返す。求めた平均値1,000個の分布を見てみよう。 以下のプログラムで、シミュレーションとグラフの作成を行う。sapplyは、ある処理を繰り返し行う関数である。詳細な説明は省くが、以下のプログラムでは、100回サイコロを振って平均値を求める処理mean(round(runif(n = 100,min=1,max=6),0))を1,000回行ってその結果を保存して、sample.meansという名前のベクトルで保存している。 sample.means = sapply(c(1:1000), function(x) {mean(round(runif(n = 100,min=1,max=6),0))} ) hist(sample.means) ヒストグラムは正規分布に似ている。もっと回数を増やすと、より正規分布っぽいかたちになる。 #サイコロを7回振ってその合計を求める。これを10,000回行ったときの出目の合計値の分布 sample.sum = sapply(c(1:10000), function(x) {sum(round(runif(n = 7,min=1,max=6),0))} )#sum()はカッコ内のベクトルの要素を足し合わせる関数 hist(sample.sum) このように、元の母集団の分布がたとえ正規分布でなくても（この例の場合は一様分布）、その標本平均は正規分布に近似する。平均値を計算しなくても、単に値を足し合わせるだけでも同じある。 中心極限定理により、たとえ元の変数が正規分布に従っていなくても、変数を加算したものは正規分布に近似する。なので、心理学で行われる様々なデータ分析も、以下のような処理を行うことで変数が正規分布に従うという前提を置いて分析が行われる。 複数の質問項目（順序尺度）をまとめて平均化した心理尺度を分析に使う。 選挙への投票（「した」もしくは「しなかった」の二値）者の割合を県ごとに算出して、県を単位として分析に使う。 中心極限定理は簡単に言うと、「どのような確率分布に従う変数でも、足し合わせると正規分布に近づく」ということである。 ただし、中心極限定理が成り立たない（足し合わせても正規分布に近似しない）確率分布もある。詳しくは、付録の「確率分布」の「コーシー分布」の解説を参照のこと。 7.7 確率分布の表現の仕方 確率変数と確率分布の関係は、以下のように表現することもある。 二項分布の場合 \\[ x \\sim \\text{Binomial}(n, q) \\] Binomialのカッコ内のn, qのように、確率分布を構成する変数のことをパラメータ(parameter)と呼ぶ。 Binomialは二項分布、\\(\\sim\\)は「従う」という意味である。つまりこの式は、「\\(x\\)は、\\(n\\)と\\(q\\)をパラメータとする二項分布に従う」ということを示している。 正規分布の場合 \\[ x \\sim \\text{Normal}(\\mu, \\sigma) \\] 正規分布のパラメータは、平均\\(\\mu\\)と標準偏差\\(\\sigma\\)である。この2つが決まれば、正規分布の形状が決まる。 ポアソン分布の場合 \\[ x \\sim \\text{Poisson}(\\lambda) \\] ポアソン分布のパラメータは、平均及び分散を表す\\(\\lambda\\)のみである。 7.8 その他の確率分布 他にも、確率分布はたくさんある。ここでは、この章でも挙げている一様分布について解説する。その他、統計分析でよく使う確率分布については付録を参照のこと。 一様分布 冒頭の確率分布の説明の際に挙げた「1個のサイコロを振ったとき、それぞれの目が出る確率」は、どれも1/6で等しい。このように、どの確率変数\\(X\\)についても常に一定の値の確率を取る確率分布は、一様分布(uniform distribution)という。 一様分布は確率変数が離散値の場合もあれば、連続値の場合もある。サイコロの例は、離散型一様分布である。 一様分布を扱うRの関数(dunif, runifなど)は連続型一様分布を扱う関数である。 連続型一様分布を式で表すと以下のようになる（\\(min\\)は取りうる値の最小値、\\(max\\)は最大値を意味する。 \\[ f(x) = \\frac{1}{max - min}\\\\ x \\sim \\text{Uniform}(min, max)\\\\ \\] 例えば、以下は最小値が1、最大値が6の連続型一様分布である。以下のグラフは確率密度を示してあり、ある範囲の面積の合計がその範囲の値が生じる確率を意味する。 7.9 確認問題 問１ あなたは野球部の監督で、自分のチームの勝率はこれまでの練習の経験から32%だとわかっている。 これから遠征で、全部で10試合を行う予定である。 勝つ試合の回数をnとし、nとそれぞれのnに対応する確率を求めよ。 ヒント:dbinom()関数を使おう。 平均して何試合勝つことができるかを求めよ。 ヒント:二項分布の平均値の求め方を復習する。 問２ ある学校で小学6年生の身長を測ったところ、平均は150.2 cmで標準偏差が3.5 cmであった。 身長152 cmから155 cmの児童の割合はいくらか。 ヒント：pnorm()を使って求めよう。 身長158 cmを超える児童の割合はいくらか。 ヒント：同じく、pnorm()を使う。なお、全ての範囲の確率の合計は1である。 "],["07-NHT.html", "Chapter 8 統計的仮説検定 8.1 準備 8.2 統計的仮説検定の考え方 8.3 統計的仮説検定のまとめ 8.4 確認問題", " Chapter 8 統計的仮説検定 統計的仮説検定の考え方について理解する。 二項検定 t検定 帰無仮説と対立仮説 p値 8.1 準備 この章でも、ggplot2を使う。 library(ggplot2) 8.2 統計的仮説検定の考え方 8.2.1 二項検定 まずは、あるカテゴリーの割合が特定の値と等しいかあるいは異なるかを検定する二項検定（binomial test）を例として、統計的仮説検定の基礎となる「帰無仮説」、「対立仮説」及び「p値」の意味について理解していく。 ここでは、コインの表と裏が出る確率に偏りがないかを検討する実験を例として見ていく。コインを10回投げて表が出た回数\\(x\\)をカウントしていく。理論的には、表が\\(x\\)回出る確率\\(P(x)\\)は、コインを投げる回数\\(n\\)と表が出る確率\\(q\\)をパラメータとする二項分布に従う。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)}\\\\ x \\sim Binomial(n, q) \\] コインにゆがみがない、すなわち表と裏が出る確率に偏りがないならば、10回投げて表が\\(x\\)回出る確率は、\\(n=10\\), \\(q=0.5\\)をパラメータとする二項分布に従うはずである。図にすると以下の通りである。 plot = data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) ggplot2::ggplot() + ggplot2::geom_bar(data=plot, aes(x=factor(x), y=p), stat=&quot;identity&quot;) + labs(y = &quot;P(x)&quot;, x =&quot;x&quot;) + theme_classic() では、実際にコインを10回投げてみて表が出た回数を数えてみたところ、表が2回しか出なかったとする。この結果から、「このコインにはゆがみがあって、片一方の面だけが出やすい」と言ってもよいのか？ これを検討するために、表と裏それぞれが出る確率の等しいコインを投げる場合（すなわち、\\(q=0.5\\)の場合）との比較を行い、今回の実験結果がどれくらいまれな事象と言えるのかを比較する。 このとき、「コインの表が出る確率は0.5である」といった仮説のことを帰無仮説(null hypothesis)といい、「コインの表が出る確率は0.5ではない」といった帰無仮説に対する仮説のことを対立仮説（alternative hypothesis）と呼ぶ。帰無仮説は\\(H_{0}\\)、対立仮説は\\(H_{1}\\)と表すこともある。つまり、この例での帰無仮説は「\\(H_{0}:q=0.5\\)」、対立仮説は「\\(H_{1}:q \\neq 0.5\\)」となる。 では、今回の帰無仮説となる二項分布（2つのパラメータが、\\(n=10, q=0.5\\)の場合)の分布を見てみよう。理論的には、表が\\(x\\)回出る確率\\(P(x)\\)は、\\(x\\)それぞれについて以下のようになる。 d = data.frame(x=0:10, p_x =dbinom(x=0:10, size=10, prob=0.5)) d ## x p_x ## 1 0 0.0009765625 ## 2 1 0.0097656250 ## 3 2 0.0439453125 ## 4 3 0.1171875000 ## 5 4 0.2050781250 ## 6 5 0.2460937500 ## 7 6 0.2050781250 ## 8 7 0.1171875000 ## 9 8 0.0439453125 ## 10 9 0.0097656250 ## 11 10 0.0009765625 表もしくは裏が出る回数が2回以下の場合の確率を計算すると、 d$p_x[1] + d$p_x[2] + d$p_x[3] + d$p_x[9] + d$p_x[10] + d$p_x[11] ## [1] 0.109375 となる。つまり、もしゆがみのないコインならば、片一方の面だけが出る回数が2回以下の確率はおおよそ0.11ということになる。 この例で求めた確率0.11のように、「帰無仮説の前提のもとで、特定の結果よりもまれな結果が得られる確率」をp値と呼ぶ。 p = 0.11 は小さい確率のように思える。なので、「ゆがみのないコインならば、一方の面が2回出る確率は本来0.11である。本来だったらあまり起こり得ない実験結果が得られたので、このコインはゆがみのないコインである（\\(H_{0}: q = 0.5\\)）とは考えにくい。ゆえに、このコインにはゆがみがないという帰無仮説（\\(H_{0}: q = 0.5\\)）を棄て、ゆがみがあって片一方の面が出やすいコインであるとする対立仮説（\\(H_{1}: q \\neq 0.5\\)）を採択する」という結論は妥当なようにも思える。 しかし、人によって0.11を小さいと評価しても良いのか、基準が分かれる。そこで、研究者の間でどこまでの数値を小さいと評価するかの基準が決まっている。この基準となる確率が、有意水準 (significance level)である。 一般的に有意水準には0.05（5%）に設定されることが多い。なぜ5％を判断基準とするのかについては、特に明確な理由はない（みんなから合意されているからという以上の理由はない）。 つまり、「帰無仮説（ゆがみのないコインを投げる）の前提のもとでは、表が出る回数が2回以下の確率は0.11 であった。これは小さい確率のように思えるが、判断基準の5％よりかは大きい。すなわち、このコインはゆがみがない（\\(H_{0}:q = 0.5\\)）という仮説を棄てるわけにはいかない」ことになる。 この例のように「コインが表か裏かに関わらず、一方の面だけが出やすい」という対立仮説を検討する場合の検定は、両側検定という。仮に、今回の仮説で表と裏を区別するとして「表が出にくい」つまり「表が出る回数が2回以下の確率」を対象とする場合、このような検定を片側検定という。二項分布は左右対称の分布なので、両側p値は片側p値の2倍の値である(厳密には左右対称ではないのであくまで近似値)。多くの場合、両側検定を使うのが一般的である。 Rには、二項検定を行うための関数binom.test()が用意されている。binom.test()に二項分布のパラメータ（\\(n\\)と\\(q\\)にあたる数値）と実験結果を入れると、p値を求めてくれる。上の例について、binom.test()でp値を求めてみよう。 binom.test(x = 2, n = 10, p = 0.5) #出てくる結果はデフォルトで両側検定になる。 ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 8.2.2 二標本の検定 先ほどの例では「\\(H_{0}:q=0.5\\)」という帰無仮説を設定し、ある値が特定の値と等しいか異なるかを検定した。統計的仮説検定は、「2つの母集団の間である値に差があるかどうか」を検定するのに使われることも多い。 代表的な例としては、「2つの母集団の平均値の間に差があるか」を検討する二標本のt検定がある。 t検定の考え方も、基本的に同じである。2つの集団の間で平均値に差がないとする帰無仮説の理論分布（t分布）と比べて、実際に得られた差の値がどれくらい珍しいのかを検討する。 例えば、母集団Aと母集団Bの平均値をそれぞれ\\(\\mu_{A}\\)、\\(\\mu_{B}\\)とする。帰無仮説は「\\(H_{0}:\\mu_{A} - \\mu_{B} = 0\\)」、対立仮説は「\\(H_{1}: \\mu_{A} - \\mu_{B} \\neq 0\\)」である。 2つの集団の標本平均の差が帰無仮説のもとの理論分布（t分布）と比べて珍しいかを検討する。 以下のサンプルデータを使って、平均値の差の検定をしてみよう。まず、以下のプログラムを実行する。 set.seed(1) Value = c(rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 10, mean = 1, sd = 1)) Group = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample_data = data.frame(Group = Group, Value = Value) head(sample_data) ## Group Value ## 1 A -0.6264538 ## 2 A 0.1836433 ## 3 A -0.8356286 ## 4 A 1.5952808 ## 5 A 0.3295078 ## 6 A -0.8204684 AとBの２つの集団(Group)から、ある値（Value）を測定したとする。 まず、2つの条件別にValueの平均値や標準偏差を求める。 tapply(sample_data$Value, sample_data$Group, mean) #グループごとに平均を求める ## A B ## 0.1322028 1.2488450 tapply(sample_data$Value, sample_data$Group, sd) #グループごとに標準偏差を求める ## A B ## 0.780586 1.069515 集団Bの方が集団Aよりも平均値が大きいよう見えるが、そう結論づけて良いのか。これをt検定で検討しよう。 まず、2つの集団間の平均値の差を元に、以下の式から「t値」を求める。 \\[ t = \\frac{\\bar{x_{A}} - \\bar{x_{B}}}{\\sqrt{s^2_{A}/n_{A}+s^2_{B}/n_{B}))}} \\] \\(\\bar{x_{A}}\\)と\\(\\bar{x_{B}}\\)はそれぞれ集団Aと集団Bの平均値、\\(s^2_{A}\\)と\\(s^2_{B}\\)はそれぞれ集団AとBの分散、\\(n_{A}\\)と\\(n_{B}\\)はそれぞれ集団AとBのサンプルサイズ（標本数）である。 AとBが同じ正規分布\\(Normal(\\mu, \\sigma^2)\\)から抽出される場合、t値は自由度\\(n_{A}+n_{B}-2\\)のt分布に従う。 t分布は、自由度によって分布が変化する（サンプルサイズの大小に応じて理論分布を調整することができる）。 x = seq(-3, 3, 0.05) y_t2 = dt(x = x, df = 2) #自由度2のt分布 y_t5 = dt(x = x, df = 5) #自由度5のt分布 y_t20 = dt(x = x, df = 20) #自由度20のt分布 dat_t2 = data.frame(df = 2,x = x, y = y_t2) dat_t5 = data.frame(df = 5,x = x, y = y_t5) dat_t20 = data.frame(df = 20,x = x, y = y_t20) dat_t = rbind(dat_t2, dat_t5, dat_t20) p = ggplot2::ggplot() + ggplot2::geom_line(data = dat_t, aes(x = x, y = y, color = factor(df))) + ggplot2::labs(x = &quot;t&quot;, y = &quot;value&quot;, color = &quot;Degree of freedom&quot;) + ggplot2::theme_classic() p 標本から得た差のt値が理論分布のどこに位置するかを検討する。 Rに入っているt.test()関数を使うことで、２つの集団の間の平均値の差の検定を行える。 t.test(data = sample_data, Value ~ Group) #dataにデータの名前、比較の対象となる変数~グループを意味する変数とうかたちで入力すると結果が出力される。 ## ## Welch Two Sample t-test ## ## data: Value by Group ## t = -2.6669, df = 16.469, p-value = 0.01658 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -2.0022169 -0.2310675 ## sample estimates: ## mean in group A mean in group B ## 0.1322028 1.2488450 p値は0.02であった。これは5%よりも小さいので、今回の結果が生じる確率はまれであり、AとBの母集団の平均値の間に差はないとする帰無仮説（\\(H_{0}:\\mu_{A}-\\mu_{B}=0\\)）を棄却し、AとBの母集団の平均値は異なる（\\(H_{1}:\\mu_{A}-\\mu_{B}\\neq0\\)）という対立仮説を採用することとなる。対立仮説が採択されたことで母集団AとBの平均値の間には大きな差があることが示され、このような結果は「有意差(statistically significant difference)がある」と表現されることが多い。 t検定には、2つの標本の母集団の分散が等しいと仮定するかしないかで二種類の検定がある。母集団の分散が等しいと仮定しない場合の検定はウェルチの検定(Welch’s t-test)と呼ばれ、Rのt.test()関数でデフォルトで出る検定結果はこのウェルチの検定による結果である。一般的に2つの標本の母分散は不明であるので、それらが等しいかどうかも不明である。なので、等分散を仮定しないt検定をしておくほうが保守的である。 8.3 統計的仮説検定のまとめ まとめると、 「差がない（あるいは偏りがない）」とする帰無仮説と「差がある（あるいは偏りがある）」とする対立仮説を立てる。 帰無仮説を前提とする理論分布のもとで、今回の結果よりも珍しい結果が生じる確率（p値）を求め、 その確率（p値）が有意水準よりも小さいかを評価し、 p値が有意水準よりも小さい場合は、帰無仮説を棄却して対立仮説を採択し、 p値が有意水準以上の場合は、帰無仮説を棄却しない というのが、統計的仮説検定のプロセスである。 p値が有意水準以上の場合、「帰無仮説を支持する、帰無仮説が正しいと結論づける」のではなく、「帰無仮説を棄却しない」という表現であることに注意。 統計的仮説検定では、背理法と同様の考え方が取られている。 帰無仮説が棄却されるか（帰無仮説が正しくなく、対立仮説が正しいか）を検討したい。 「帰無仮説が正しい」という前提のもとで今回の結果が得られるかを検討する。 「帰無仮説が正しい」という前提では矛盾が生じる（帰無仮説の前提のもとでは今回の結果はほとんど生じ得ない）。 「帰無仮説は正しい」を棄却し、「対立仮説が正しい」という結論を導く。 しかし、3)の段階で矛盾が生じなかったとしても、「帰無仮説が正しい」ことを積極的に示すことにはならない。統計的仮説検定では「帰無仮説が棄却できるか」を検討しているのであって、棄却されないからといって帰無仮説を正しいという結論を導くことはできない。p値が有意水準以上だった場合には、「帰無仮説が正しいか誤っているかの結論は今回の検定の結果からは出せないので、判断を保留する」という結論になる。 また、次の章の第1種の過誤の説明にもあるように、p値が有意水準未満であっても、必ずしも「対立仮説が正しい」とも限らない。 8.4 確認問題 以下のプログラムを読み込む。 ある教授法に児童の学力向上の効果があるかを検討した。学校Bにはその教授法を実施し、学校Aには何もしなかった。その後、学校Aと学校Bそれぞれ10人の生徒に学力テストを行った。A、Bそれぞれが学校A、Bそれぞれの生徒の成績である（架空のデータである）。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(48, 40, 43, 56, 69, 53, 47, 41, 42, 91) Value = c(A, B) Treatment = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample = data.frame(Treatment = Treatment, Value = Value) str(sample) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Treatment: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Value : num 38 53 61 27 54 55 44 45 44 41 ... 学校Aと学校Bそれぞれについて、テストの得点の平均値及び標準偏差を求めて報告せよ。 この教授法に成績向上があったかどうかについてt検定（等分散を仮定しない）で検討し、結果について報告するとともに結論を述べよ。 ※t.test()関数を使う。等分散を仮定しない検定の場合は、特にオプションをしていしないでもよい。 "],["08-NHT_problem.html", "Chapter 9 統計的仮説検定が抱える問題 9.1 準備 9.2 シミュレーション 9.3 p値とサンプルサイズの関係 9.4 効果量（差の大きさとp値の関係） 9.5 第1種の過誤と第2種の過誤 9.6 検出力 9.7 検出力分析 9.8 p値と第1種の過誤 9.9 多重比較の問題 9.10 まとめ", " Chapter 9 統計的仮説検定が抱える問題 統計的仮説検定が抱える問題について理解する。 p値とサンプルサイズの関係 効果量 第1種の過誤、第2種の過誤 検出力 この章では乱数を用いたシミュレーションを通して、サンプルサイズ、効果量、p値のそれぞれの関係について理解していく。 9.1 準備 この章では、ggplot2、pwrパッケージを使う。 library(ggplot2) library(pwr) 9.2 シミュレーション 正規分布を母集団とする２つのグループA, Bがあるとする。グループAの正規分布は平均\\(\\mu_{A}\\)、標準偏差\\(\\sigma_{A}\\)、グループBの正規分布は平均\\(\\mu_{B}\\)、標準偏差\\(\\sigma_{B}\\)とする。それぞれのグループから\\(n\\)個のサンプルを抽出するとする。 rnorm()関数を使って、正規分布から乱数を生成する。以下は、平均及び標準偏差が同じ(\\(\\mu_{A} = \\mu_{B} = 0, \\sigma_{A} = \\sigma_{B} = 0\\))グループAとBから20個ずつサンプルを抽出したシミュレーションの例である。 set.seed(1) #乱数の種の設定。これに任意の数値を設定しておけば同じ結果が再現できる。 N = 20 g_A = rnorm(n = N, mean = 0, sd = 1) g_B = rnorm(n = N, mean = 0, sd = 1) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) #データフレームdとして保存 head(d) ## group value ## 1 A -0.6264538 ## 2 A 0.1836433 ## 3 A -0.8356286 ## 4 A 1.5952808 ## 5 A 0.3295078 ## 6 A -0.8204684 t.test(data = d, value ~ group) #グループの間でvalueの平均値に差があるかをt検定で検定 ## ## Welch Two Sample t-test ## ## data: value by group ## t = 0.69794, df = 37.917, p-value = 0.4895 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.3744327 0.7684235 ## sample estimates: ## mean in group A mean in group B ## 0.190523876 -0.006471519 9.2.1 関数の作成 上記のプログラムを毎回書き直すのも大変なので、関数(function)を作成して手順を簡略化する。 グループA, Bのサンプルサイズ、平均、標準偏差を任意のものに設定して上記の一連のプログラムをできるように、関数simulation_pを自作する。以下のプログラムを全て実行する。 実行すると、RStudioの右ウィンドウ「Environmment」の「Functions」に、simulation_pが表示される。 simulation_p = function(N, mu_A, sigma_A, mu_B, sigma_B){ g_A = rnorm(n = N, mean = mu_A, sd = sigma_A) g_B = rnorm(n = N, mean = mu_B, sd = sigma_B) d = data.frame(group = c(rep(&quot;A&quot;, N), rep(&quot;B&quot;, N)), value = c(g_A, g_B)) result = t.test(data = d, value ~ group) #t検定の結果 return(result) } 以下はグループAの母集団の平均値を0かつ標準偏差を1(\\(\\mu_{A} = 0, \\sigma_{A} = 1\\))、 グループBの母集団の平均値を1かつ標準偏差を1(\\(\\mu_{B} = 1, \\sigma_{A} = 1\\))として、それぞれのグループから100個(\\(n=100\\))のサンプルを抽出し、t検定を行った結果である。 set.seed(1) simulation_p(N = 100, mu_A = 0, sigma_A = 1, mu_B = 1, sigma_B = 1) ## ## Welch Two Sample t-test ## ## data: value by group ## t = -6.4983, df = 197.19, p-value = 6.492e-10 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -1.1122615 -0.5943477 ## sample estimates: ## mean in group A mean in group B ## 0.1088874 0.9621919 以降では、この関数を使ってサンプルサイズやグループAとBの平均値の差を変えてシミュレーションを行い、p値がどう変化するかを見ていく。 9.3 p値とサンプルサイズの関係 p値はサンプルサイズ（標本数）に依存する。サンプルサイズが多くなるほどp値は小さくなる。 例えば、平均0, 標準偏差1の正規分布に従う母集団Aと平均0.2, 標準偏差1の正規分布に従う母集団Bからそれぞれ標本を抽出し、AとBの間で平均値に差があるかを検討する。帰無仮説\\(H_{0}:\\mu_{A} - \\mu_{B} = 0\\)が棄却されるかをt検定で検定する。 実際の母集団の真の平均値の差は\\(|0-0.2|=0.2\\)である。 図で表してみると、分布はほとんど重なりあっていて2つのグループの平均値の間にあまり差がなさそうに見える。 先ほど作成したsimulation_p関数を使って、シミュレーションをしてみよう。まずは、それぞれのグループから100個サンプルを抽出してt検定を行ってみる。 set.seed(1) result = simulation_p(N = 100, mu_A = 0, sigma_A = 1, mu_B = 0.2, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -0.40594, df = 197.19, p-value = 0.6852 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.3122615 0.2056523 ## sample estimates: ## mean in group A mean in group B ## 0.1088874 0.1621919 p値はround(result$p.value, 3)であり、有意水準5%とすると有意な差があるという結論は出ない。つまり、10人ずつ標本を抽出したこの結果からは、帰無仮説\\(H_{0}:\\mu_{A} - \\mu_{B}=0\\)を棄却することはできない。 次に、同じ平均値の差で、各グループそれぞれ500個, 500個標本を抽出してt検定をしたシミュレーション結果を見てみよう。 set.seed(1) result = simulation_p(N = 500, mu_A = 0, sigma_A = 1, mu_B = 0.2, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -2.0079, df = 996.09, p-value = 0.04493 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.259852446 -0.002978632 ## sample estimates: ## mean in group A mean in group B ## 0.02264409 0.15405963 今度は、p値が0.05よりも小さい。500人ずつ標本を抽出したこの結果からは、帰無仮説\\(H_{0}:\\mu_{A} - \\mu_{B}=0\\)は棄却され、AとBとの間に平均値に有意な差があるという結論が導かれる。 更に、1,000個ずつサンプルしてt検定をした結果もみてみよう。 set.seed(1) result = simulation_p(N = 1000, mu_A = 0, sigma_A = 1, mu_B = 0.2, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -4.2113, df = 1998, p-value = 2.652e-05 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.2863763 -0.1043961 ## sample estimates: ## mean in group A mean in group B ## -0.01164814 0.18373809 p値は更に小さくなり、AとBとの間に平均値に有意な差があるという結論が導かれる。 しかし、サンプルサイズが100個、500個、1,000個いずれのシミュレーションも、母集団の平均値差は0.2である。 このように、p値はサンプルサイズが大きくなるほど小さくなるという性質がある。実質的にあまり大きくない差でも、サンプルを多く取れば「有意な差がある」と結論が出てしまう可能性がある。 異なるグループから標本を抽出したのならば、平均値に差が存在しないということはありえない。どんなに小さくても、差は存在する(差の大きさが0.00001でも)。小さい標本では、わずかな差は誤差として評価されて「有意差がある」という結論は導かれにくい。しかし、サンプルサイズを大きくすることで差を検出しやすくなる。 すなわち、p値が示しているのは差の大きさ（効果の大きさ）ではない。 9.4 効果量（差の大きさとp値の関係） そこで、平均値の差のように、効果の大きさそのものを表す指標が別にある。効果量（effect size）と呼ばれるものである。効果量には様々な種類が提案されている。 例えば、2グループ間の差を表す効果量としてCohenのd (Cohen’s d)という指標がある。Cohenの dは，以下の式で計算される。\\(n\\)はそれぞれのグループ（ここでは、AもしくはB）のサンプルサイズ、\\(\\bar{x}\\)はそれぞれのグループの標本平均，\\(s_{A}^2\\)と\\(s_{B}^2\\)はそれぞれ2群の不偏分散とする。\\(s\\)は2群を合成した上での標準偏差を意味する。 \\[ d = \\frac{|\\bar{x_{A}} - \\bar{x_{B}|}}{s} \\\\ s = \\sqrt{\\frac{(n_{A}-1)s_{A}^2 + (n_{B}-1)s_{B}^2}{n_{A}+n_{B}}} \\] つまり、Cohenのdは２つのグループの平均値の差を標準偏差で調整したものである。一般的に、\\(d=0.2\\)は小さい効果、\\(d=0.5\\)が中程度の効果、\\(d=0.8\\)が大きい効果として理解されている。このように、指標によって効果の大きさの基準が決まっている。 以下にsimulation_pを使って、サンプルサイズを100個で固定したうえで、差の効果量(d)を変化させたシミュレーションを行う。 set.seed(1) result = simulation_p(N = 100, mu_A = 0, sigma_A = 1, mu_B = 0.2, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -0.40594, df = 197.19, p-value = 0.6852 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.3122615 0.2056523 ## sample estimates: ## mean in group A mean in group B ## 0.1088874 0.1621919 result = simulation_p(N = 100, mu_A = 0, sigma_A = 1, mu_B = 0.5, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -3.6425, df = 197.65, p-value = 0.0003449 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.8044964 -0.2393602 ## sample estimates: ## mean in group A mean in group B ## 0.02967354 0.55160186 result = simulation_p(N = 100, mu_A = 0, sigma_A = 1, mu_B = 0.8, sigma_B = 1 ) result ## ## Welch Two Sample t-test ## ## data: value by group ## t = -5.2378, df = 191.19, p-value = 4.265e-07 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -1.0938491 -0.4953807 ## sample estimates: ## mean in group A mean in group B ## -0.03913424 0.75548064 効果量が大きいほど、同じサンプルサイズでも有意な差が検出しやすくなる。 他にも、効果量には様々な種類がある。相関係数も係数の値が２変数間の関連の大きさを表すので、効果量の仲間である。 9.5 第1種の過誤と第2種の過誤 例えば、２つの集団AとBの間で平均値に差があるかどうかを検定する。帰無仮説\\(H_{0}\\)（母集団AとBの平均は等しい）と対立仮説\\(H_{1}\\)（母集団AとBの平均の間に差がある）を以下のように表す。 \\[ H_{0}: \\mu_{A} - \\mu_{B}=0\\\\ H_{1}: \\mu_{A} - \\mu_{B}\\neq0\\\\ \\] 「帰無仮説が真なのに、帰無仮説を棄却してしまう（本当は差がないのに、“差がある”と判断してしまう）」の誤りのことを、第1種の過誤（type Ⅰ error）と呼ぶ。 これに対し、「帰無仮説が偽なのに、帰無仮説を棄却しない（本当は差があるのに、“差がない”と判断してしまう）」誤りのことを、第2種の過誤（type Ⅱ error）と呼ぶ。 第1種の過誤を犯す確率\\(\\alpha\\)は、要は有意水準として設定した値そのものである（\\(\\alpha=0.05\\)）。有意水準を高くする、すなわち「差があると判断する基準をゆるく」してしまうとより帰無仮説を棄却しやすくなってしまう。それは同時に、「帰無仮説を誤って棄却してしまう」可能性を高めてしまう。 第2種の過誤を犯す確率は、\\(\\beta\\)と表現される。 それぞれの関係をまとめると、以下のように表現できる。 例えば、以下に帰無仮説と対立仮説との関係のイメージを図で示している。２つの集団の母集団の平均値の差の効果量を\\(d\\)とし、帰無仮説\\(H_{0}: d = 0\\)と対立仮説\\(H_{1}: d &gt; 0\\)とした場合の分布を示している（単純化のため片側検定とする）。 差\\(d\\)が赤い部分に当てはまるときは、帰無仮説を棄却することとなる。同時に、赤い部分は第1種の過誤を犯す確率\\(\\alpha\\)を意味する。逆に、差\\(d\\)が青い部分に含まれるときは、対立仮説の分布にも含まれているにも関わらず帰無仮説を棄却しないことを意味する。すなわち、青い部分が第２種の過誤を犯す確率\\(\\beta\\)を意味する。 第1種の過誤と第2種の過誤はトレード・オフの関係にある。第1種の過誤を避けようとして有意水準を小さくすれば帰無仮説の棄却が厳しくなり、逆に第2種のエラーを犯してしまう確率も高くなる（帰無仮説が偽であるにもかかわらず、棄却しない）。 例えば、以下の左図が\\(\\alpha = 0.05\\)のとき、右図が\\(\\alpha = 0.01\\)の場合である。右図は左図と比べて赤い部分（\\(\\alpha\\)）が小さくなった一方、青い部分（\\(\\beta\\)）が大きくなっているのがわかる。 以下の図は、第1種の過誤\\(\\alpha\\)（有意水準）と第2種の過誤\\(\\beta\\)との関係を示したものである。\\(d\\)は2グループ間の差の効果量を意味する。それぞれのグループのサンプルサイズは50としている（縦の赤い点線が\\(\\alpha = 0.05\\)の場合）。\\(\\alpha\\)と\\(\\beta\\)は一方が増えれば、もう一方が減る関係にあることがわかる。 9.6 検出力 帰無仮説が偽（対立仮説が正しい）のときに帰無仮説を棄却するのが、正しい判断である。さきほどの表でも示しているように、このときの確率は\\(1 - \\beta\\)で表すことができる。この\\(1 - \\beta\\)は検出力(power)と呼ばれる。以下の図でいうと、対立仮説の分布全体から、青い部分（\\(\\beta\\)）を差し引いたところが検出力を示している。 検出力とは、「実際に差があるときに、“差がある”と正しく判断できる確率」である。統計的仮説検定では、この検出力をいかに高くするかが重要となる。 9.6.1 検出力に影響する要因 検出力は、サンプルサイズが大きいほど、または効果量が大きいほど上昇する。 以下の図は、サンプルサイズを変えたときの検出力の変化を示したものである。右図は、左図よりも2つの集団A, Bそれぞれのサンプルサイズを大きくした場合である。サンプルサイズを大きくすると分布が狭くなる（標準誤差が小さくなる）ので、差を検出しやすくなる。 以下の図は、効果量を変えたときの検出力の変化を示したものである。右図は、左図よりも母集団AとBとの差（効果量）が大きい場合である。効果量が大きいほど、統計的仮説検定で有意な差を検出しやすくなる。 以下の図は、サンプルサイズ、効果量及び検出力の関係をまとめた図である。有意水準\\(\\alpha = 0.05\\)としている（横の赤い点線が、一般的に必要とされる検出力\\(\\beta = 0.80\\)である）。サンプルサイズが増えるほど検出力は上昇し、効果量が大きいほど検出力も上昇する。 9.7 検出力分析 これまで見てきたように、サンプルサイズ、p値、検出力、効果量はそれぞれ関わり合っている。 サンプルサイズが増えれば、p値は小さくなる。 サンプルサイズが増えれば、検出力は上昇する。 効果量が大きければ、検出力は上昇する。 事前に効果量、有意水準、検出力を決めておけば、有意な差を検出するために最低限必要なサンプルサイズを求めることができる。このようなサンプルサイズの設計方法は、事前の検出力分析（prior power analysis）という。 ＊効果量は事前に知ることはできないが（研究によって調べたいことそのものなので）、先行研究からこれくらいだろうという予想あるいはこのくらいの大きさを検出したいという期待によって決める。 また、データの取得後に、取ったサンプルサイズ、明らかになった効果量及び有意水準から、検出力を求めることができる。これを、事後の検出力分析（post-hoc power analysis）という。 pwrパッケージに、検出力分析をするための関数がいくつか用意されている。 #事前の検出力分析（サンプルサイズの設計） #2群間の差をt検定で検定する場合 #各群のサンプルサイズ(n)，効果量（d: Cohen&#39;s d），有意水準（sig.level），検出力（power）のどれか３つを入れると，入れなかったものの結果が出力される。 pwr::pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, n = NULL) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group #事後の検出力分析 #２群それぞれのサンプルサイズ(n1, n2)，効果量(d), 有意水準(sig.level)を入れると，検出力が求められる。 pwr::pwr.t2n.test(n1 = 40, n2 = 40, d = 0.8, sig.level = 0.05, power = NULL) ## ## t test power calculation ## ## n1 = 40 ## n2 = 40 ## d = 0.8 ## sig.level = 0.05 ## power = 0.9421818 ## alternative = two.sided 9.8 p値と第1種の過誤 今度は、同じ平均0、標準偏差1の正規分布に従う母集団AとBから1000個ずつ標本を抽出しグループAとBの間で平均値に差があるかをt検定で検定する。帰無仮説は\\(H_{0}:\\mu_{A} = \\mu_{B}\\)であり、母集団の真の平均値差もゼロ（母集団AとBは全く同じ分布）なので帰無仮説が正しい。 ここで、帰無仮説が正しい前提のもとで抽出された標本についてt検定を行い、統計的仮説検定によって正しい結論（帰無仮説を棄却しない）が導かれるかをシミュレーションしてみる。有意水準は、5%（0.05）とする。この章の最初で作成したsimulation_p関数を使ってシミュレーションを行う。 set.seed(1) result = simulation_p(N = 1000, mu_A = 0, sigma_A = 1, mu_B = 0, sigma_B = 1) result$p.value #p値を取り出す ## [1] 0.9207967 \\(p \\geq .05\\)であり、帰無仮説は棄却されない。 では、このシミュレーションを1,000回やってみる。以下のプログラムを実行してみよう。 set.seed(1) p = NULL for(i in 1:1000){ result = simulation_p(N = 1000, mu_A = 0, sigma_A = 1, mu_B = 0, sigma_B = 1) p = c(p, result$p.value) } head(p) #1,000回のシミュレーションで得られたp値をpという名前で保存した。head()で最初の数個のみを表示する。 ## [1] 0.9207967 0.9756433 0.8493272 0.7180123 0.4981026 0.4375691 1,000回のシミュレーション中、\\(p&lt;.05\\)が得られた結果をカウントする。 significant = ifelse(p &lt; 0.05, &quot;significant&quot;, &quot;insignificant&quot;) #p&lt;0.05ならば&quot;significant（有意）&quot;, ちがうのならば&quot;insignificant（非有意）&quot;とする table(significant) #有意、非有意の数を集計 ## significant ## insignificant significant ## 952 48 帰無仮説が真である（母集団の間で差がない）にも関わらず、標本抽出してt検定をするのを1,000回行うと、\\(p&lt;.05\\)となって帰無仮説を誤って棄却する結果が1,000回中ras.numeric(table(significant)[2])`回確認された。これは約5%で、有意水準5%と近似している。 有意水準を5%に設定することはすなわち、全ての検定のうちの5%（1,000回中約50回）については誤って帰無仮説を棄却することを許してしまうのである（有意水準が第1種の過誤の確率そのものであることの理由）。有意水準は慣例的に0.05と設定されているが、上記のシミュレーションのようにたまたま有意な結果が得られて間違った結論を導いてしまう恐れが存在する。 これを利用して、本当は差がないにもかかわらず「有意な差がある結果」を導くこともできる。pハッキング(p-hacking)とも表現される。 たくさん実験を行って，有意な差が出た結果だけを報告する。 たくさんの質問項目について個別に検定を行い、有意な結果だけについて議論する。 参加者を少しずつ追加して、追加した度に検定を行い、有意差が出たら追加するのをやめる。 9.9 多重比較の問題 上述のシミュレーションのように、統計的仮説検定を繰り返すほど、差がなくても差があると評価してしまう確率（第1種の過誤を犯す確率）は増える。 検定をn回行ったときに、「1回でも帰無仮説を誤って棄却してしまう確率」は以下の式で表すことができる（1から全て正しく帰無仮説を棄却しなかった確率を引く）。 \\[ p(\\text{Type 1 Error}) = 1 - (1 -\\alpha)^n \\] 例えば、5%水準で10回検定を行えば、少なくとも1回は帰無仮説を誤って棄却してしまう確率が0.4 になる。 1 - (1 - 0.05)^10 #全ての確率から、「10回検定を行って全て正しい判断を行う」確率を差し引いたものが、「少なくとも1回は誤った判断をしてしまう確率」 ## [1] 0.4012631 このように複数回検定を行うことを多重比較という。多重比較を行うと、第1種の過誤を犯す確率が上昇する。以下は、検定の回数と「少なくとも1回は第1種の過誤を犯してしまう確率」との関係を有意水準\\(\\alpha\\)別に示したものである。 図からも、有意水準を厳しく設定すれば、第1種の過誤を回避することができる。そこで、分散分析で3つ以上の条件間で平均値を比較するときなどのように検定を複数行う場合には、有意水準を通常よりも厳し目に調整する多重比較の補正がなされる。 alpha = 0.05 x = 1:20 m_c = function(x = 1:20, alpha = 0.05){ p = 1 - (1 - alpha)^x return(p) } d = rbind( data.frame(x = 1:20, p = m_c(alpha = 0.001), alpha = 0.001), data.frame(x = 1:20, p = m_c(alpha = 0.01), alpha = 0.01), data.frame(x = 1:20, p = m_c(alpha = 0.05), alpha = 0.05) ) p = ggplot() + geom_line(data = d, aes(x = x, y = p, linetype = factor(alpha), color = factor(alpha))) + labs(x = &quot;number of comparisons&quot;, y = &quot;Type 1 error&quot;, linetype = &quot;alpha&quot;, color = &quot;alpha&quot;) + theme_classic() p 9.10 まとめ この章では統計的仮説検定の結果を解釈するうえで注意すべき点について挙げてきた。 以下に要点をまとめる。 サンプルサイズが大きくなるほど、同じ差でも小さいp値が出やすくなる。 p値は効果（差）そのものを表す指標ではない。効果の大きさを表す「効果量」という指標が別にある。 有意水準は、第１種の過誤（本当は差がないのに、「有意差がある」と判断してしまうこと）を犯す確率と等しい。しかし、第１種の過誤を避けるために有意水準を小さくすると、第２種の過誤（本当は差があるのに、「有意差がない」と判断してしまうこと）も犯しやすくなる。 正しく差を検出する確率を「検出力」と呼ぶ。効果量が大きいほど差を検出しやすくなるのはもちろんのこと、サンプルサイズが大きければ差を検出しやすくなる。 サンプルサイズ、効果量、有意水準、検出力はそれぞれ密接に関わっている。すなわち、効果量、有意水準、検出力があらかじめ決まっていれば、有意な差を検出するために最低限必要なサンプルサイズが求まる。このように必要なサンプルサイズを計算することを、検出力分析と呼ぶ。 "],["09-analysis.html", "Chapter 10 様々な解析法 10.1 t検定 10.2 分散分析 10.3 ノンパラメトリック検定 10.4 「統計モデル」との関係", " Chapter 10 様々な解析法 この章では、t検定、χ二乗検定、分散分析、ノンパラメトリック検定など、心理学の基礎統計で学んだ手法について、Rでの解析方法を見ながらおさらいしていく。 まず、この章で使うパッケージをロードする。 library(dplyr) library(tidyr) 10.1 t検定 「分析の対象が量的変数で、２つのグループの間でその変数の平均値を比較する」ときには、t検定を使う。更に、t検定には2つのグループに対応があるかないかで、「対応のあるt検定」と「対応のないt検定」で区別される。 前の章でもみてきたように、Rにはt検定を行うためのt.test()関数が用意されている。 10.1.1 対応のないt検定 Rに標準で入っているsleepデータを使って、Rでt検定をやってみよう。 以下のプログラムを読み込み、サンプルデータを作る。 sleep_1 = sleep |&gt; dplyr::select(-ID) sleep_1$ID = 1:nrow(sleep_1) sleep_1 ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 11 ## 12 0.8 2 12 ## 13 1.1 2 13 ## 14 0.1 2 14 ## 15 -0.1 2 15 ## 16 4.4 2 16 ## 17 5.5 2 17 ## 18 1.6 2 18 ## 19 4.6 2 19 ## 20 3.4 2 20 IDは参加者を意味する番号で、1から20までの人がグループ1かグループ2のどれかに属し、変数extraを測定したとする。グループの間でextraに違いがあるかどうかを検討したい。 このように参加者が２つのグループのうちどれか一つに属しているケースが「対応のない場合」で、この場合は対応のないt検定で検討する。 前の章で見たように、t.test()関数で以下のように入力すれば結果が出力される。 t.test(data = sleep_1, extra~group) ## ## Welch Two Sample t-test ## ## data: extra by group ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.75 2.33 10.1.2 対応のあるt検定 同じく、sleepデータを使って対応のある場合について解析をしてみる。 sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 今度は10名の参加者が、グループ1とグループ2の両方に属して、それぞれで変数extraを測定したとする。 このように同じ参加者が２つのグループの両方に属しているケースが「対応のある場合」である。 データを横並び（wide型）にしてから、t.test()関数で以下のようにプログラムを書けば、対応のあるt検定を実施できる。オプションにpaired = TRUEを指定する。 sleep_2 = sleep |&gt; tidyr::pivot_wider(names_from = group, values_from = extra) |&gt; dplyr::rename(group_1 = `1`, group_2 = `2`) head(sleep_2) ## # A tibble: 6 × 3 ## ID group_1 group_2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 t.test(sleep_2$group_1, sleep_2$group_2, paired = TRUE) ## ## Paired t-test ## ## data: sleep_2$group_1 and sleep_2$group_2 ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean difference ## -1.58 10.2 分散分析 t検定で比較できるのは2つのグループの間の平均値である。3グループ以上の間で平均値の比較を行いたい場合は、分散分析（ANOVA）を行う。 ここでは、一要因３水準の分散分析（3つのグループの間で平均値を比較する）を例として、Rでの解析法について説明する。 Rでは、一要因の分散分析をするための関数aov()が標準で入っている。同じくRで標準で入っているPlantGrowthデータを使って解析をしてみよう。 anova_sample = PlantGrowth #PlantGrowthをanova_sampleという名前で保存する anova_sample ## weight group ## 1 4.17 ctrl ## 2 5.58 ctrl ## 3 5.18 ctrl ## 4 6.11 ctrl ## 5 4.50 ctrl ## 6 4.61 ctrl ## 7 5.17 ctrl ## 8 4.53 ctrl ## 9 5.33 ctrl ## 10 5.14 ctrl ## 11 4.81 trt1 ## 12 4.17 trt1 ## 13 4.41 trt1 ## 14 3.59 trt1 ## 15 5.87 trt1 ## 16 3.83 trt1 ## 17 6.03 trt1 ## 18 4.89 trt1 ## 19 4.32 trt1 ## 20 4.69 trt1 ## 21 6.31 trt2 ## 22 5.12 trt2 ## 23 5.54 trt2 ## 24 5.50 trt2 ## 25 5.37 trt2 ## 26 5.29 trt2 ## 27 4.92 trt2 ## 28 6.15 trt2 ## 29 5.80 trt2 ## 30 5.26 trt2 植物の生長を3つの条件で調べたデータである。 まず、３つのグループごとに平均値や標準偏差を確認しよう。 anova_sample |&gt; dplyr::group_by(group) |&gt; dplyr::summarise(Mean = mean(weight), SD = sd(weight), N = length(weight)) ## # A tibble: 3 × 4 ## group Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 ctrl 5.03 0.583 10 ## 2 trt1 4.66 0.794 10 ## 3 trt2 5.53 0.443 10 ctrl、trt1、trt2の間で平均値に差があるかを一要因の分散分析で検討する。以下のようにプログラムを書く。 result = aov(data = anova_sample, weight ~ group) summary(result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 3.766 1.8832 4.846 0.0159 * ## Residuals 27 10.492 0.3886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary()を使うと分散分析表が出力され、変数の効果が有意かを検討できる。 または、一要因の分散分析ならばoneway.test()でも可能である。 oneway.test(data = anova_sample, weight ~ group, var.equal = TRUE)#等分散の仮定のオプションを加える ## ## One-way analysis of means ## ## data: weight and group ## F = 4.8461, num df = 2, denom df = 27, p-value = 0.01591 一要因の分散分析では「グループの間で平均値に差がない」という帰無仮説を検討する。この例については、p値は0.0159であり、5%水準で帰無仮説は棄却されることとなる。「グループの間で平均値に差がない」という可能性は棄却されたが、どのグループの間に有意な差があるかはわからない。そこで、グループ１とグループ２、グループ２とグループ３、グループ１とグループ３との間、計３つの組み合わせで平均値の比較を行う。つまり、t検定を3回行って条件間の比較をする。 検定を繰り返すことは第１種の過誤を犯す確率を高めてしまう。そこで、検定を行う回数に応じてp値を厳し目に見積もることで、第１種の過誤を犯す確率を低くする工夫がなされる。この工夫が、「多重比較補正」と呼ばれるものである。 多重比較の補正を行うときは、pairwise.t.test関数を使う。各群間の比較について、補正後のp値が出力される。 Rでは、多重比較補正を行うための関数として、pairwise.t.test()がある。 pairwise.t.test(anova_sample$weight, g = anova_sample$group, p.adjust.method = &quot;bonferroni&quot;) #gにグループを意味する変数、p.adjust.methodに補正方法を指定する。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: anova_sample$weight and anova_sample$group ## ## ctrl trt1 ## trt1 0.583 - ## trt2 0.263 0.013 ## ## P value adjustment method: bonferroni グループの組み合わせごとに、補正後のp値が表示される。これを見ると、trt1とtrt2との間で5%水準で有意な差があることがわかる。 多重比較補正には他にも、ボンフェローニ（Bonferroni)、チューキー(Tukey)、ホルム(Holm)の方法など様々な補正方法が提案されている。 チューキーによる補正の結果は、以下のプログラムで行う。 result = aov(data = anova_sample, weight ~ group) TukeyHSD(result) #分散分析の結果をTukeyHSDに入れる。 ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight ~ group, data = anova_sample) ## ## $group ## diff lwr upr p adj ## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711 ## trt2-ctrl 0.494 -0.1972161 1.1852161 0.1979960 ## trt2-trt1 0.865 0.1737839 1.5562161 0.0120064 分散分析は選択肢も多く、非常に複雑な解析方法である。二要因の分散分析、三要因の分散分析、更には二要因対応あり一要因対応なしの分散分析と、要因の数やそれぞれの要因の対応ありなしで分散分析のやり方も非常に複雑になる。 更には、平方和の値の計算方法もタイプ1, タイプ2、タイプ3と様々な選択肢がある。 10.3 ノンパラメトリック検定 以上のt検定や分散分析は、分析の対象の変数が「正規分布に従う」を前提とする解析手法である。変数が正規分布に従わない変数、例えば質的変数（人数の比率、順序尺度など）の場合には、t検定や分散分析を用いるのは適切でなく、「ノンパラメトリック検定」を使う。 ノンパラメトリック検定とは「変数が正規分布に従うという前提を置かない解析手法」の総称であり、様々な種類が提案されている。 以下では、ウィルコクソンの順位和検定、クラスカル・ウォリスの検定、カイ二乗検定について触れる。 10.3.1 ウィルコクソンの順位和検定 2群間で値を比較するノンパラメトリック検定である。データを順位データに変換し、2群間でデータの大きさを比較する。「マン・ホイットニーのU検定」という名前でも知られる。 Rではwilcox.test()が用意されている。 wilcox_sample = airquality |&gt; filter(Month &gt;= 8) #Rに入っているサンプルデータairqualityから2群だけ取り出したデータで試してみる wilcox.test(data = wilcox_sample, Ozone ~ Month) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Ozone by Month ## W = 552, p-value = 0.003248 ## alternative hypothesis: true location shift is not equal to 0 10.3.2 クラスカル・ウォリスの検定 3群以上で値を比較するノンパラメトリック検定である。同じく、データを順位データに変換して比較を行う。 Rではkruskal.test()が用意されている。 kruskal_sample = airquality #Rに入っているサンプルデータairqualityで試してみる kruskal.test(data = kruskal_sample, Ozone ~ Month) ## ## Kruskal-Wallis rank sum test ## ## data: Ozone by Month ## Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06 10.3.3 カイ二乗検定 解析の対象の変数が質的変数で、頻度に偏りがあるかを比較したい場合は、カイ二乗検定が使われる。 Rには、カイ二乗検定を行うためのchisq.test()がある。 tab = matrix(c(12, 30, 25, 16), ncol=2) #表を作成する chisq.test(tab) #chisq.testの中に表を入れる ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 7.5549, df = 1, p-value = 0.005985 カイ二乗検定は比率に偏りがあるかを検定してくれるが、どこに偏りがあるかは研究者自身が表を見て判断するしかない。 10.4 「統計モデル」との関係 この章では、心理統計で学んできた代表的な分析手法をRで行う方法について解説してきた。 一般的に心理統計では、「変数が正規分布に従い、グループが２つならばt検定」、「変数が正規分布に従い、グループが3つ以上ならば分散分析」、「変数がカテゴリカル変数ならばカイ二乗検定」といったように、変数のタイプや研究デザインに応じて行う解析を選ぶ方法が提案されている。 しかし、上に挙げた条件に当てはまらないデータの場合は、どのような分析をすれば良いのだろうか？ 以降の章では、これまで学んできた分析手法を統計モデルという一つの枠組みで捉え直していく。 "],["10-linear_model.html", "Chapter 11 線形モデル（回帰分析） 11.1 準備 11.2 線形モデルの概要 11.3 線形モデルによる解析 11.4 最尤法 11.5 確認問題", " Chapter 11 線形モデル（回帰分析） これまで学んできた様々な統計手法（t検定、分散分析など）を、「線形モデル」という一つの枠組みで捉えていく。 この章では「線形モデル」の一部である回帰分析を通して、線形モデルの概要を学んでいく。 11.1 準備 データの可視化のために、ggplot2パッケージをロードする。 library(ggplot2) 11.2 線形モデルの概要 まず、線形モデルの表現の仕方を理解する。以下の式は、変数\\(x\\)から、変数\\(y\\)を予測するプロセスを記述したものである。変数\\(x\\)は予測変数（predictor variable）、変数\\(y\\)は応答変数（response variable）と呼ばれる。このように、応答変数と予測変数との関係を式で表現したものをモデルと呼ぶ。 予測変数は、「独立変数」や「説明変数」とも呼ばれる。応答変数は、「従属変数」や「被説明変数」とも呼ばれる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\tag{1}\\\\ y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] 1番目の式の右側に\\(\\alpha + \\beta x\\)という線形の式がある。この式は、線形予測子(linear predictor)と呼ばれる。変数\\(x\\)に係る\\(\\beta\\)は予測変数に係る傾き(slope)、\\(\\alpha\\)は切片(intercept)である。1番目の式は、変数\\(x\\)の持つ効果（傾き）及びそれ以外の効果（切片）と変数\\(y\\)の予測値（\\(\\hat{y}\\)）との関係を示している。 予測変数は2個以上でも構わない。予測変数の個数を\\(K\\)とすると、(1)の1番目の式は以下のように表現できる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x_{k} \\\\ \\tag{2} \\end{equation} \\] \\(y \\sim \\text{Normal}(\\hat{y}, \\sigma)\\)は、「応答変数\\(y\\)が、予測値\\(\\hat{y}\\)を平均、\\(\\sigma\\)を標準偏差とする正規分布に従う」ことを示している。つまり、線形予測子から予測された値\\(\\hat{y}\\)と誤差\\(\\sigma\\)から、実際の値\\(y\\)が推定されるプロセスを表現している。 応答変数が正規分布に従うという前提をおいたモデルのことを、一般的に線形モデル(linear model)と呼ぶ。 線形モデルは、基本的に心理統計でも学んだ「回帰分析(regression analysis)」と同じである。 応答変数\\(y\\)を決定づける変数、\\(\\alpha\\), \\(\\beta\\)、及び \\(\\sigma\\)はパラメータ(parameter)と呼ばれる。このパラメータを、既知の変数である\\(x\\)と\\(y\\)から推定する。 11.2.1 まとめ まずは、「応答変数」、「予測変数」、「（応答変数が従う）確率分布」、「線形予測子」、「傾き」、「切片」など、線形モデルを構成するキーワードを覚えよう。 線形モデルは、応答変数と予測変数の関係を線形の式で表したモデルである。 線形予測子の傾き、切片及び誤差（正規分布の標準偏差）を推定する。 予測変数が応答変数に及ぼす効果を推定することが、線形モデルの目的である。 11.3 線形モデルによる解析 実際に、Rで線形モデルの解析をしてみよう。 Rには線形モデルを扱える関数lm()がある。irisデータを使って解析をしてみよう。 Sepal.LengthとPetal.Lengthの関係を散布図で確認する。 p = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) p Sepal.Lengthが大きいほどPetal.Lengthが大きいという関係（正の相関）がありそうである。そこで、Sepal.Lengthの大きさから、Petal.Lengthの大きさを予測することを試みる。 lm()関数に、「応答変数~ 1 + 予測変数」のかたちで入力する。以下のプログラムを実行してみよう。 ＊1 +の部分は省略しても構わない。1 +は線形予測子の切片の部分を表している。省略しても、lm()は自動で切片の値を求めてくれる。 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) 結果をresultという名前でいったん保存した（名前はresult以外でも構わない）。summary()関数の中に、resultを入れて実行すると詳細な結果が出力される。 summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 色んな情報が出力されるが、まずは係数（Coefficients）の部分を見てみよう。ここでは、データから推定された切片や予測変数の傾きの結果が出力されている。 Interceptの部分が切片の推定結果である。各変数の名前の部分（ここではSepal.Length）が予測変数の傾きの推定結果を示している。 Estimateが推定された切片または傾きの値である。 Std.Errorは推定された係数の標準誤差である。 予測変数が応答変数に対して影響力を持っているか？ それは傾きの係数の推定結果からわかる。係数の値は、予測変数が1単位増えたら応答変数がどのくらい増えるか、あるいは減るかを意味している。 係数がプラスならば、予測変数の値が増えると応答変数の値が増加する関係にあることを意味する。 係数がマイナスならば、予測変数の値が増えると応答変数が減少する関係にあることを意味する。 t value及びPrは係数の有意性検定の結果を示している（それぞれt値、p値）。ここでは、「母集団の係数がゼロである」という帰無仮説を検定している。p値が極端に低い場合は、「求めた係数の値は有意にゼロから離れている」と結論付けることができる。 この例の場合は、Sepal.Lengthが1単位増えると、Petal.Lengthが1.86だけ上昇することを示している。 切片の値は-7.1となっているが、Sepal.Lengthの値がゼロのときのPetal.Lengthの予測値を意味している（この例ではアヤメのがくの長さとしてありえない負の値になっているが、これは変数の標準化等を行っていないためである。詳しくは次章で解説する）。 Sepal.LengthとPetal.Lengthの散布図に、線形モデルから推定された切片と傾きの値を持つ以下の直線を引いてみよう。 \\[ \\begin{equation} Petal.Length = -7.10 + 1.86 Sepal.Length\\\\ \\tag{3} \\end{equation} \\] p = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_smooth(data = iris, aes(x = Sepal.Length, y = Petal.Length), formula = y~x, method = &quot;lm&quot;, se = FALSE) p 線形モデルでは、直線の式で予測変数と応答変数の関係を表現する。実際のデータ（散布図の点）とのズレが最小になるような、直線の式を推定する（予測値と実測値とのズレが最小になるのが、最も良い予測である）。 11.4 最尤法 （ここでは、線形モデルのパラメータがどのように推定されているかについて解説している。読み飛ばしても、線形モデルを使った解析自体はできる） 線形モデル（及び一般化線形モデル）では、パラメータの推定に最尤法（maximum likelihood method）という最適化手法が用いられる。 11.4.1 最尤法によるパラメータ推定 ここにコインが1枚ある。コインの表が出るかを決定づけるパラメータ（つまりコインを投げて表が出る確率）を\\(\\theta\\)（シータ）とする。この\\(\\theta\\)の値を何回かコインを投げる実験を通して推定する。 1回目は、表が出た。この時点で、この実験結果が生じる確率は\\(\\theta\\)である。 2回目は、裏が出た。1回目と2回目までの実験結果が生じる確率は\\(\\theta(1-\\theta)\\)である。 3回目は、表が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)\\theta\\)である。 その後、4回目は裏、5回目は裏だったとする。5回目で実験をストップすることにする。この実験結果が生じる確率\\(L\\)は以下のように表すことができる。 \\[ L = (1-\\theta)^3 \\theta^2 \\tag{4} \\] \\(L\\)のことを尤度(likelihood)と呼ぶ（”ゆうど”と読む）。 尤度とは「もっともらしさ」を示す概念である。イメージとしては、「今回の観測結果が得られる確率」である。今回の観測データに対して最も当てはまりが良くなる、すなわち尤度が最も高くなるときのパラメータを求めるのが、最尤法 (maximum likelihood method)と呼ばれる手法である。 掛け算を扱う尤度は計算が困難なので、実際のパラメータ推定の際には対数化して足し算を扱う。対数化した尤度を対数尤度(log-likelihood)と呼ぶ。対数尤度が最大となるパラメータ\\(\\theta\\)を求める。 \\[ \\log L = \\log(1-\\theta)+\\log(1-\\theta)+\\log(1-\\theta)+\\log(\\theta)+\\log(\\theta) \\tag{5} \\] 以下のプログラムで、上のコイン投げの例で最も対数尤度が高くなるときの\\(\\theta\\)を求めている。maximumが対数尤度が最も高くなるパラメータの値で、objectiveがそのときの対数尤度である。（プログラムの意味については理解しなくて良い） D = c(1, 0, 1, 0, 0) #観測データのベクトル：1=表、0=裏とする #対数尤度を求める関数 LogLikelihood = function(x){ return(function(theta){ L = 1 for(i in 1:length(x)){ L = L * theta^(x[i]) * (1-theta)^(1-x[i]) } return(log(L)) }) } #optimize関数で、対数尤度が最も高くなるパラメータthetaを推定する result_mlm = optimize(f = LogLikelihood(D), c(0, 1), maximum=TRUE) result_mlm ## $maximum ## [1] 0.4000015 ## ## $objective ## [1] -3.365058 パラメータ\\(\\theta\\)と対数尤度\\(\\log L\\)との関係を以下に示す。 #図示する theta = seq(0.01,0.99,0.01) logL = log(theta)+log(1-theta)+log(theta)+log(1-theta) + log(1-theta) data_mlm = data.frame(x=theta, y=logL) ggplot2::ggplot() + ggplot2::geom_line(data=data_mlm, aes(x=x, y= y), size=1) + ggplot2::geom_vline(xintercept = result_mlm$maximum, linetype=&quot;dashed&quot;, colour=&quot;red&quot;, size=1) + ggplot2::labs(x =&quot;theta&quot;, y= &quot;log L&quot;) ## Warning: Using `size` aesthetic for lines was ## deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every ## 8 hours. ## Call ## `lifecycle::last_lifecycle_warnings()` ## to see where this warning was ## generated. 対数尤度が最も大きくなるのは、\\(\\theta=\\) 0.4のときである（表が出た割合である2/5と一致）。 ここではわかりやすくパラメータを1つのみ使って説明しているが、線形モデルの傾きや切片の推定も同じである。上の例の\\(\\theta\\)を線形予測子に置き換えて同様の計算をする。 線形モデルのパラメータ推定には、最小二乗法と呼ばれる別の推定方法もある。ただし、最小二乗法を使っても最尤法を使っても、線形モデルの傾きや切片の推定値は同じになる。 11.5 確認問題 Rに入っているサンプルデータtreesを使って、線形モデルの結果の解釈の仕方とlm()関数の扱い方を復習をする。 head(trees) ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 問１ Heightを応答変数、Girthを予測変数として、切片と傾きの推定値を求めよ。 問２ Heightを応答変数、Volumeを予測変数として、切片と傾きの推定値を求めよ。 問３ 問2の推定結果から、Volumeが1単位増えるとHeightがどう変化するかを説明せよ。 "],["11-linear_model2.html", "Chapter 12 線形モデル（t検定, 分散分析, 共分散分析） 12.1 準備 12.2 線形モデルに含まれる統計解析 12.3 変数の標準化 12.4 予測変数がカテゴリカル変数の場合（ダミー変数） 12.5 グループが複数ある場合（一要因の分散分析） 12.6 交互作用（2要因の分散分析） 12.7 予測変数が複数ある場合（共分散分析または重回帰分析） 12.8 まとめ 12.9 確認問題 12.10 補足(emmeans)", " Chapter 12 線形モデル（t検定, 分散分析, 共分散分析） 前の章では、線形モデルの概要を見てきた。前の章で線形モデルを用いて行った分析は、要は「単回帰分析（一つの連続変量から、もう一方の連続変量の値を予測する分析）」であった。 この章では、線形モデルを用いて他の分析を行う。予測変数のパターンが異なるケースの演習を通して、線形モデルは正規分布を扱うあらゆるタイプの分析を内包したモデルであることを確認していく。 変数の標準化 予測変数がカテゴリの場合（ダミー変数） 予測変数がカテゴリの場合（複数のダミー変数） 交互作用 12.1 準備 必要なパッケージをロードする。 library(dplyr) library(ggplot2) 12.2 線形モデルに含まれる統計解析 線形モデルとは特定の解析を指すものではなく、正規分布を扱う様々な統計解析を包括的に扱う統計モデルである。例えば、変数の正規性を前提とするt検定や分散分析も線形モデルの中に含まれる。予測変数の種類や個数の違いによって、線形モデルは以下のそれぞれの統計解析と一致する。 分析 予測変数 予測変数の個数 t検定 二値(0 or 1) 1個 分散分析 二値 2個以上 共分散分析 二値及び連続量 二値が2個以上、連続量が1個以上 単回帰分析 連続量 1個 重回帰分析 連続量（二値を含んでも可） 2個以上 前章では単回帰分析（予測変数が連続量で1個）を例として演習を行った。以下では、t検定や分散分析と似たことをlm()関数で行う練習を通して、予測変数がカテゴリーの場合の扱い方を学んでいく。 12.3 変数の標準化 解析の演習に入る前に、標準化(standardizing)について確認しておこう。標準化とは、元の値を「ゼロが平均値、1が標準偏差」と等しくなるように値を変換する処理のことをいう。変数を標準化しておくと、線形モデルの係数の解釈が直感的に理解しやすくなる事が多い。 例えば、前の章ではirisデータを使って単回帰分析を行った。 dat = iris #ここではirisを別の名前（dat）に変えて練習に用いる result = lm(data = dat, Petal.Length ~ 1 + Sepal.Length) summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 切片の値はSepal.LengthがゼロのときのPetal.Lengthの予測値である。しかし、アヤメの花弁の長さがマイナスやゼロの値を取るというのはありえない。また、切片は予測変数がゼロのときの応答変数の値であるが、このデータも予測変数（すなわちがくの長さ）がゼロの場合も論理的には有り得ない。このように、元の値をそのまま使っても係数の解釈にこまる場面がある。 応答変数及び予測変数を標準化したものを使って同じ解析をして、結果を比較してみよう。具体的には、元の得点から平均値を引いて差の得点を求め、その差の得点を標準偏差で割る。 dat = dat |&gt; dplyr::mutate(Petal.Length_std = (Petal.Length - mean(Petal.Length))/sd(Petal.Length), Sepal.Length_std = (Sepal.Length - mean(Sepal.Length))/sd(Sepal.Length)) head(dat) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Length_std ## 1 5.1 3.5 1.4 0.2 setosa -1.335752 ## 2 4.9 3.0 1.4 0.2 setosa -1.335752 ## 3 4.7 3.2 1.3 0.2 setosa -1.392399 ## 4 4.6 3.1 1.5 0.2 setosa -1.279104 ## 5 5.0 3.6 1.4 0.2 setosa -1.335752 ## 6 5.4 3.9 1.7 0.4 setosa -1.165809 ## Sepal.Length_std ## 1 -0.8976739 ## 2 -1.1392005 ## 3 -1.3807271 ## 4 -1.5014904 ## 5 -1.0184372 ## 6 -0.5353840 標準化した得点を用いて線形モデルで解析を行う。 result_std = lm(data = dat, Petal.Length_std ~ 1 + Sepal.Length_std) summary(result_std) ## ## Call: ## lm(formula = Petal.Length_std ~ 1 + Sepal.Length_std, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.40343 -0.33463 -0.00379 0.34263 1.41343 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.333e-16 4.014e-02 0.00 1 ## Sepal.Length_std 8.718e-01 4.027e-02 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4916 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 標準化しない得点を使ったときの解析結果と比べると、係数の値が変わっている。しかし、傾きのt値及びp値は変わっていない（標準化は単にデータを平行移動させただけなので、回帰直線の傾きは変わらない）。 切片は0、Sepal.Length_stdの効果は0.87である。切片の値は、Sepal.Length_stdがゼロのとき、つまりSepal.Lengthが平均値と等しいとき、Petal.Length_stdはほぼゼロの値を取る、つまりPetal.Lengthの平均値であることを意味している。また、Sepal.Length_stdの傾きは、Sepal.Length_stdが1のとき（つまりSepal.Lengthが1標準偏差分増加したとき）、Petal.Length_stdが0.87増えることを意味する。 このように、標準化することで算出された係数の解釈がしやすくなることが多い。以降の線形モデルを扱う分析の例でも、応答変数は標準化したものを用いる。 Rには標準化を行うための関数も用意されている。scale()を用いる。 dat = iris |&gt; dplyr::mutate(Petal.Length_std = scale(Petal.Length)) head(dat) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Length_std ## 1 5.1 3.5 1.4 0.2 setosa -1.335752 ## 2 4.9 3.0 1.4 0.2 setosa -1.335752 ## 3 4.7 3.2 1.3 0.2 setosa -1.392399 ## 4 4.6 3.1 1.5 0.2 setosa -1.279104 ## 5 5.0 3.6 1.4 0.2 setosa -1.335752 ## 6 5.4 3.9 1.7 0.4 setosa -1.165809 12.4 予測変数がカテゴリカル変数の場合（ダミー変数） 予測変数はカテゴリカル変数（質的変数）でも構わない。ただし、予測変数を0か1のどちらかの値を取るダミー変数(dummy variable)に変換する必要がある。 Rに入っているsleepデータを少し変えたもの使って、カテゴリカル変数を予測変数に含む線形モデルの解析をしてみよう。 sleep_1 = sleep |&gt; dplyr::select(-ID) |&gt; dplyr::mutate(group = ifelse(group == 1, &quot;control&quot;, &quot;treatment&quot;)) sleep_1 ## extra group ## 1 0.7 control ## 2 -1.6 control ## 3 -0.2 control ## 4 -1.2 control ## 5 -0.1 control ## 6 3.4 control ## 7 3.7 control ## 8 0.8 control ## 9 0.0 control ## 10 2.0 control ## 11 1.9 treatment ## 12 0.8 treatment ## 13 1.1 treatment ## 14 0.1 treatment ## 15 -0.1 treatment ## 16 4.4 treatment ## 17 5.5 treatment ## 18 1.6 treatment ## 19 4.6 treatment ## 20 3.4 treatment groupはグループを意味する変数である（統制群controlもしくは実験群treatment）。まずこれを、「treatmentなら1、controlなら0」とする新たな変数dgroupを作る。 sleep_1 = sleep_1 |&gt; dplyr::mutate(dgroup = ifelse(group == &quot;treatment&quot;, 1, 0)) sleep_1 ## extra group dgroup ## 1 0.7 control 0 ## 2 -1.6 control 0 ## 3 -0.2 control 0 ## 4 -1.2 control 0 ## 5 -0.1 control 0 ## 6 3.4 control 0 ## 7 3.7 control 0 ## 8 0.8 control 0 ## 9 0.0 control 0 ## 10 2.0 control 0 ## 11 1.9 treatment 1 ## 12 0.8 treatment 1 ## 13 1.1 treatment 1 ## 14 0.1 treatment 1 ## 15 -0.1 treatment 1 ## 16 4.4 treatment 1 ## 17 5.5 treatment 1 ## 18 1.6 treatment 1 ## 19 4.6 treatment 1 ## 20 3.4 treatment 1 ifelse()関数は、ifelse(XXX, A, B)と表記することで、「XXXの条件に当てはまればA、当てはまらなければB」という処理をしてくれる。ここでは、変数groupについて、treatmentならば1, それ以外なら0に変換し、0か1を取る変数dgroupを新たに作った。 このdgroupがダミー変数である。 解析に用いるモデルを確認すると、以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta dgroup \\\\ \\tag{1}\\\\ y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] \\(dgroup\\)は0か1のどちらかを取る変数で、\\(dgroup = 0\\)のとき、つまり統制群のとき、応答変数の予測値は\\(\\mu = \\alpha\\)となる。\\(dgroup = 1\\)のとき、つまり実験群のとき、応答変数の予測値は\\(\\mu = \\alpha + \\beta\\)となる。すなわち、切片\\(\\alpha\\)は統制群のときの効果、傾き\\(\\beta\\)は実験群の時に加わる実験群特有の効果を意味する。 lm()を使って、上のモデル式のパラメータの推定をしよう。 sleep_1 = sleep_1 |&gt; dplyr::mutate(extra_std = scale(extra)) #応答変数を標準化したものを用いる result = lm(data = sleep_1, extra_std ~ 1 + dgroup) summary(result) ## ## Call: ## lm(formula = extra_std ~ 1 + dgroup, data = sleep_1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2042 -0.6467 -0.2874 0.7210 1.5709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.3915 0.2975 -1.316 0.2048 ## dgroup 0.7830 0.4208 1.861 0.0792 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9409 on 18 degrees of freedom ## Multiple R-squared: 0.1613, Adjusted R-squared: 0.1147 ## F-statistic: 3.463 on 1 and 18 DF, p-value: 0.07919 2つの群間で平均値を比較するときにはt検定がよく使われる。t.test()関数を使って\\(dgroup=0\\)と\\(dgroup=1\\)との間で\\(y\\)の値の平均値を比較したときのt値及びp値の結果が、lm()の傾きのt値及びp値と一致することを確認しよう。 t.test(data = sleep_1, extra_std ~ dgroup) ## ## Welch Two Sample t-test ## ## data: extra_std by dgroup ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1.6677984 0.1018292 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.3914923 0.3914923 lm()の傾きの検定は、「傾きがゼロである」という帰無仮説を検定している。傾きの係数が意味することは、予測変数\\(dgroup\\)が1単位増えたときの応答変数\\(y\\)の変化量であった。傾きの検定は、「\\(dgroup=0\\) から \\(dgroup=1\\) に変化することによって、 応答変数（extra） が上昇（下降）するか（傾きがゼロではないか）」を検定している。要は、「\\(dgroup=0\\)と\\(dgroup=1\\)の間で\\(y\\)の値に差があるか」を検定しているのと論理的に同じである。 このように、予測変数が1つで、予測変数が二値（0もしくは1）であるときの線形モデルは、t検定に対応する。 12.5 グループが複数ある場合（一要因の分散分析） 先ほどの例は、統制群と実験群の二つのグループの場合であった。例えば実験で統制群、実験群1、実験群2といったように三つ以上のグループを設定した場合は、どうダミー変数を作成すればよいのか？ Rに入っているPlantGrowthを例として見ていこう。以下のプログラムを実行して、データを作ろう。 dat = PlantGrowth |&gt; dplyr::mutate(t1 = ifelse(group == &quot;trt1&quot;, 1, 0), t2 = ifelse(group == &quot;trt2&quot;, 1, 0) ) dat ## weight group t1 t2 ## 1 4.17 ctrl 0 0 ## 2 5.58 ctrl 0 0 ## 3 5.18 ctrl 0 0 ## 4 6.11 ctrl 0 0 ## 5 4.50 ctrl 0 0 ## 6 4.61 ctrl 0 0 ## 7 5.17 ctrl 0 0 ## 8 4.53 ctrl 0 0 ## 9 5.33 ctrl 0 0 ## 10 5.14 ctrl 0 0 ## 11 4.81 trt1 1 0 ## 12 4.17 trt1 1 0 ## 13 4.41 trt1 1 0 ## 14 3.59 trt1 1 0 ## 15 5.87 trt1 1 0 ## 16 3.83 trt1 1 0 ## 17 6.03 trt1 1 0 ## 18 4.89 trt1 1 0 ## 19 4.32 trt1 1 0 ## 20 4.69 trt1 1 0 ## 21 6.31 trt2 0 1 ## 22 5.12 trt2 0 1 ## 23 5.54 trt2 0 1 ## 24 5.50 trt2 0 1 ## 25 5.37 trt2 0 1 ## 26 5.29 trt2 0 1 ## 27 4.92 trt2 0 1 ## 28 6.15 trt2 0 1 ## 29 5.80 trt2 0 1 ## 30 5.26 trt2 0 1 ダミー変数を2つ作成した。ctrlのときは「t1 = 0, t2 = 0」,trt1のときは「t1 = 1, t2 = 0」,trt2のときは「t1 = 0, t2 = 1」となっている。これら２つのダミー変数を線形予測子に加えたモデルは、モデルは以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{t1} t_1 + \\beta_{t2} t_2 \\\\ \\tag{3} y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] 切片\\(\\alpha\\)は「t1 = 0, t2 = 0（すなわち、ctrlのとき）」の\\(\\hat{y}\\)の値と等しい。 傾き\\(\\beta_{t1}\\)は「t1 = 1, t2 = 0（すなわち、trt1のとき）」の\\(\\hat{y}\\)の値と等しい。 傾き\\(\\beta_{t2}\\)は「t1 = 0, t2 = 1（すなわち、trt2のとき）」の\\(\\hat{y}\\)の値と等しい。 このように、グループの数が\\(K\\)個ある場合には、ダミー変数を\\(K-1\\)個作れば全てのグループの予測値\\(\\hat{y}\\)を線形予測子で表すことができる。 lm()で傾き及び切片のパラメータを推定しよう。 dat = dat |&gt; dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる result = lm(data = dat, weight_std ~ t1 + t2 + 1) summary(result) ## ## Call: ## lm(formula = weight_std ~ t1 + t2 + 1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52740 -0.59613 -0.00856 0.37472 1.95239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.05847 0.28113 -0.208 0.8368 ## t1 -0.52910 0.39758 -1.331 0.1944 ## t2 0.70451 0.39758 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.889 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 式（3）より、切片の推定値は\\(t_1=0\\)かつ\\(t_2=0\\)のときの\\(\\mu\\)、つまり統制群(ctrl)のときの応答変数weight_stdの推定値を意味している。各ダミー変数の係数（傾き）は、切片に加わる各条件の効果を意味している。例えば、t2の係数は0.7であるが、これは\\(t_2\\)のとき（つまりtrt2のとき）の応答変数の予測値は、 0.65(= 切片 + t2の傾き)となることを示している。 係数の意味することは、基準となるグループ（どのダミー変数も0となるグループ）と比べての効果ということになる。 図でも条件別にweight_stdの分布を確認してみよう。分布を見ても同様の傾向があるが、線形モデルの解析の結果その効果が有意であることが確認できた。 ggplot() + geom_boxplot(data = dat, aes(x = group, y = weight_std)) モデル（式）を確認しながら、係数が何を意味しているのかを常に意識するようにしよう。 このテキストでは練習のためにダミー変数を自分でプログラムを書いて作っているが、lm()関数にカテゴリカル変数をそのまま入れても結果を出力してくれる。カテゴリのうちアルファベット順で最初に出てくるカテゴリ(以下のPlantGrowthの例ではctrl)を基準として、残りのダミー変数を自動で作ってくれている。 dat = PlantGrowth |&gt; dplyr::mutate(weight_std = scale(weight)) #応答変数を標準化したものを用いる result = lm(data = dat, weight_std ~ group) summary(result) ## ## Call: ## lm(formula = weight_std ~ group, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52740 -0.59613 -0.00856 0.37472 1.95239 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.05847 0.28113 -0.208 0.8368 ## grouptrt1 -0.52910 0.39758 -1.331 0.1944 ## grouptrt2 0.70451 0.39758 1.772 0.0877 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.889 on 27 degrees of freedom ## Multiple R-squared: 0.2641, Adjusted R-squared: 0.2096 ## F-statistic: 4.846 on 2 and 27 DF, p-value: 0.01591 12.6 交互作用（2要因の分散分析） 次は、線形モデルで交互作用を扱う方法について確認する。2要因以上の分散分析と同様のことを線形モデルで行う。 以下のプログラムを実行して、サンプルデータdを作ろう。 set.seed(1) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.1 + 0.4 * x y = rnorm(n = 20, mean = mu, sd = 1) d_M = data.frame(x = x, y = y, gender = &quot;M&quot;) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.3 + -0.6 * x y = rnorm(n = 20, mean = mu, sd = 1) d_F = data.frame(x = x, y = y, gender = &quot;F&quot;) d = rbind(d_M, d_F) head(d) ## x y gender ## 1 3 2.811781 M ## 2 4 2.089843 M ## 3 6 1.878759 M ## 4 9 1.485300 M ## 5 3 2.424931 M ## 6 9 3.655066 M このデータdには、x, y, genderの3つの変数が含まれている。genderは性別を意味する変数とする。M（男性）かF（女性）のいずれかである。男女別に、実験で2つの変数を測定したとしよう。 応答変数をy、予測変数をxとして線形モデルで切片及びxの傾きのパラメータを推定する。モデルは以下のようになる。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\\\ \\tag{4} y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] lm()関数を使って推定しよう（\\(x\\)と\\(y\\)の散布図及び係数の信頼区間も図示する）。 result = lm(data = d, y ~ 1 + x) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8209 -2.5577 -0.7021 2.4363 5.1560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7389 1.3472 0.549 0.587 ## x -0.1811 0.2060 -0.879 0.385 ## ## Residual standard error: 3.231 on 38 degrees of freedom ## Multiple R-squared: 0.01993, Adjusted R-squared: -0.005863 ## F-statistic: 0.7727 on 1 and 38 DF, p-value: 0.3849 newdat = data.frame(x = seq(1,10,0.1)) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(x = seq(1,10,0.1), result_conf) ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y), size = 3) + ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.4) 予測変数xの傾きはほぼフラットで、yに対してあまり効果がないようにみえる。 しかし、このデータdにはもう一つ性別を意味するgenderという変数が含まれていた。genderを区別して、またxとyの散布図を見てみよう。 ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) 性別が女性（F）か男性（M）かで、xとyの関係が違うようである。 このように、別の変数との組み合わせにより、変数間の関係が変化することを交互作用(interaction)という。このデータでも、応答変数yに対して性別genderとxの交互作用がありそうである。 交互作用のあるモデルは、以下のように表現する。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta_{1} x + \\beta_{2} M + \\beta_{3} xM \\\\ \\tag{5} y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] \\(M\\)は性別genderのダミー変数で、M（男性）ならば1、F（女性）ならば0の変数とする。 線形モデルでは、交互作用は予測変数同士の積で扱う。男性（M=1）の場合の\\(\\mu\\)の推定値は、\\(\\mu = \\alpha +(\\beta_{1} + \\beta_{3}) x +\\beta_{2}\\)となる。一方、女性（M=0）の場合は、\\(mu = \\alpha +\\beta_{1} x\\)となる。\\(\\beta_{3}\\)は、男性のときの\\(x\\)に係る傾きの変化量を意味することになる。このように、交互作用を考慮する予測変数の積をモデルに加えることで、男性か女性かで切片及び傾きが変化することを表現できる。 d$M = ifelse(d$gender == &quot;M&quot;, 1, 0) #genderがMならば1, Fならば1のダミー変数を作る result = lm(data = d, y ~ 1 + x*M) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x * M, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3555 -0.6534 0.2205 0.5636 1.6618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05079 0.53625 0.095 0.925 ## x -0.53691 0.08107 -6.622 1.03e-07 *** ## M 1.04827 0.73745 1.421 0.164 ## x:M 0.77868 0.11274 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8827 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 2つの予測変数の積の傾き（\\(\\beta_{3}\\)）は、x:Mである。p値も小さく、有意な効果を持っているようである。 サンプルデータについて、推定されたパラメータを元に、男女別に線形モデルの直線の信頼区間を図示したのが以下の図である。 new_x = seq(1,10,0.1) newdat = data.frame(x = rep(new_x,2), M = c(rep(0,length(new_x)), rep(1,length(new_x)))) result_conf = predict(result, new = newdat, interval = &quot;confidence&quot;, level = 0.95) plot_conf = data.frame(newdat, result_conf) plot_conf$gender = ifelse(plot_conf$M == 1, &quot;M&quot;, &quot;F&quot;) ggplot2::ggplot() + ggplot2::geom_point(data = d, aes(x = x, y = y, shape = gender, color=gender), size = 3) + ggplot2::geom_line(data = plot_conf, aes(x = x, y = fit, color=gender)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = x, ymax = upr, ymin = lwr, color =gender), alpha = 0.4) 12.6.1 交互作用を扱うモデルの解釈 2要因以上の分散分析と同様に、線形モデルでも交互作用の有無を検討することができる。しかし、ここで注意が必要なのは、交互作用を含む線形モデルは解釈が複雑になることである。    分散分析では、変数の組み合わせの効果である交互作用とは別に、その変数そのものの効果である主効果についての検定も行われる。では、上述の線形モデルの\\(\\beta_{1}\\)及び\\(\\beta_{2}\\)のパラメータはそれぞれ、性別\\(M\\)及び予測変数\\(x\\)の主効果として解釈できるのだろうか？ 例えば\\(\\beta_{1}\\)について見てみよう。先述のように、男性（M=1）の場合の\\(y\\)の推定値は\\(\\hat{y} = \\alpha +(\\beta_{1} + \\beta_{3}) x +\\beta_{2}\\)、女性（M=0）の場合は\\(\\hat{y} = \\alpha +\\beta_{1} x\\)となる。すなわち、\\(\\beta_{1}\\)は「女性のときのxの傾き」を意味し、性別が無関係なxの効果を必ずしも意味しない。 このように、交互作用を含むモデルの場合、予測変数の傾きは必ずしも主効果を意味するわけではない。次の節の補足で、この問題に対する対処法について説明する。 12.7 予測変数が複数ある場合（共分散分析または重回帰分析） 予測変数は連続量もカテゴリカル変数でも何でも含めても良い。 12.7.1 変数の効果の統制 予測変数を複数加えた線形モデルの解析のメリットは、ある予測変数について他の予測変数の効果を統制(control)したときの効果を検討できることにある。 Rで標準で入っているattitudeデータを使って、予測変数が複数ある場合の線形モデルの解析の結果を確認してみよう。 head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 以下のように、complaints, privileges, learning, raisesの4つを予測変数として、ratingの値の推定を行ってみよう。 dat = attitude |&gt; dplyr::mutate(rating_std = scale(rating), complaints_std = scale(complaints), privileges_std = scale(privileges), learning_std = scale(learning), raises_std = scale(raises) ) #変数を標準化したものを使う result = lm(data = dat, rating_std ~ 1 + complaints_std + privileges_std + learning_std + raises_std) summary(result) ## ## Call: ## lm(formula = rating_std ~ 1 + complaints_std + privileges_std + ## learning_std + raises_std, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9255 -0.4433 0.0492 0.4765 0.9231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.056e-15 1.049e-01 0.000 1.000 ## complaints_std 7.560e-01 1.593e-01 4.745 7.21e-05 *** ## privileges_std -1.034e-01 1.326e-01 -0.780 0.443 ## learning_std 2.375e-01 1.488e-01 1.596 0.123 ## raises_std -2.179e-02 1.571e-01 -0.139 0.891 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5748 on 25 degrees of freedom ## Multiple R-squared: 0.7152, Adjusted R-squared: 0.6697 ## F-statistic: 15.7 on 4 and 25 DF, p-value: 1.509e-06 切片（Intercept）は全ての予測変数の値がゼロ（ここでは標準化しているので平均）のときの応答変数の予測値であり、各予測変数の係数は予測変数が1単位（1標準偏差）増えた場合の応答変数の変化量を意味している。例えば、complaints_stdの係数は0.76であるが、これは「complaintsが1増えるとratingは0.76増える傾向にある」ことを意味している。 各係数の値は「他の変数の値がゼロであるときの効果」を意味している。先程のcomplaintsの係数0.76は、その他の予測変数privileges, learning, raisesがゼロのときの、complaintsがratingに与えるそのものの効果を示している。 このように複数の予測変数を入れたモデルで推定される係数は、他の予測変数の効果を統制した上での予測変数が応答変数に及ぼす効果を意味する。 交互作用を含むモデルの場合も、（ダミー変数も含めて）予測変数を標準化すると係数を解釈しやすくなる。 #データを再度作成する。 set.seed(1) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.1 + 0.4 * x y = rnorm(n = 20, mean = mu, sd = 1) d_M = data.frame(x = x, y = y, gender = &quot;M&quot;) x = round(runif(n = 20, min = 1, max = 10),0) mu = 0.3 + -0.6 * x y = rnorm(n = 20, mean = mu, sd = 1) d_F = data.frame(x = x, y = y, gender = &quot;F&quot;) d = rbind(d_M, d_F) d$M = ifelse(d$gender == &quot;M&quot;, 1, 0) #genderがMならば1, Fならば1のダミー変数を作る #標準化する前の結果 result = lm(data = d, y ~ 1 + x*M) summary(result) ## ## Call: ## lm(formula = y ~ 1 + x * M, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3555 -0.6534 0.2205 0.5636 1.6618 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05079 0.53625 0.095 0.925 ## x -0.53691 0.08107 -6.622 1.03e-07 *** ## M 1.04827 0.73745 1.421 0.164 ## x:M 0.77868 0.11274 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8827 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 応答変数と予測変数を標準化する。ダミー変数も標準化する。 #変数を標準化 d = d |&gt; dplyr::mutate(y_std = scale(y), x_std = scale(x), M_std = scale(M) ) head(d) ## x y gender M y_std x_std M_std ## 1 3 2.811781 M 1 0.9835716 -1.21465482 0.9874209 ## 2 4 2.089843 M 1 0.7594727 -0.81640734 0.9874209 ## 3 6 1.878759 M 1 0.6939495 -0.01991237 0.9874209 ## 4 9 1.485300 M 1 0.5718146 1.17483007 0.9874209 ## 5 3 2.424931 M 1 0.8634883 -1.21465482 0.9874209 ## 6 9 3.655066 M 1 1.2453383 1.17483007 0.9874209 result = lm(data = d, y_std ~ 1 + x_std*M_std) summary(result) ## ## Call: ## lm(formula = y_std ~ 1 + x_std * M_std, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73117 -0.20282 0.06843 0.17496 0.51584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01209 0.04336 0.279 0.7821 ## x_std -0.11502 0.04394 -2.618 0.0129 * ## M_std 0.90526 0.04391 20.615 &lt; 2e-16 *** ## x_std:M_std 0.30734 0.04450 6.907 4.35e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.274 on 36 degrees of freedom ## Multiple R-squared: 0.9307, Adjusted R-squared: 0.9249 ## F-statistic: 161.1 on 3 and 36 DF, p-value: &lt; 2.2e-16 それぞれの係数とp値が変わった。それぞれの係数の値は、他の変数がゼロのときの応答変数の増減分を意味する、すなわち他の全ての変数が平均（=0）であるときの、その変数そのものの効果（平均的効果）を意味することになる。 変数を標準化する前のモデルでは、パラメータ\\(\\beta_{1}\\)は「女性のときのxの傾き」であり、予測変数xそのものの効果を意味するものではなかった。 これに対し、標準化した後のモデルではx_stdの傾きを「変数xの平均的効果」として捉えることができる。x_stdの傾きが意味することは、「他の変数がゼロのとき、つまり平均であるときに、x_stdが1単位（1標準偏差）変化したときの応答変数の変化量」を意味することになり、係数の値をそのまま用いて直感的に解釈することができる。 交互作用項の係数も、一方の変数の効果が一定の場合（0のとき）、Mまたはxに追加分で係る効果として理解することができる。 12.8 まとめ この章では、予測変数がカテゴリカル変数の場合及び交互作用を含むモデルの場合を学んできた。 予測変数がカテゴリカル変数の場合は、0か1の値を取るダミー変数にして線形予測子に投入する。 3つ以上のカテゴリの場合は、カテゴリの数-1個分のダミー変数を線形予測子に投入する。 2つの予測変数の組み合わせの効果（交互作用）を見たい場合は、2つの予測変数の積を線形予測子に投入する。 予測変数を複数加えたときの各予測変数の傾きは、他の予測変数がゼロのときのその予測変数が応答変数に及ぼす効果を意味する。 線形予測子を拡張することで、正規分布を扱う様々な統計解析を線形モデルを扱う関数(lm)のみで行うことができることを学んできた。 この章では、応答変数は正規分布に従うという前提をおいてきたが、応答変数が従う確率分布を正規分布以外にすることも可能である。以降の章では、応答変数が従う確率分布を変更して一般化した「一般化線形モデル」を扱っていく。 12.9 確認問題 問1 Rで標準で入っているデータwarpbreaksを使って練習をする。 prac_dat_1 = warpbreaks #別の名前で保存する head(prac_dat_1) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_1, aes(x = wool, y = breaks)) ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_1, aes(x = tension, y = breaks)) 1-1 変数woolについて, 「Aを1, それ以外を0」としたダミー変数を作成し、そのダミー変数を予測変数、breaksを応答変数として線形モデルを行い、切片及びダミー変数に係る傾きの推定値を報告せよ。 また、ダミー変数の傾きの推定値からどのような結論が導かれるかを述べよ。 1-2 変数tensionについて, 「Lを1, それ以外を0」、「Mを1, それ以外を0」とした2種類のダミー変数を作成し、それら2つのダミー変数を予測変数、breaksを応答変数として線形モデルを行い、切片及び各ダミー変数に係る傾きの推定値を報告せよ。 更に、そのときの切片及び各ダミー変数の係数が意味することを説明せよ。 1-3 1-2で作ったダミー変数に加え、更に「Hを1, それ以外を0」としたダミー変数を追加で作成する。 更に、breaksから全体のbreaksの平均を引いた変数breaks_2を作成する。 それら3つのダミー変数を予測変数、breaks_2を応答変数として線形モデルを行い、各ダミー変数に係る傾きの推定値を報告せよ。ただし、モデルには切片の項は加えないものとする。 更に、そのときの各ダミー変数の係数が意味することを説明せよ。 問2 問1に引き続き、Rで標準で入っているデータwarpbreaksを使って練習をする。ただし、tensionがHの部分を除いたデータを用いる。 prac_dat_2 = subset(warpbreaks, tension != &quot;H&quot;) #tension == Hは除き、別の名前で保存する head(prac_dat_2) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L ggplot2::ggplot() + ggplot2::geom_boxplot(data = prac_dat_2, aes(x = wool, y = breaks, fill = tension)) breaksを応答変数、wool, tension, wool及びtensionの交互作用項を予測変数とした線形モデルを行い、切片、woolの傾き、tensionの傾き、交互作用項の推定値を報告せよ。 12.10 補足(emmeans) この章では、ダミー変数を複数加えたモデル及び交互作用を加えたモデルで一要因の分散分析や二要因の分散分析と同様のことができることを学んできた。ただし、分散分析では条件の効果が有意だった場合には、どの条件間で差があるかを多重比較補正をした上で比較するプロセスが必要となる。 ここでは、emmeansパッケージを用いてlm()関数で多重比較補正を行う手順を紹介する（詳細はemmeansパッケージのヘルプを参照のこと）。 library(emmeans) 12.10.1 一要因の分散分析における多重比較 dat = PlantGrowth |&gt; dplyr::mutate(weight_std = scale(weight)) result = lm(data = dat, weight_std ~ group) emm = emmeans::emmeans(result, ~ group) #モデルから推定された各カテゴリのmarginal mean(emmeans)を算出する emm ## group emmean SE df lower.CL upper.CL ## ctrl -0.0585 0.281 27 -0.6353 0.5184 ## trt1 -0.5876 0.281 27 -1.1644 -0.0107 ## trt2 0.6460 0.281 27 0.0692 1.2229 ## ## Confidence level used: 0.95 emmeans::contrast(emm, method = &quot;pairwise&quot;, adjust = &quot;tukey&quot;) #pairwiseで条件の組み合わせごとのemmeansの比較の結果が表示される。adjustで多重比較補正の方法を指定できる。 ## contrast estimate SE df t.ratio p.value ## ctrl - trt1 0.529 0.398 27 1.331 0.3909 ## ctrl - trt2 -0.705 0.398 27 -1.772 0.1980 ## trt1 - trt2 -1.234 0.398 27 -3.103 0.0120 ## ## P value adjustment: tukey method for comparing a family of 3 estimates 12.10.2 二要因の分散分析における単純主効果検定 dat = warpbreaks |&gt; dplyr::mutate(breaks_std = scale(breaks)) result = lm(data = dat, breaks_std ~ wool * tension) emm = emmeans::emmeans(result, ~ wool * tension) emm ## wool tension emmean SE df lower.CL upper.CL ## A L 1.24311 0.276 48 0.688 1.799 ## B L 0.00561 0.276 48 -0.550 0.561 ## A M -0.31429 0.276 48 -0.870 0.241 ## B M 0.04770 0.276 48 -0.508 0.603 ## A H -0.27219 0.276 48 -0.828 0.283 ## B H -0.70995 0.276 48 -1.265 -0.154 ## ## Confidence level used: 0.95 pairs(emm, simple = &quot;wool&quot;) #wool内でtension間の比較を行う ## tension = L: ## contrast estimate SE df t.ratio p.value ## A - B 1.238 0.391 48 3.167 0.0027 ## ## tension = M: ## contrast estimate SE df t.ratio p.value ## A - B -0.362 0.391 48 -0.926 0.3589 ## ## tension = H: ## contrast estimate SE df t.ratio p.value ## A - B 0.438 0.391 48 1.120 0.2682 pairs(emm, simple = &quot;tension&quot;) #tension内でwool間の比較を行う ## wool = A: ## contrast estimate SE df t.ratio p.value ## L - M 1.5574 0.391 48 3.986 0.0007 ## L - H 1.5153 0.391 48 3.878 0.0009 ## M - H -0.0421 0.391 48 -0.108 0.9936 ## ## wool = B: ## contrast estimate SE df t.ratio p.value ## L - M -0.0421 0.391 48 -0.108 0.9936 ## L - H 0.7156 0.391 48 1.831 0.1704 ## M - H 0.7577 0.391 48 1.939 0.1389 ## ## P value adjustment: tukey method for comparing a family of 3 estimates "],["12-glm_logistic.html", "Chapter 13 一般化線形モデル（ロジスティック回帰） 13.1 準備 13.2 一般化線形モデル 13.3 ロジスティック回帰 13.4 まとめ 13.5 確認問題", " Chapter 13 一般化線形モデル（ロジスティック回帰） 確率分布が正規分布以外の場合の「一般化線形モデル」について学ぶ。まずは、ロジスティック回帰について学ぶ。 13.1 準備 データの可視化のために、ggplot2パッケージをロードする。 更に、MASSパッケージ内のサンプルデータを使う。 library(ggplot2) library(MASS) 13.2 一般化線形モデル 線形モデルは、以下の式で表されるモデルであった。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\sum_{k=1}^{K} \\beta_{k} x \\\\ \\tag{1} y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] 線形モデルでは、応答変数が正規分布に従うという前提で、応答変数\\(y\\)を予測するパラメータ（線形予測子の切片と係数、及び正規分布の分散）を求めた。 今回は、応答変数が正規分布以外の確率分布に従うモデルを扱う。 線形モデルを正規分布以外の確率分布に拡張したモデルを、一般化線形モデル(generalized linear model)という（GLMと略されることも多い）。一般化線形モデルを理解する上で重要なのは、応答変数が従う確率分布に加え、リンク関数(link function)という考え方である。 確率分布とリンク関数の組み合わせよって様々な分析を表現することができる。 13.3 ロジスティック回帰 前の章までは応答変数が量的変数の例を扱ってきた。では、応答変数がカテゴリカル変数である場合は、どのような解析をすればよいのだろうか。 応答変数が二値のカテゴリカル変数の場合を例として見ていく。 MASSパッケージに入っているサンプルデータbiopsyを使いながら検討していこう。まず、以下のプログラムを実行して、練習用のデータdatを作成する。 library(MASS) dat = biopsy dat$y = ifelse(dat$class == &quot;malignant&quot;, 1, 0) #classがbenignならばゼロ、それ以外なら1という変数yを作る dat$x = dat$V1 #V1という変数をxという名前に変える head(dat) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class y x ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 5 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 5 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 3 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 6 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 4 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 8 xは整数の変数、yは1ならば癌、0ならば癌ではないことを意味する変数とする。 xが変化すると癌である確率が変化するかを検討したい。 まず、xとyとの関係を図で確認してみる。 ggplot2パッケージで、xをx軸、yをy軸にしてプロットしてみよう。 普通にgeom_pointで散布図を作っても点が重なって見にくいので、geom_jitterを使って描画する。geom_jitterは、ランダムで点をずらして描画してくれる。 ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) では、前章までで学んだとおりに、xを予測変数、yを応答変数とした線形モデルでxの効果を検討しよう。 \\[ \\begin{equation} \\hat{y} = \\alpha + \\beta x \\\\ \\tag{2} y \\sim \\text{Normal}(\\hat{y}, \\sigma) \\end{equation} \\] result_lm = lm(data = dat, y ~ 1 + x) summary(result_lm) ## ## Call: ## lm(formula = y ~ 1 + x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77804 -0.17331 -0.01994 0.06859 1.06859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.189535 0.023395 -8.102 2.43e-15 *** ## x 0.120947 0.004467 27.078 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3323 on 697 degrees of freedom ## Multiple R-squared: 0.5127, Adjusted R-squared: 0.512 ## F-statistic: 733.2 on 1 and 697 DF, p-value: &lt; 2.2e-16 xに係る傾きの推定値を数値通りに解釈すると、「xが1単位増えると、yが0.12増える」ことを示している。 では、求めた傾きと切片から直線を先程のxとyとの関係の図に引いてみよう。 predict_lm = predict(result_lm, interval = &quot;confidence&quot;, level = 0.95) #直線の95%信頼区間を求める dat_predict = cbind(dat, predict_lm) ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = dat_predict, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.5) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) 線形モデルから推定された直線は、「xが増えるほどyが増える」関係を表しているように見える。 しかし、この線形モデルの結果は、yを予測する上で問題がある。 解析の目的は、\\(y = 1\\)の確率、つまりがんにかかる確率を推定することであるが、例えば\\(x\\)が10を超えると、応答変数の予測値は1以上の値を取る。また、\\(x\\)が2.5を下回ったときも、0未満の数値が推定されてしまう。応答変数は0か1しか取らないのに、それぞれを超える値が予測されてしまう。これは確率の推定としては不都合である。 応答変数\\(y\\)は連続量ではなく、0か1の値を取るカテゴリカル変数である。連続量の確率分布である正規分布に応答変数が従うという前提を置くのは予測モデルとして適切ではない。 ではどうすれば良いのか？ 解決策として、モデルを以下のように変更する。 \\[ \\begin{equation} q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\\\ \\tag{3} y \\sim \\text{Binomial}(1, q) \\end{equation} \\] \\(\\exp(\\alpha + \\beta x)\\)は、\\(e^{(\\alpha + \\beta x)}\\)とも表記できる。 \\(y = 1\\)である確率（がんである確率）を\\(q\\)とする。 13.3.1 応答変数が従う確率分布 まず、式(3)の2つ目の式が何を意味しているのかを確認する。 \\[ y \\sim \\text{Binomial}(1, q) \\] これは、応答変数\\(y\\)が試行回数1回、成功確率\\(q\\)の二項分布に従うということを示している。 例として、二項分布から乱数を作るrbinom()関数を使って、試行回数1回、成功確率\\(q\\)を0.5とした二項分布から乱数を20個を生成してみる。 q = 0.5 rand = rbinom(n =20, size = 1, prob = q) rand ## [1] 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 このように、0か1が生成される。 13.3.2 リンク関数 式(3)の1つ目は何を意味しているのか？以下の式について、\\(z = \\alpha + \\beta x\\)として、\\(z\\)を変化させると\\(q\\)がどう変化するか図で見てみよう。 z = seq(-10, 10, 0.1) #-10から10まで0.1刻みのベクトルzを作成 q = exp(z)/(1+exp(z)) #上の式にzを代入して、qを求める d = data.frame(z = z, q = q) #グラフを作るために、データフレームを作る ggplot2::ggplot()+ ggplot2::geom_line(data = d, aes(x=z, y=q)) \\(z\\)は\\(-\\infty\\)から\\(\\infty\\)の範囲を取るが、\\(z\\)がどのような値をとっても、\\(0&lt;q&lt;1\\)となる（限りなく0もしくは1に近づく）。\\(q\\)は確率なので、この0から1の範囲に収まるようになる変換は都合が良い。 また、式(3)の一つ目は、 \\[ q = \\frac{\\exp(\\alpha + \\beta x)}{1+\\exp(\\alpha + \\beta x)} \\] 右辺を線形予測子にして整理すると、以下のようにできる。 \\[ \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\] この変換は、ロジット関数（logit function）と呼ばれる。 13.3.3 ここまでのまとめ 線形予測子を変換する関数は、「リンク関数」と呼ばれる。上の例のように、応答変数が二値の場合は、推定値を0から1に収めるためにロジット関数をリンク関数として使うのが適切である。 このように、「応答変数が従う確率分布」と「線形予測子に変換をほどこすリンク関数」を選ぶことにより、線形モデルを様々なデータ解析に一般化させたものが一般化線形モデル(generalized linear model)である。一般化線形モデルは、「確率分布」と「リンク関数」を応答変数のタイプに応じてカスタマイズするというイメージで捉えると良い。 上の例で見た「応答変数が従う確率分布をベルヌーイ分布（または二項分布）」、「リンク関数をロジスティック関数（ロジット関数）」とした一般化線形モデルは、ロジスティック回帰と呼ばれる。 13.3.4 Rでのロジスティック回帰 Rには、一般化線形モデルを扱うための関数glm()が用意されている。線形モデルを扱うlm()と同じ要領でプログラムを書けばよいが、確率分布とリンク関数のオプションを自分で指定する必要がある。先程のサンプルデータdatで、glm()関数を使ってロジスティック回帰をやってみよう。 result_glm = glm(data = dat, y ~ 1 + x, family = binomial(link=&quot;logit&quot;)) glmで設定すること： 「線形予測子」、「応答変数が従う確率分布」、「リンク関数」を指定する。 familyで、応答変数が従う確率分布を指定する。 family = binomial、すなわち二項分布（binomial distribution）に従うとする。（式(3)で示しているように正確にはベルヌーイ分布であるが、binomialで構わない） (link=)で、リンク関数を指定する。ロジット関数(logit)を指定しよう。 ちなみに、(link=\"logit\")は省略してもかまわない。family=binomialとすると、デフォルトでリンク関数をlogitとしてくれる。 では、出力結果を見てみよう。 summary(result_glm) ## ## Call: ## glm(formula = y ~ 1 + x, family = binomial(link = &quot;logit&quot;), data = dat) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## x 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 出力はlm()と似ている。Coefficientsの部分を見よう。Estimateがパラメータの推定結果である。Prがp値である。パラメータの推定値は、プラスならば応答変数が1の値、マイナスならば応答変数が0の値を取りやすいことを意味する。 xに係る傾きの値0.94は何を意味しているのか？ 線形モデルでは傾きの推定値は、「予測変数が1単位増えたときの応答変数の変化量」を意味していた。今回の例も、xが1増えると確率が0.94上がるということを示しているのか？ 一般化線形モデルの場合、係数の値が意味することの解釈には注意が必要である。 \\[ \\begin{equation} \\log\\frac{q}{1-q} = \\alpha + \\beta x \\\\ \\end{equation} \\] 右辺を線形の式とすると、左辺は対数オッズとなる。つまり、\\(x\\)に係る傾き\\(\\beta\\)は、「\\(x\\)が1増えた時の\\(q\\)の対数オッズの変化量」を意味しており、確率\\(q\\)そのものの変化量ではない。このように、正規分布以外の確率分布を用いた一般化線形モデルでは、係数そのものの値を解釈するのが難しくなる点に注意が必要である。 対数オッズと確率\\(q\\)との関係を図で見てみよう。x軸を\\(\\log(q/[1-q])\\)、y軸を\\(q\\)とした図を示す。 q = seq(0, 1, 0.01) logit = log(q/(1-q)) sample_dat = data.frame(q = q, logit = logit) ggplot2::ggplot() + ggplot2::geom_line(data = sample_dat, aes(x = logit, y = q)) つまり、対数オッズがプラスだと確率\\(q\\)は0.5より大きくなり、対数オッズがマイナスだと確率\\(q\\)は0.5より小さくなる関係にある。要は、対数オッズがプラスだと\\(y = 1\\)が起こりやすくなり、マイナスだと起こりにくくなることを意味している。 求めた係数の推定値を元に、確率を予測する線を引いてみよう。 new = data.frame(x = seq(0, 11, 0.1)) predict_glm = predict(result_glm, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot2::ggplot() + ggplot2::geom_jitter(data = dat, aes(x = x, y = y), height = 0.05) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = y)) + ggplot2::scale_y_continuous(breaks = seq(0,1,0.1)) 予測線は0から1の範囲に収まっており、線形予測子から確率の予測ができている。 13.4 まとめ 応答変数が二値のデータや割合である場合は、ロジスティック回帰を用いる。 確率分布はベルヌーイ分布もしくは二項分布を設定する。 リンク関数はロジット関数を設定する。 13.5 確認問題 この章では予測変数が1つの場合で練習してきたが、一般化線形モデルでももちろん予測変数を複数加えたモデルを扱うことができる（第11章参照）。以降の確認問題では、予測変数が複数のケースで練習する。 以下のプログラムを実行し、サンプルデータを作成する。 変数の意味は以下の通りである。 Disease: ある病気にかかっているか（1=かかっている、0=かかっていない） BMI: BMI（肥満度を表す指標） Exercise: 1週間あたりの運動時間（単位：時間） Sleep: 1日の睡眠時間（単位：時間） Disease = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) BMI = c(15, 16, 16, 18, 19, 20, 21, 22, 22, 23, 23, 23, 24, 24, 24, 30, 31, 31, 33, 34, 34, 34, 35, 36, 40, 40, 40, 41, 43, 43) Exercise = c(2, 1, 1, 2, 0, 3, 1, 1, 4, 4, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 0, 2, 2, 3, 0, 4) Sleep = c(7, 4, 5, 4, 4, 6, 5, 6, 4, 6, 4, 7, 4, 7, 4, 6, 5, 4, 5, 6, 7, 5, 4, 6, 4, 7, 5, 5, 4, 7) data_q01 = data.frame(Disease = Disease, BMI = BMI, Exercise = Exercise, Sleep = Sleep) Diseaseを応答変数、BMI、Exercise、Sleepの3つを予測変数としたロジスティック回帰を行い、それぞれの予測変数の係数について報告せよ。また、5%水準で有意な効果を持っていた予測変数を報告せよ。 "],["13-glm_poisson.html", "Chapter 14 一般化線形モデル（ポアソン回帰） 14.1 準備 14.2 ポアソン回帰 14.3 Rでのポアソン回帰 14.4 ポアソン回帰の注意点：過分散 14.5 過分散への対処法 14.6 まとめ 14.7 確認問題", " Chapter 14 一般化線形モデル（ポアソン回帰） 「一般化線形モデル」のもう一つのバリエーションである、ポアソン回帰について学ぶ。 14.1 準備 データの可視化のために、ggplot2パッケージをロードする。 library(ggplot2) 14.2 ポアソン回帰 一般化線形モデルは、応答変数が従う確率分布として正規分布以外の確率分布も扱うモデルであった。前章では一般化線形モデルの一つとして応答変数が0か1の二値の場合、ロジスティック回帰について学んできた。 同じく応答変数の範囲に制約がある場合の例として、次は応答変数が正の値の整数しか取らない場合（0を含む）を扱う。 具体的には、応答変数がカウントデータの場合である（非負の整数。0個、1個、2個,3個といった個数など）。この場合は、ポアソン回帰と呼ばれる一般化線形モデルを扱うのが適切とされている。 サンプルデータを用いながら、ポアソン回帰について学んでいこう。以下のプログラムを実行して、サンプルデータdat_poissonを作成しよう。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd=1) lambda = exp(0.01+ 0.6*x) y = rpois(n=N, lambda = lambda) dat_poisson = data.frame(y=y, x=x) xとyの関係を散布図で確認してみる。 ggplot2::ggplot()+ ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) xが大きいほど、yが大きいという関係がありそうである。\\(x\\)から、\\(y\\)を予測する。 まずは、線形モデルを当てはめてみよう。 model = lm(data = dat_poisson, y ~ 1 + x) summary(model) ## ## Call: ## lm(formula = y ~ 1 + x, data = dat_poisson) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0384 -1.3050 -0.0837 0.5879 8.3524 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.0977 1.0022 -2.093 0.0417 * ## x 2.9887 0.4442 6.728 1.92e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.585 on 48 degrees of freedom ## Multiple R-squared: 0.4853, Adjusted R-squared: 0.4746 ## F-statistic: 45.26 on 1 and 48 DF, p-value: 1.924e-08 求めた傾きと切片をもとに、yを予測する直線を引いてみよう。 predict_lm = predict(model, interval = &quot;confidence&quot;, level = 0.95) #直線の95%信頼区間を求める dat_predict = cbind(dat_poisson, predict_lm) ggplot2::ggplot() + ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = dat_predict, aes(x = x, ymax = upr, ymin = lwr), alpha = 0.5) + ggplot2::scale_y_continuous(breaks = seq(0,20,1)) 直線の左側が、0より下にはみ出てしまっている。\\(y\\)は正の値を取る離散値（整数）である。しかし、線形モデルで求めた直線の式ではマイナスの値も予測されてしまう。 ポアソン回帰は、この問題を解消してくれる。ポアソン回帰を数式で表すと、以下のようになる。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\tag{4}\\\\ y \\sim \\text{Poisson}(\\lambda) \\end{equation} \\] 14.2.1 応答変数が従う確率分布 まず、2つ目の式は、 \\[ y \\sim \\text{Poisson}(\\lambda) \\] \\(\\lambda\\)をパラメータとするポアソン分布から、応答変数\\(y\\)が生成されることを示している。 例えば、以下にポアソン分布から乱数を生成するrpois()関数を使って、\\(\\lambda\\)が3のポアソン分布から乱数を20個作ってみよう。 lambda = 3 rand = rpois(n = 20, lambda = lambda) rand ## [1] 3 3 2 3 3 1 3 1 2 2 2 5 3 4 5 2 1 2 4 2 正の離散値（整数）が生成される。 ポアソン分布は、パラメータ\\(\\lambda\\)を持つ確率分布である。 \\[ P(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!} \\\\ \\] \\(y\\)は0以上の整数（0, 1, 2, 3, …）、\\(P(y)\\)は\\(y\\)が生じる確率とする。 ポアソン分布のかたちを決定づけるパラメータは、\\(\\lambda\\)のみである。\\(\\lambda\\)は、ポアソン分布の期待値（平均）と分散の両方を意味する。つまり、ポアソン分布は平均と分散が等しい分布である。 set.seed(1) x = rpois(n = 30, lambda = 2) #ポアソン分布から乱数を生成する関数 lambda =2のポアソン分布から30個乱数を生成 x #整数が生成される ## [1] 1 1 2 4 1 4 4 2 2 0 1 1 3 1 3 2 3 6 1 3 4 1 2 0 1 1 0 1 4 1 mean(x) ## [1] 2 var(x) ## [1] 2.206897 d = data.frame(x = x) ggplot2::ggplot() + ggplot2::geom_histogram(data = d, aes(x=x)) ## `stat_bin()` using `bins = 30`. Pick ## better value `binwidth`. 以下に、パラメータ\\(\\lambda = 1\\), \\(\\lambda = 2\\), \\(\\lambda = 3\\)それぞれの場合のポアソン分布を図で示す。 ポアソン分布は、二項分布とも関連している。 二項分布のパラメータは、試行回数\\(n\\)と成功確率\\(p\\)であった。二項分布の期待値（平均）は\\(np\\)、分散は\\(np(1-p)\\)である。 \\[ y \\sim \\text{Binomial}(n, p) \\\\ E(y) = np\\\\ Var(y) = np(1-p)\\\\ \\] 二項分布の試行回数\\(n\\)が大きく、成功確率\\(p\\)が小さい場合、二項分布の平均と分散はほとんど等しくなり、ポアソン分布に近似する。つまり、めったに起こらないイベントが生じる回数は、ポアソン分布に従うとされている。 14.2.2 リンク関数 線形予測子とポアソン分布のパラメータ\\(\\lambda\\)との関係をもう一度確認しよう。 \\[ \\lambda = \\exp(\\alpha + \\beta x) \\\\ \\] なぜ指数関数（\\(\\exp()\\)）を用いるのか？\\(z=\\alpha + \\beta x\\)として、\\(\\lambda=\\exp(z)\\)との関係を図で見てみよう。 z = seq(-5, 5, 0.1) #-10から10まで0.1刻みのベクトルzを作成 lambda = exp(z) #上の式にzを代入して、lambdaを求める d = data.frame(z = z, q = lambda) #グラフを作るために、データフレームを作る ggplot2::ggplot()+ ggplot2::geom_line(data = d, aes(x=z, y=lambda)) 図からもわかるように、\\(z\\)の値に関わらず、\\(\\lambda\\)は常に正の値を取る。ポアソン分布のパラメータ\\(\\lambda\\)は\\(\\lambda&gt;0\\)という制約があるため、このような変換をする必要がある。 また、式の右辺を線形予測子にして整理すると、以下の式になる。 \\[ \\log\\lambda_{i}=\\alpha+\\beta x \\\\ \\] つまり、ポアソン回帰では、線形予測子と応答変数をリンクさせるリンク関数として対数関数（log）を設定する。 14.3 Rでのポアソン回帰 Rでポアソン回帰をやってみよう。一般化線形モデルを扱う関数glm()で、以下のように確率分布にポアソン分布、リンク関数に対数を指定する。 なお、(link = \"log\")は省略しても構わない。family = poissonで確率分布をポアソン分布に指定すれば、自動でリンク関数を対数にしてくれる。 result_poisson = glm(data = dat_poisson, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_poisson) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.6829 0.2807 -2.433 0.015 * ## x 0.8951 0.1052 8.506 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 130.32 on 49 degrees of freedom ## Residual deviance: 46.52 on 48 degrees of freedom ## AIC: 197.96 ## ## Number of Fisher Scoring iterations: 5 推定された式が\\(y\\)をうまく予測できているか、図で確認してみよう。 new = data.frame(x = seq(0, 4, 0.1)) predict_glm = predict(result_poisson, newdata = new, type = &quot;response&quot;) dat_predict = data.frame(new, y = predict_glm) ggplot2::ggplot() + ggplot2::geom_point(data = dat_poisson, aes(x=x, y=y)) + ggplot2::geom_line(data = dat_predict, aes(x = x, y = y)) ゼロよりも大きい値が予測されており、予測線も各データ（点）の近くに位置している。 ロジスティック回帰のときと同様に、ポアソン回帰の予測変数の傾きの値も単純に「予測変数が1単位増えたときの応答変数の変化量」を意味するわけではない点に注意が必要である。式(10)をもう一度確認すると、 \\[ \\log\\lambda_{i}=\\alpha+\\beta x \\\\ \\] であった。つまり、予測変数の傾きは「その予測変数が1単位増えたときの、応答変数（パラメータ\\(\\lambda\\)）の対数の変化量」を意味する。しかし、これでは数値をどう解釈すればいいのか直感的に理解しにくい。 式(10)は、以下の式にも直すことができる。 \\[ \\begin{equation} \\lambda = \\exp(\\alpha + \\beta x) \\\\ \\lambda = \\exp(\\alpha)\\exp(\\beta x)\\\\ \\end{equation} \\] つまり、予測変数の傾きを指数関数で変換した値が、応答変数（パラメータ\\(\\lambda\\)）の変化量を意味する。ポアソン回帰の係数を解釈する際には、係数を指数関数で変換した後の値を使う方が解釈がしやすい。 14.4 ポアソン回帰の注意点：過分散 次のプログラムを実行して、サンプルデータdat_disを作成する。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd = 1) e = rnorm(n=N, mean = 0, sd = 1) lambda = exp(0.01 + 0.1*x + e) y = rpois(n=N, lambda = lambda) dat_dis = data.frame(y=y, x=x) 先ほどと同様に、このデータの変数xとyを用いて、xからyを予測するポアソン回帰をやってみる。 result_dis = glm(data = dat_dis, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_dis) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_dis) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1417 0.3139 -0.451 0.65177 ## x 0.4393 0.1268 3.465 0.00053 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.37 on 49 degrees of freedom ## Residual deviance: 153.39 on 48 degrees of freedom ## AIC: 260.41 ## ## Number of Fisher Scoring iterations: 6 xはyを予測する上で、かなり強い効果を有しているように見える。しかし、単純にそのような結論を出すことはできない。このデータでは、分散が平均よりも過剰に大きい過分散が生じているためである。 mean(dat_dis$y) ## [1] 2.32 var(dat_dis$y) ## [1] 12.8751 14.4.1 過分散とは？ （yのヒストグラム。白いバーが実際のyの分布、薄い赤のバーはパラメータ\\(\\lambda\\)がyの実際の平均と等しい場合のポアソン分布） ポアソン分布は、平均と分散が等しい分布である。しかし、現実にはデータの平均と分散が等しいケースは少ない。逆に、分散が平均よりもかなり大きいケースがよく見られる。 分散が平均よりも大きいときにポアソン回帰を使うと、分散を実際よりも小さいと推定してしまい、予測変数が平均に与える効果を過大に評価してしまう恐れがある（p値を過剰に小さく判断してしまい、第一種の過誤を犯しやすくなる。ポアソン回帰を行うとかなり低いp値が出ることが多いのは、このためである）。この問題は、過分散(overdispersion)と呼ばれる。 14.4.2 過分散の確認方法 performanceパッケージのcheck_overdispersion()関数で過分散の有無を確認することができる。 library(performance) performance::check_overdispersion(result_dis) ## # Overdispersion test ## ## dispersion ratio = 4.048 ## Pearson&#39;s Chi-Squared = 194.291 ## p-value = &lt; 0.001 ## Overdispersion detected. このデータでは過分散が生じてしまっていることがわかる。 14.5 過分散への対処法 ポアソン回帰で過分散が疑われる場合、対処法としては例えば以下の方法がある。 応答変数が従う確率分布として、負の二項分布を用いたモデルで分析する。 一般化線形混合モデルで、個体差を意味するパラメータを追加したモデルを用いる（分散を個体差パラメータに吸収させる）。 対処法1については、次の章で解説する（対処法2については、「マルチレベルモデル」の章で説明する）。 14.6 まとめ 応答変数がカウントデータである場合は、ポアソン回帰を用いる。 確率分布はポアソン分布を設定する。 リンク関数は対数関数を設定する。 14.7 確認問題 以下のプログラムを実行し、サンプルデータを作成する。 変数の意味は以下の通りである。 Birds: その日に観測した鳥の数 Temp: 気温（摂氏） Cloud: 天気（0 = 晴れ, 1 = くもり） Humid: 湿度（%） Birds = c(2, 9, 8, 3, 6, 6, 5, 5, 7, 2, 9, 3, 13, 5, 7, 5, 5, 10, 10, 13) Temp = c(23.8, 25.3, 26.1, 22.7, 25.4, 25.5, 24.4, 24.5, 24.4, 24.1, 24.5, 24.0, 24.2, 25.1, 26.0, 24.9, 24.5, 24.1, 24.2, 27.4) Cloud = c(1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1) Humid = c(50.3, 49.0, 49.1, 50.9, 48.6, 47.1, 51.1, 48.0, 50.0, 48.1, 52.2, 49.0, 48.6, 49.0, 46.7, 47.7, 45.6, 47.3, 49.4, 49.1) data_q02 = data.frame(Birds = Birds, Temp = Temp, Cloud = Cloud, Humid = Humid) Birdsを応答変数、Temp、Cloud、Humidの3つを予測変数としたポアソン回帰を行い、それぞれの予測変数の係数について報告せよ。また、5%水準で有意な効果を持っていた予測変数を報告せよ。 "],["14-glm_disperse.html", "Chapter 15 一般化線形モデル（過分散への対処） 15.1 準備 15.2 負の二項回帰 15.3 ゼロ過剰ポアソンモデル 15.4 ゼロ過剰ポアソンモデルの詳細 15.5 確認問題", " Chapter 15 一般化線形モデル（過分散への対処） この章では、カウントデータをその他の一般化線形モデルを紹介する。それぞれ、過分散のあるデータの場合にポアソン回帰の代替となるモデルになり得る。 負の二項回帰 ゼロ過剰ポアソン回帰 15.1 準備 可視化のためのggplot2パッケージに加え、MASS、psclパッケージを使う。 MASSパッケージは負の二項分布を用いたモデルのときに、psclパッケージはゼロ過剰ポアソン回帰のときに必要になる。初めて使う際には、事前にインストールが必要なので注意。 library(ggplot2) library(MASS) library(pscl) 15.2 負の二項回帰 前の章で、応答変数がカウントデータの場合、ポアソン回帰で解析するのが適切であると学んだ。しかし、実際のデータは分散が平均よりも大きい場合が多く、平均と分散が等しいという前提のポアソン分布を用いると予測変数の効果を誤って判断してしまう恐れがある。これが、過分散（overdispersion）と呼ばれる問題である。 過分散対策として、応答変数が従う分布としてポアソン分布の代わりに、負の二項分布(negative binomial distribution)を用いる方法がよく使われる。 15.2.1 負の二項分布 例えばコインを投げて表が出る確率を0.5として、表が3回出るまで投げると決めたとする。8回投げたところで表が3回出た場合、表が3回出る確率は以下から求めることができる。 choose(8-1, 3-1) * 0.5^2 * (1-0.5)^(8-3) * 0.5 #つまり、7回中表が2回、裏が5回出て、最後の１回で表が出る確率を求める。 ## [1] 0.08203125 これを一般化した式が以下である。成功確率を\\(q\\)、成功回数を\\(r\\)、全試行数（成功回数+失敗回数）を\\(n\\)とした場合の確率\\(P(n)\\)を表している。（上の例の場合は、\\(q=0.5\\), \\(r=3\\), \\(n=8\\)である） \\[ P(n) = {}_{n-1}\\mathrm{C}_{r-1} q^{r}(1-q)^{n-r} \\] 失敗回数を\\(x\\)として、以下のように置き換えることもできる。ある事象が\\(r\\)回生じるまでに、\\(x\\)回失敗する確率と言い換えることができる。この確率分布を負の二項分布という。 \\[ P(x) = {}_{x+r-1}\\mathrm{C}_{r-1} q^{r}(1-q)^{x} \\] Rでもnbinomで負の二項分布の確率を計算することができる。 x = 0:10 p_y = dnbinom(x = x, size = 3, prob = 0.5) #x = 失敗回数, size = 成功回数, prob = 成功確率 d_plot = data.frame(x = x, p_y = p_y) ggplot2::ggplot() + ggplot2::geom_bar(data = d_plot, aes(x = factor(x), y = p_y), stat = &quot;identity&quot;) + ggplot2::labs(x = &quot;number of failures&quot;, y = &quot;probability&quot;, title = &quot;number of success = 3&quot;) 期待値\\(E(x)\\)と分散\\(Var(x)\\)は、以下から計算される。 \\[ E(x) = \\frac{r(1-q)}{q}\\\\ Var(x) = \\frac{r(1-q)}{q^2}\\\\ \\] dnbinom()に期待値muを入力しても、確率を計算してくれる。 dnbinom(x = x, size = 3, prob = 0.5) ## [1] 0.125000000 0.187500000 0.187500000 0.156250000 0.117187500 0.082031250 ## [7] 0.054687500 0.035156250 0.021972656 0.013427734 0.008056641 dnbinom(x = x, size = 3, mu = 3*0.5/(1-0.5)) ## [1] 0.125000000 0.187500000 0.187500000 0.156250000 0.117187500 0.082031250 ## [7] 0.054687500 0.035156250 0.021972656 0.013427734 0.008056641 期待値を\\(E(x)=\\mu\\)とすると、分散は\\(Var(x)=\\mu + \\mu^{2}/r\\)で、分散が期待値（平均）よりも\\(\\mu^{2}/r\\)大きい。負の二項分布によって、分散が平均よりも大きい分布を扱うことができる。 15.2.2 Rでの負の二項回帰 Rでは、MASSパッケージに含まれているglm.nb()関数で、負の二項回帰を扱うことができる。Rに入っているwarpbreaksをサンプルデータとして、ポアソン回帰と負の二項回帰の結果を比較してみよう。 d = warpbreaks #別の名前(d)で保存する d$A &lt;- ifelse(d$wool == &quot;A&quot;, 1, 0) #Aなら1, Bなら0のダミー head(d) ## breaks wool tension A ## 1 26 A L 1 ## 2 30 A L 1 ## 3 54 A L 1 ## 4 25 A L 1 ## 5 70 A L 1 ## 6 52 A L 1 ggplot2::ggplot() + ggplot2::geom_histogram(data = d, aes(x = breaks, fill = wool), binwidth = 1) breaksに対するwool(A or B)の効果を検討する。まずは、ポアソン回帰の結果を見てみる。breaksを\\(y\\)、Aを\\(x\\)とすると、モデルは以下のように表現できる。 \\[ \\lambda = \\alpha + \\beta x\\\\ y \\sim \\text{Poisson}(\\lambda) \\] model_poisson = glm(data = d, breaks ~ 1 + A, family = poisson(link = &quot;log&quot;)) summary(model_poisson) ## ## Call: ## glm(formula = breaks ~ 1 + A, family = poisson(link = &quot;log&quot;), ## data = d) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.22919 0.03829 84.331 &lt; 2e-16 *** ## A 0.20599 0.05157 3.994 6.49e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 297.37 on 53 degrees of freedom ## Residual deviance: 281.33 on 52 degrees of freedom ## AIC: 560 ## ## Number of Fisher Scoring iterations: 4 mean(d$breaks) ## [1] 28.14815 var(d$breaks) ## [1] 174.2041 Aに係る傾きの推定値について、かなり小さいp値が推定されている。 次に、負の二項回帰の結果と比較してみよう。 \\[ \\mu = \\alpha + \\beta x\\\\ y \\sim \\text{NegativeBinom}(\\mu, r) \\] MASSパッケージのglm.nb()を使う。 model_nb = MASS::glm.nb(data = d, breaks ~ 1 + A) #lm関数と同じ要領で、線形の式を入力する。確率分布はオプションで指定しないで良い。 summary(model_nb) ## ## Call: ## MASS::glm.nb(formula = breaks ~ 1 + A, data = d, init.theta = 6.960797279, ## link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.22919 0.08238 39.197 &lt;2e-16 *** ## A 0.20599 0.11533 1.786 0.0741 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(6.9608) family taken to be 1) ## ## Null deviance: 57.400 on 53 degrees of freedom ## Residual deviance: 54.212 on 52 degrees of freedom ## AIC: 419.97 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 6.96 ## Std. Err.: 1.63 ## ## 2 x log-likelihood: -413.975 ポアソン回帰と比べるとAのp値が大きくなり、過分散が解消されたことがうかがえる。 15.3 ゼロ過剰ポアソンモデル ゼロ過剰ポアソンモデル(Zero-inflated Poisson model)は、データにゼロが多いカウントデータに適用されるモデルであり、ベルヌーイ分布とポアソン分布を混合させた統計モデルである。 15.3.1 例題 夏休みの間に毎日、カブトムシを採りに森に出かけた。カブトムシが何匹か見つかる日もあれば、全く見つからない日もある。その日の天気や気温などがカブトムシが見つかる確率に影響を及ぼしていたかを検討する。 以下のプログラムを実行し、サンプルデータを作成する。yがその日見つかったカブトムシの数、Rainがその日に雨が降っていたか（1=雨, 0=雨ではない）、HumidityとTemperatureはそれぞれ湿度と気温とする。 y = c(1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 4, 4, 0, 0, 0, 3, 0, 1, 1, 7, 0, 0, 5, 1, 4, 0, 2) Rain = c(0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0) Humidity = c(50, 50, 59, 58, 56, 59, 58, 51, 30, 56, 49, 48, 35, 45, 54, 64, 49, 54, 49, 36, 46, 46, 49, 61, 58, 48, 47, 57, 56, 43) Temperature = c(29, 30, 31, 30, 31, 30, 29, 30, 29, 31, 32, 30, 29, 31, 30, 32, 30, 31, 30, 29, 30, 28, 31, 30, 32, 30, 29, 31, 29, 29) d_zip = data.frame(y = y, Rain = Rain, Humidity = Humidity, Temperature = Temperature) head(d_zip) ## y Rain Humidity Temperature ## 1 1 0 50 29 ## 2 2 0 50 30 ## 3 0 1 59 31 ## 4 0 1 58 30 ## 5 2 0 56 31 ## 6 0 1 59 30 ヒストグラムでデータの頻度を確認する。このデータにはゼロが非常に多く、ほとんどの日でカブトムシが見つからなかったことがわかる。 ggplot2::ggplot() + ggplot2::geom_histogram(data = d_zip, aes(x = y), binwidth = 1, fill = &quot;white&quot;, color=&quot;black&quot;) + theme_classic() 15.4 ゼロ過剰ポアソンモデルの詳細 ゼロ過剰ポアソンモデルは、ロジスティック回帰とポアソン回帰を混合させたモデルである。 1. ベルヌーイ分布から過剰なゼロが生じる確率を推定する まず、ロジスティック回帰のモデルからゼロが生じる過程を推定する。ゼロが生じる確率を\\(q\\)とする。 2. ポアソン分布から応答変数が生じる確率を推定する 確率\\(1-q\\)のときに、ポアソン回帰のモデルに従い、応答変数（ゼロを含む正の整数）が生じるとする。 ゼロが生じる確率\\(Pr(0|q,\\lambda)\\)とカウントデータである\\(y\\)(\\(y&gt;0\\))が生じる確率\\(Pr(y|q,\\lambda)\\)は、以下のように表現する。 \\[ \\log \\frac{q}{1-q} = \\alpha_{1} + \\beta x_{1}\\\\ \\log(\\lambda) = \\alpha_{2} + \\beta x_{2}\\\\ Pr(0|q,\\lambda) = q + (1-q)\\exp(-\\lambda)\\\\ Pr(y|q,\\lambda) = (1-q)\\frac{\\lambda^y\\exp(-\\lambda)}{y!}\\ \\] 15.4.1 Rでのゼロ過剰ポアソンモデル psclパッケージのzeroinfl()関数でゼロ過剰ポアソンモデルの当てはめをすることができる。 ここでは、過剰なゼロを予測する変数として天気（Rain）を想定し、湿度（Humidity）と温度（Temperature）でカブトムシの数を推定する。 library(pscl) model_zeroinfl = pscl::zeroinfl(data = d_zip, y ~ Humidity + Temperature | Rain) summary(model_zeroinfl) ## ## Call: ## pscl::zeroinfl(formula = y ~ Humidity + Temperature | Rain, data = d_zip) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.3357 -0.4763 -0.3775 0.3633 3.0017 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -15.13389 6.88209 -2.199 0.02788 * ## Humidity -0.04123 0.05112 -0.806 0.41996 ## Temperature 0.60345 0.23266 2.594 0.00949 ** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.9197 0.5988 -1.536 0.1246 ## Rain 2.0315 0.9017 2.253 0.0243 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 23 ## Log-likelihood: -39.5 on 5 Df ベルヌーイ分布からゼロを予測するモデル（Zero-inflation model）とポアソン回帰からカウントデータを予測するモデル（Count model）の2種類の推定結果が出力される。 カウントデータが従う確率分布として、ポアソン分布以外の確率分布を設定することもできる。以下は負の二項分布に変えた場合である。 model_zeroinfl_2 = pscl::zeroinfl(data = d_zip, y ~ Humidity + Temperature | Rain, dist = &quot;negbin&quot;) #distで指定する summary(model_zeroinfl_2) ## ## Call: ## pscl::zeroinfl(formula = y ~ Humidity + Temperature | Rain, data = d_zip, ## dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.3357 -0.4763 -0.3775 0.3633 3.0018 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -15.13409 6.88214 -2.199 0.02788 * ## Humidity -0.04123 0.05112 -0.806 0.41996 ## Temperature 0.60345 0.23266 2.594 0.00949 ** ## Log(theta) 12.83152 273.28934 0.047 0.96255 ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.9197 0.5988 -1.536 0.1246 ## Rain 2.0315 0.9017 2.253 0.0243 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 373816.6121 ## Number of iterations in BFGS optimization: 26 ## Log-likelihood: -39.5 on 6 Df 15.4.2 ハードルモデル ゼロ過剰ポアソンモデルはベルヌーイ分布からだけではなくポアソン分布からもゼロが予測されるが、ゼロが生成されるモデルと1以上の正の整数が生成されるモデルを分けたハードルモデル(Hurdle model)というものもある。 psclパッケージにあるhurdle関数で実行することができる。 model_hurdle = pscl::hurdle(data = d_zip, y ~ Humidity + Temperature | Rain) summary(model_hurdle) ## ## Call: ## pscl::hurdle(formula = y ~ Humidity + Temperature | Rain, data = d_zip) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.2984 -0.4556 -0.4446 0.2950 3.1423 ## ## Count model coefficients (truncated poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -20.93522 8.10270 -2.584 0.00977 ** ## Humidity -0.02487 0.04360 -0.570 0.56839 ## Temperature 0.76575 0.28073 2.728 0.00638 ** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.7885 0.5394 1.462 0.1438 ## Rain -2.0877 0.8457 -2.469 0.0136 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 25 ## Log-likelihood: -38.2 on 5 Df 15.5 確認問題 MASSパッケージに入っているサンプルデータ、housingを使って練習をする。 d = housing #dという名前で保存する d$ID = 1:nrow(d) head(d) ## Sat Infl Type Cont Freq ID ## 1 Low Low Tower Low 21 1 ## 2 Medium Low Tower Low 21 2 ## 3 High Low Tower Low 28 3 ## 4 Low Medium Tower Low 34 4 ## 5 Medium Medium Tower Low 22 5 ## 6 High Medium Tower Low 36 6 (1)Freqを応答変数、Contを予測変数としたポアソン回帰と、(2)同じくFreqを応答変数、Contを予測変数とした負の二項回帰を行い、結果を比較せよ。 "],["15-glm_category.html", "Chapter 16 一般化線形モデル（複数カテゴリの分析） 16.1 準備 16.2 カテゴリカル変数の種類 16.3 順序ロジスティック回帰 16.4 多項ロジスティック回帰", " Chapter 16 一般化線形モデル（複数カテゴリの分析） 応答変数が2つのカテゴリの場合においては、ロジスティック回帰モデルを用いることを学んできた。では、応答変数が3つ以上のカテゴリの場合にはどのようなモデルを想定すればよいのだろうか。 この章では更に一般化線形モデルの一部である順序ロジスティック回帰と多項ロジスティック回帰について学び、複数のカテゴリを持つ応答変数を扱う方法について理解していく。 順序ロジスティック回帰 多項ロジスティック回帰 16.1 準備 可視化のためのggplot2パッケージに加え、ordinal、nnetパッケージを使う。 ordinalパッケージは順序ロジスティック回帰のときに、nnetパッケージは多項ロジスティック回帰のときに必要になる。初めて使う際には、事前にインストールが必要なので注意。 library(ggplot2) library(ordinal) library(nnet) 16.2 カテゴリカル変数の種類 第3章で、カテゴリカル変数は順序尺度と名義尺度に区別されることを学んだ。 順序尺度とは、順序関係のあるカテゴリカル変数である。「優、良、可、不可」といった成績、「1. 賛成、2. どちらでもない、3. 反対」といった尺度などは順序尺度に該当する。順序関係はあるが、得点を用いて演算に用いることは出来ないので、変数の種類としては数値ではなくカテゴリカル変数である。 名義尺度とは、順序関係のないカテゴリカル変数である（例：性別[男、女]、血液型、出身地など）。 順序ロジスティック回帰は応答変数が順序変数の場合、多項ロジスティック回帰は応答変数が名義尺度の場合に用いられる。 16.3 順序ロジスティック回帰 16.3.1 例題 例えば、Scoreを試験の成績を意味する順序尺度として「1=不可、2=可、3=良、4=優、5=秀」の値を取るとする。この成績Scoreに対して、試験前日の睡眠時間Sleepが及ぼす影響を検討するとしよう。 ###サンプルデータの作成 Sleep = c(6,1,5,2,5,6,2,6,2,5,6,2,5,3,5,3,3,7,2,7,6,1,2,1,7,1,1,7,5,3) Score = c(3,3,3,2,3,3,5,5,2,2,2,3,4,1,3,2,3,5,1,4,4,3,3,3,4,1,3,3,3,2) sample_ordered = data.frame(Score = Score, Sleep = Sleep) head(sample_ordered) ## Score Sleep ## 1 3 6 ## 2 3 1 ## 3 3 5 ## 4 2 2 ## 5 3 5 ## 6 3 6 ggplot2::ggplot() + ggplot2::geom_jitter(data = sample_ordered, aes(x = Sleep, y = Score)) 成績の得点は順序尺度なので、連続量として正規分布に従うという前提を置くのは適切ではない。二値のカテゴリカル変数の場合は二項分布を用いるロジスティック回帰で検討できたが、3つ以上のカテゴリを持つ変数の場合はどうすればよいか？ 16.3.2 順序ロジスティック回帰モデルの詳細 累積確率と累積ロジット 順序のあるカテゴリカル変数を扱う場合には、累積確率（cumulative probability）で各変数が生じる確率を表現する。累積確率とは、順序変数のある値以下が生じる確率のことをいう。例えば、\\(y\\)が\\(k\\)以下の値を取る累積確率を\\(Pr(y≤k)\\)と表現する。 カテゴリ\\(k\\)が生じる確率\\(p_{k}\\)とすると、\\(p_{k}\\)は累積確率を用いて以下の式で表現することができる。 \\[ p_{k}=Pr(y≤k)−Pr(y≤k−1) \\] 例えば試験の成績（1=不可、2=可、3=良、4=優、5=秀）を\\(y\\)として考えると、\\(Pr(y≤3)\\)は、試験の成績が1, 2もしくは3である確率を示している。試験の成績が3である確率\\(p_{3}\\)は、累積確率\\(Pr(y≤3)\\)から累積確率\\(Pr(y≤2)\\)を引くことで求めることができる。 なお、カテゴリの最大値が出る確率は、全体の確率から引けば求まる。例えば、試験の成績が5である確率\\(p_{5}\\)は累積確率\\(Pr(y≤5)-Pr(y≤4)\\)を計算しなくとも、\\(1-Pr(y≤4)\\)で求めることができる。 線形予測子との関係 カテゴリ\\(k\\)が得られる累積確率\\(Pr(y ≤ k)\\)は、K-1個の切片\\(\\alpha_{k}\\)で示すことができる（上で述べたように、最大カテゴリの確率は1から\\(Pr(y≤K-1)\\)を引けば求まるので、すべての確率を表現するために切片をK個用意する必要はない）。\\(\\alpha_{k}\\)は累積確率を区切るポイントを意味し、カットポイント(cutpoint)とも呼ばれる。 \\[ Pr(y≤k) = \\frac{\\exp(\\alpha_{k})}{1+\\exp(\\alpha_{k})} \\] 更に、予測変数の効果（傾き）を考慮すると、累積確率は以下のように表現できる。 \\[ \\eta = \\beta x\\\\ Pr(y ≤ k) = \\frac{\\exp(\\alpha_{k} - \\eta)}{1+\\exp(\\alpha_{k} - \\eta)} \\] 以下のように書き換えることもできる（左辺を累積確率の対数オッズ、右辺を線形の式としたもの）。 \\[ \\eta = \\beta x\\\\ \\log\\frac{Pr(y ≤ k)}{Pr(y &gt; k)} = \\alpha_{k} - \\eta \\] 各切片から傾きの効果を引いているところに注意する必要がある。引くことによって、予測変数の値が大きいほど、累積確率の値が低くなる。言い換えれば、\\(Pr(y&gt;k)\\)が大きくなる。つまり、傾きの効果を引くことによって、予測変数の値が大きくなるほど、より大きい値のカテゴリが生じる確率が大きくなることを表現できる。 このように、順序ロジスティック回帰のモデルでは、各カテゴリの累積確率を決定づける切片\\(\\alpha_{k}\\)と傾き\\(\\beta\\)を用いて、各カテゴリが生じる確率を推定する。 16.3.3 Rでの順序ロジスティック回帰 Rで順序ロジスティック回帰を行うには、外部パッケージの関数を利用する。以下では、ordinalパッケージに含まれているclm関数を使って、先ほど作成したサンプルデータsample_orderedで順序ロジスティック回帰を行う。 16.3.4 変数の因子型への変換 解析の前に、Rで順序尺度を扱う場合は、変数を順序付きの因子型(factor)変数にする必要がある。factor()もしくはordered()のいずれかの方法で作成する。 #以下のいずれかの方法で因子型に変換する sample_ordered$Score = factor(sample_ordered$Score, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;), ordered = TRUE) sample_ordered$Score = ordered(sample_ordered$Score, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;)) #levelsで、水準の順序を指定する #factor()では、オプションoredered=TRUEを加える 16.3.5 解析 応答変数を順序付きの因子型変数に変更したら、ordinalパッケージに含まれているclm()で解析する。lm()と同じ要領で、応答変数~予測変数のモデルを書けば結果を出力してくれる。 model_ordinal = ordinal::clm(data = sample_ordered, Score ~ 1 + Sleep) summary(model_ordinal) ## formula: Score ~ 1 + Sleep ## data: sample_ordered ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 30 -38.90 87.81 6(0) 2.11e-11 5.3e+02 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## Sleep 0.4422 0.1827 2.421 0.0155 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1|2 -0.7339 0.8376 -0.876 ## 2|3 0.7129 0.7439 0.958 ## 3|4 3.1642 0.9920 3.190 ## 4|5 4.3135 1.1377 3.791 16.3.6 解釈 Coefficientsに予測変数に係る傾きの係数の推定値が出力されている。傾きの解釈は、一般化線形モデルのときと同じである（累積確率の対数オッズの変化量：要は係数がプラスならば、予測変数は高順位のカテゴリが生じる確率を上昇させる効果を持つと解釈すれば良い）。Interceptsに出力されているのは、順序ロジスティック回帰モデルの各切片（カットポイント）の推定値である。 16.4 多項ロジスティック回帰 16.4.1 例題 高校生が進学先（大学の学部）を選択する場合を例として考える。学部の種類（文学部、経済学部、理学部など）には順序関係はないので、学部の種類は名義尺度である。性別、高校のときの成績が学科選択に及ぼす影響を検討する。 ###サンプルデータの作成 set.seed(1) Male = c(rep(0:1, 25)) Grade = rnorm(n=50, 5, 2) Faculty = c(rep(&quot;Literature&quot;, 15), rep(&quot;Economics&quot;, 20), rep(&quot;Physical&quot;, 15)) sample_mnl = data.frame(Faculty = Faculty, Male = Male, Grade = Grade) head(sample_mnl) ## Faculty Male Grade ## 1 Literature 0 3.747092 ## 2 Literature 1 5.367287 ## 3 Literature 0 3.328743 ## 4 Literature 1 8.190562 ## 5 Literature 0 5.659016 ## 6 Literature 1 3.359063 Maleは性別（男=1, 女=0）、Gradeは高校の時の成績、Facultyは志望学部を意味する変数とする。Facultyには、Literature（文学部）、Economics（経済学部）、Physical（理学部）の3種類のカテゴリがあるとする。 先ほどの成績（1=不可、2=可、3=良、4=優、5=秀）の例とは異なり、今回の予測の対象となる応答変数は順序関係のないカテゴリカル変数（名義尺度）であった。順序関係がないカテゴリカル変数の場合はどうすればよいか？ 16.4.2 多項ロジスティック回帰モデルの詳細 多項ロジスティック回帰では基準となるカテゴリを設定し、基準カテゴリと比べて各カテゴリが生じやすいかを推定する複数のモデルを設定する。 例えば、経済学部(Economics)を基準カテゴリとする。他のカテゴリ(Literature, Physical)が生じる確率を線形の式で表した例を以下に示す。 \\[ \\log\\frac{Pr(Literature)}{Pr(Economics)}= \\alpha_{1} + \\beta_{1,1} Male + \\beta_{2,1} Grade \\\\ \\log\\frac{Pr(Physical)}{Pr(Economics)}= \\alpha_{2} + \\beta_{1,2} Male + \\beta_{2,2} Grade \\\\ \\] 多項ロジスティック回帰では、カテゴリごとに異なる線形予測子を設定し、それぞれ異なる切片と傾きの値を推定する。 16.4.3 Rでの多項ロジスティック回帰 Rには多項ロジスティック回帰を行うための関数として、nnetパッケージのmultinom()関数がある。lm()と同じ要領でモデルを記述すると、推定結果を出力してくれる。先ほど作ったサンプルデータsample_mnlで、多項ロジスティック回帰を行ってみる。 result_mnl = nnet::multinom(data = sample_mnl, Faculty ~ 1 + Male + Grade) ## # weights: 12 (6 variable) ## initial value 54.930614 ## final value 54.309165 ## converged summary(result_mnl) ## Call: ## nnet::multinom(formula = Faculty ~ 1 + Male + Grade, data = sample_mnl) ## ## Coefficients: ## (Intercept) Male Grade ## Literature -0.3637660 -0.1228603 0.02625298 ## Physical -0.7764841 0.1628974 0.07759005 ## ## Std. Errors: ## (Intercept) Male Grade ## Literature 1.214153 0.6892241 0.2085267 ## Physical 1.243610 0.6894528 0.2104109 ## ## Residual Deviance: 108.6183 ## AIC: 120.6183 Coeffficientsの部分に、係数の推定結果が出力される。このモデルではEconomicsが基準カテゴリとなっている（デフォルトで、アルファベット順で一番はじめに出てくるカテゴリが基準となる）。Literatureの部分に出力されるのが上の式でいう\\(\\alpha_{1}\\), \\(\\beta_{1, 1}\\), \\(\\beta_{2, 1}\\)に、Physicalの部分に出力されるのが\\(\\alpha_{2}\\), \\(\\beta_{1, 2}\\), \\(\\beta_{2, 2}\\)に相当する。 Literatureの予測変数の傾きの推定値は、基準カテゴリ（Economics）と比べた上でのその予測変数の効果を意味する（その予測変数が1単位変化したときのLiteratureとEconomicsの対数オッズの変化量）。 このように、多項ロジスティック回帰の係数はある基準カテゴリと比較した上での効果を意味するため、解釈は複雑になる。 multinom()ではp値を出力してくれないので、求めたい場合は自分で計算する必要がある。以下には、z scoreを元に計算する方法を示す。 #p値の出力 z = summary(result_mnl)$coefficients/summary(result_mnl)$standard.errors p = (1 - pnorm(abs(z), mean = 0, sd = 1)) * 2 p ## (Intercept) Male Grade ## Literature 0.7644787 0.8585197 0.8998131 ## Physical 0.5323786 0.8132228 0.7123104 "],["16-glmm.html", "Chapter 17 マルチレベルモデル 17.1 準備 17.2 個人差や集団差の問題 17.3 マルチレベルモデルの概要 17.4 Rでのマルチレベルモデル 17.5 正規分布以外を扱う例 17.6 まとめ 17.7 確認問題", " Chapter 17 マルチレベルモデル 一般化線形モデルを拡張し、個人差や集団差を扱うモデルについて学ぶ。 17.1 準備 ggplot2パッケージに加え、新たにlme4及びlmerTestというパッケージを使う。lme4とlmerTestは初めて使うので、インストールした上でロードしよう。 library(ggplot2) install.packages(&quot;lme4&quot;, &quot;lmerTest&quot;) library(lme4) library(lmerTest) 17.2 個人差や集団差の問題 以下では、Rにデフォルトで入っている iris データを例として使う。 head(iris) #irisデータの上数行を表示 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa まず、がくの長さ（Sepal.Length）とがくの幅（Sepal.Width）の関係を散布図で示してみよう。 graph_1 = ggplot2::ggplot() + ggplot2::geom_point(data=iris, aes(x=Sepal.Width, y=Sepal.Length),size = 3) graph_1 まず、lm()を使って、がくの長さを応答変数、がくの幅を予測変数とした線形モデルで係数を推定する。 iris_lm = lm(data = iris, Sepal.Length ~ 1 + Sepal.Width) summary(iris_lm) ## ## Call: ## lm(formula = Sepal.Length ~ 1 + Sepal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.5561 -0.6333 -0.1120 0.5579 2.2226 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.5262 0.4789 13.63 &lt;2e-16 *** ## Sepal.Width -0.2234 0.1551 -1.44 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8251 on 148 degrees of freedom ## Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 ## F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 推定された切片及び傾きの値から予測直線を引くと、以下のようになる。 graph_lm = ggplot2::ggplot()+ ggplot2::geom_point(data = iris, aes(x = Sepal.Width, y = Sepal.Length), size = 3) + ggplot2::geom_smooth(data = iris, aes(x = Sepal.Width, y = Sepal.Length), formula = y~ 1 + x, method = &quot;lm&quot;, se = FALSE) graph_lm がくの幅（Sepal.Width）は、がくの長さに対して負の影響を持っているように見える。 では、この散布図を種（Species）ごとに色わけして示してみる。 graph_2 = ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Width, y = Sepal.Length, color = Species, shape = Species), size = 3) graph_2 種を無視して検討したところ、がくの幅と長さの間には負の関係があるようにみえたが、種ごとに分けてみると「がくの長さが大きくなるほど、がくの幅が大きくなる」関係にあるように見える。 このあやめのデータのように、いくつかのデータが同じグループに属している構造の場合、グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には、統計的独立性が保証されていないためである。つまり、同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。 独立(independence)とは、各データが他のデータに影響されないという意味である。これまで学んできた確率分布では、独立同分布(independent and identically distributed: i.i.d.)が前提とされている。例えば、コインを数回投げて表が出る回数は二項分布に従うが、表が出るかどうかは前の試行に影響されることはない（前回表が出たら、次も表が出やすいということはありえないという前提を置く）。 しかし、現実のデータでは、データ間の相関などにより、事象の独立性が保たれていないケースもありえる。その場合、統計的独立性を前提とした解析を行うと、上の例のように誤った結論を導いてしまう恐れがある。 この例に限らず、階層構造を持つデータや繰り返し測定データにも、同じことがいえる。例えば、学校ごとに学力テストを行った場合、同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合、その参加者のデータは似たような傾向になる可能性も考えられる。 このようなデータに対して、個人や集団の影響を考慮した統計モデルとして、マルチレベルモデル(multilevel model)が提案されている。 マルチレベルモデルは、「階層モデル(hierarchical model)」、「混合モデル(mixied model)」など別の名称もある。 17.3 マルチレベルモデルの概要 マルチレベルモデルでは、予測変数が応答変数に及ぼす効果だけではなく、個人や集団の効果を扱う。予測変数そのものの効果は固定効果（fixed effect）と呼ばれ、個人や集団ごとの効果はランダム効果（random effect）と呼ばれて区別される。前章まで扱ってきた、一般化線形モデルは固定効果のみを含むモデルである。 例として、 繰り返し測定されたデータを扱う。以下のプログラムを実行して、サンプルデータexampleを作ろう。 set.seed(1) example = data.frame(i = 1:6, j = c(1, 1, 2, 2, 3, 3), y = round(rnorm(6), 2), x = rep(c(0, 1),3) ) example ## i j y x ## 1 1 1 -0.63 0 ## 2 2 1 0.18 1 ## 3 3 2 -0.84 0 ## 4 4 2 1.60 1 ## 5 5 3 0.33 0 ## 6 6 3 -0.82 1 \\(i\\)がデータを意味する番号（何行目か）、\\(j\\)を個人もしくはグループを意味する番号とする。例えば、個人\\(j\\)が\\(x=0\\)の場合と\\(x=1\\)の場合の2回\\(y\\)を測定している、あるいは同じ集団\\(j\\)から2人が選ばれてそれぞれの人について\\(y\\)が測定された、といったケースが当てはまる。 一般化線形モデルの線形予測子は、以下のような数式で表現できた。 \\[ \\hat{y}_{i} = \\alpha + \\beta x_{i}\\\\ y_{i} \\sim \\text{Normal}(\\hat{y}_{i}, \\sigma) \\] \\(\\alpha\\)が切片、\\(\\beta\\)が予測変数\\(x\\)に係る傾きであった。 これに対し、マルチレベルモデルでは、以下のように線形予測子に\\(\\alpha_{j}\\)が加わる。 \\[ \\hat{y}_{i} = \\alpha_{0} + \\beta x_{i} + \\alpha_{j} \\\\ \\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y_{i} \\sim \\text{Normal}(\\hat{y}_{i}, \\sigma) \\] 線形予測子には、これまでと同様に切片（\\(\\alpha\\)）と傾き （\\(\\beta\\)）が含まれている。これは全てのグループに共通する効果であり、固定効果（Fixed effect）と表現する。 更に、モデルにはグループごとに異なる切片\\(\\alpha_{j}\\)が加わっている。このようにグループごとに追加で加わる効果をランダム効果（random effect）と表現する。ここではランダム効果を切片で表しており、ランダム切片(random intercept)とも呼ばれる。 また、\\(\\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\)とあるように、「グループごとに異なる切片\\(\\alpha_{j}\\)は、平均をゼロ、\\(\\sigma_{\\alpha}\\)を標準偏差とする正規分布から生成される」という仮定を置く。 このように線形予測子にランダム切片を加えることで、グループ全体に共通する効果とは別に、グループごとの差をモデルで表現できる。 傾きを\\(\\beta_{ j}\\)にする、すなわちグループごとに予測変数に係る効果が異なるという前提を置くこともできる。これは、ランダム傾き（random slope）と呼ばれる。 \\[ \\hat{y}_{i} = \\alpha_{0} + \\beta_{j} x_{i} + \\alpha_{j} \\\\ \\alpha_{j} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ \\beta_{j} \\sim \\text{Normal}(0, \\sigma_{\\beta})\\\\ y_{i} \\sim \\text{Normal}(\\hat{y}_{i}, \\sigma) \\] ただ、以下のRプログラムの解説でも述べるように、ランダム傾きを含むモデルの推定は困難な場合が多いので、グループ差の統計的統制はランダム切片のみを加えたモデルで行うのが一般的である。 17.4 Rでのマルチレベルモデル Rでマルチレベルモデルで解析を行うためには、外部パッケージが必要になる。様々なパッケージがあるが、lme4パッケージが扱いやすい。以下では、lme4パッケージに含まれるglmer()を使った解析の例を示す。 基本的に、lm()関数と似た表記で使うことができる。ランダム切片は、(1|グループを意味する変数名)のかたちで線形予測子に入れる。 model_lmm = lme4::lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) #(1|Species)を加える summary(model_lmm) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 3.4062 0.6683 5.097 ## Sepal.Width 0.7972 0.1062 7.506 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 出力結果を見てみると、Fixed effectsという部分がある。ここに、固定効果の推定結果が表示される。見方は一般化線形モデルのときと同じである。切片(intercept)と予測変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず、すべての個体共通に係る予測変数の効果）。 がくの幅（Sepal.Width)の回帰係数（Estimate）を見ると、lm()での推定結果とは逆に、プラスになっている。やはり、グループの違いを統制すると、実際にはがくの幅が大きくなるほど、がくの長さも大きくなる関係にあることが、lmer()による推定結果からわかる。 ランダム傾きを加える場合には、以下のようにプログラムで指定する。 model_lmm_2 = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1 + Sepal.Length|Species)) summary(model_lmm_2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 + Sepal.Length | Species) ## Data: iris ## ## REML criterion at convergence: -4239.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.0182513 -0.0040510 0.0003048 0.0034958 0.0172606 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Species (Intercept) 1.084e-13 3.292e-07 ## Sepal.Length 2.028e-02 1.424e-01 -0.99 ## Residual 1.012e-14 1.006e-07 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.286e-06 1.043e-07 5.325e-02 21.922 0.757 ## Sepal.Width -4.300e-11 2.994e-08 5.410e-02 -0.001 1.000 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.223 ## optimizer (nloptwrap) convergence code: 0 (OK) ## Model failed to converge with max|grad| = 2.26442 (tol = 0.002, component 1) ## Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? ランダム傾きも加えた複雑なモデルの推定の場合は、最尤推定法では解が求まらない場合がある。ランダム傾きを含むマルチレベルモデルを扱う際には、ベイズ統計モデリングの手法を用いるのが望ましい。 lmer()では、デフォルトで係数のp値は表示されない。p値も出したいならば、lmerTest()パッケージをインストールしておく必要がある。 model_lmm = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) summary(model_lmm) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.4062 0.6683 3.4050 5.097 0.0107 * ## Sepal.Width 0.7972 0.1062 146.6648 7.506 5.45e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 他には、直接p値を計算する方法ではないが、信頼区間を算出して有意かどうか（信頼区間にゼロが含まれていないか）を判断するという手もある。 model_lmm = lmer(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species)) confint(model_lmm, level = 0.95) #confintで信頼区間を計算する。デフォルトで95%信頼区間が出力される（levelで範囲を指定可能）。 ## 2.5 % 97.5 % ## .sig01 0.4320405 2.4380677 ## .sigma 0.3909640 0.4915558 ## (Intercept) 1.9780097 4.8131087 ## Sepal.Width 0.5844733 1.0030191 17.5 正規分布以外を扱う例 17.5.1 ロジスティック回帰 応答変数が正規分布以外に従う場合のマルチレベルモデルについても見ていこう。 lme4パッケージのglmer()で、正規分布以外の確率分布を指定したマルチレベルモデルの解析を行うことができる。以下では、ランダム効果を加えたロジスティック回帰分析の例を示す。 まず、以下のプログラムを実行してサンプルデータdata_sampleを作ろう。 x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0) y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) ID = c(rep(&quot;a&quot;,5),rep(&quot;b&quot;,5),rep(&quot;c&quot;,5)) data_sample = data.frame(ID, x1, y1) head(data_sample) ## ID x1 y1 ## 1 a 1.0 0 ## 2 a 2.0 0 ## 3 a 3.0 1 ## 4 a 4.2 1 ## 5 a 5.1 1 ## 6 b 3.1 0 x1を予測変数（量的変数）、y1を応答変数（0か1のいずれかを取る）、IDが個体を示す変数とする。1つの個体からx1を変えて5回、y1が計測がされた実験をイメージしてほしい。 予測変数と応答変数の関係に、個体特有の効果を加えたモデルは以下となる。 \\[ \\log\\frac{q}{1-q} = \\alpha_{0} + \\beta x + \\alpha_{\\text{ID}} \\\\ \\alpha_{\\text{ID}} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y \\sim \\text{Binomial}(1, q) \\] \\(y=1\\)が生じる確率を\\(q\\)とし、線形予測子をロジット関数で変換する。更に、応答変数\\(y\\)は、\\(q\\)をパラメータとする二項分布から生成されるとする。これらの点は、一般化線形モデルで学んだ。 更に、線形予測子には、IDごとに異なるランダム切片\\(\\alpha_{\\text{ID}}\\)を加えている。\\(\\alpha_{\\text{ID}}\\)は、平均ゼロ、標準偏差\\(\\sigma_{\\alpha}\\)の正規分布に従って生成されるとする。 正規分布以外の確率分布を扱うマルチレベルは、Rではlme4パッケージのglmer()で扱うことができる。さっきのlmer()と同じ要領で、線形予測子に個体を識別する変数（ID）を加える。以下のように、(1|ID)というかたちで入れる。 あとは、確率分布とリンク関数を指定する。指定の仕方は、glm()のときと同じ要領である。確率分布はbinomial（二項分布）、リンク関数はlogit（ロジット関数）を指定する。リンク関数の指定は省略しても構わない（二項分布を指定すれば、デフォルトでロジット関数を選択してくれる）。 model_logistic_glmm = lme4::glmer(data = data_sample, y1 ~ 1 + x1 + (1|ID), family = binomial(link=&quot;logit&quot;)) summary(model_logistic_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: y1 ~ 1 + x1 + (1 | ID) ## Data: data_sample ## ## AIC BIC logLik -2*log(L) df.resid ## 14.3 16.4 -4.2 8.3 12 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.01928 0.00000 0.00000 0.00000 0.04031 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 67795 260.4 ## Number of obs: 15, groups: ID, 3 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -220.575 27.607 -7.99 1.35e-15 *** ## x1 38.996 4.887 7.98 1.46e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x1 -0.987 17.5.2 ポアソン回帰 同じく、lme4パッケージのglmer()でランダム効果を扱ったポアソン回帰を実行することができる。 まず、以下のプログラムを実行してサンプルデータdatを作る。 set.seed(1) alpha = 0.5 beta = 0.2 x = rnorm(n=50, mean = 0, sd = 1) alpha_0 = rnorm(n=5, mean = 0, sd = 0.2) lambda_1 = exp(alpha + beta * x[1:10] + alpha_0[1]) lambda_2 = exp(alpha + beta * x[11:20] + alpha_0[2]) lambda_3 = exp(alpha + beta * x[21:30] + alpha_0[3]) lambda_4 = exp(alpha + beta * x[31:40] + alpha_0[4]) lambda_5 = exp(alpha + beta * x[41:50] + alpha_0[5]) y_1 = rpois(n = 10, lambda_1) y_2 = rpois(n = 10, lambda_2) y_3 = rpois(n = 10, lambda_3) y_4 = rpois(n = 10, lambda_4) y_5 = rpois(n = 10, lambda_5) dat = data.frame(y = c(y_1, y_2, y_3, y_4, y_5), x = x + 20, ID = sort(rep(1:5, 10))) dat$x_std = dat$x - mean(dat$x) head(dat) ## y x ID x_std ## 1 4 19.37355 1 -0.72690209 ## 2 3 20.18364 1 0.08319504 ## 3 1 19.16437 1 -0.93607689 ## 4 2 21.59528 1 1.49483252 ## 5 0 20.32951 1 0.22905949 ## 6 0 19.17953 1 -0.92091666 xを予測変数（量的変数）、yを応答変数（正の整数を取るカウントデータ）、IDが集団を意味する変数とする。5つの集団があり、それぞれの集団にx及びyが10個ずつあるデータである。 ggplot2::ggplot() + ggplot2::geom_point(data = dat, aes(x = x, y = y)) + ggplot2::facet_wrap(vars(factor(ID))) モデルは以下のようになる。 \\[ \\log\\lambda = \\alpha_{0} + \\beta x + \\alpha_{\\text{ID}} \\\\ \\alpha_{\\text{ID}} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y \\sim \\text{Poisson}(\\lambda) \\] 線形予測子を対数関数で変換する。応答変数\\(y\\)は、\\(\\lambda\\)をパラメータとするポアソン分布から生成されるとする。 更に、線形予測子に、ランダム切片\\(\\alpha_{\\text{ID}}\\)を加えている。\\(\\alpha_{\\text{ID}}\\)は、平均ゼロ、標準偏差\\(\\sigma_{\\alpha}\\)の正規分布に従って生成されるとする。 ロジスティック回帰のときと同様に、lme4パッケージのglmer関数を使って分析を行う。確率分布はpoisson（ポアソン分布）、リンク関数はlog（対数）を指定する。リンク関数の指定は省略しても構わない（ポアソン分布を指定すれば、デフォルトで対数関数を選択してくれる）。 result = lme4::glmer(data = dat, y ~ 1 + x_std + (1|ID), family = poisson(link=&quot;log&quot;)) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(result) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ 1 + x_std + (1 | ID) ## Data: dat ## ## AIC BIC logLik -2*log(L) df.resid ## 161.4 167.2 -77.7 155.4 47 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4045 -0.6295 -0.0597 0.4675 3.2093 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0 0 ## Number of obs: 50, groups: ID, 5 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4235 0.1157 3.660 0.000253 *** ## x_std 0.2560 0.1474 1.737 0.082439 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x_std -0.206 ## optimizer (Nelder_Mead) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 17.5.3 マルチレベルモデルによる過分散への対処 ポアソン分布の理論的な分散よりも実際のデータの分散が大きい「過分散」がある場合には、ポアソン回帰の結果が信頼できなくなる問題があった。 #サンプルデータの作成 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd = 1) e = rnorm(n=N, mean = 0, sd = 1) lambda = exp(0.01 + 0.1*x + e) y = rpois(n=N, lambda = lambda) dat_dis = data.frame(y=y, x=x) result_dis = glm(data = dat_dis, y ~ 1 + x, family = poisson(link = &quot;log&quot;)) summary(result_dis) ## ## Call: ## glm(formula = y ~ 1 + x, family = poisson(link = &quot;log&quot;), data = dat_dis) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1417 0.3139 -0.451 0.65177 ## x 0.4393 0.1268 3.465 0.00053 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.37 on 49 degrees of freedom ## Residual deviance: 153.39 on 48 degrees of freedom ## AIC: 260.41 ## ## Number of Fisher Scoring iterations: 6 library(performance) performance::check_overdispersion(result_dis) #過分散のチェック ## # Overdispersion test ## ## dispersion ratio = 4.048 ## Pearson&#39;s Chi-Squared = 194.291 ## p-value = &lt; 0.001 ## Overdispersion detected. マルチレベルモデルでは、過分散の問題にも対処することができる。 dat_dis$ID = 1:nrow(dat_dis) #観測値ごとに番号を割り振る（1からデータ数までの数値の連続） head(dat_dis) ## y x ID ## 1 1 1.373546 1 ## 2 0 2.183643 2 ## 3 1 1.164371 3 ## 4 0 3.595281 4 ## 5 3 2.329508 5 ## 6 8 1.179532 6 このデータに対して、以下のモデルを当てはめる。 \\[ \\log\\lambda = \\alpha_{0} + \\beta x + \\alpha_{\\text{ID}} \\tag{5} \\\\ \\alpha_{\\text{ID}} \\sim \\text{Normal}(0, \\sigma_{\\alpha})\\\\ y \\sim \\text{Poisson}(\\lambda) \\] このデータは繰り返し測定のデータ（同じ個人もしくは集団から複数のデータが測定されているデータ）ではないが、観測値ごとのランダム切片（\\(\\alpha_{\\text{ID}}\\)）を加えることで、余分な分散を正規分布の標準偏差のパラメータ（\\(\\sigma_{\\alpha}\\)）で表現する。 result_poisson_glmm = lme4::glmer(data = dat_dis, y ~ 1 + x + (1|ID), family = poisson(link = &quot;log&quot;)) summary(result_poisson_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: y ~ 1 + x + (1 | ID) ## Data: dat_dis ## ## AIC BIC logLik -2*log(L) df.resid ## 202.3 208.0 -98.1 196.3 47 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.9658 -0.2773 -0.1231 0.3444 0.8207 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.7871 0.8872 ## Number of obs: 50, groups: ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.01303 0.47791 -0.027 0.978 ## x 0.18846 0.20904 0.902 0.367 ## ## Correlation of Fixed Effects: ## (Intr) ## x -0.921 ランダム切片を入れない通常のポアソン回帰では予測変数xはyに対して有意な効果を有していたが、観測値ごとのランダム切片を加えたマルチレベルモデルでは、xの係数が小さくなり有意な効果も見られなくなった。通常のポアソン回帰では過分散の影響でxの効果を過剰に評価していたことが伺える。余計な分散をランダム切片に吸収させることで、予測変数が持つ効果を推定することができた。 17.5.4 その他の一般化線形モデルの場合 順序ロジスティック回帰や多項ロジスティック回帰など、他の一般化線形モデルをマルチレベルモデルに拡張するには、別のパッケージを用いる必要がある（例えば、順序ロジスティック回帰の場合はordinalパッケージのclmm関数がある）。 ただし、扱う確率分布が特殊でランダム効果も含むといった複雑なモデルを扱う場合には、第20章で紹介するベイズ統計モデリングを扱うbrmsパッケージを使うのが良い。これを用いる場合でも、一般化線形モデル及びマルチレベルモデルの知識が前提となる。自分で分析のためのモデルの式を立てられるように、理解を固めておこう。 17.6 まとめ 同じグループから生成されたデータ同士は相関している可能性が高い。分析の際には、同じグループ同士のデータで見られる特徴（グループごとの差）の調整が必要になる。 マルチレベルモデルはデータ全体の効果を表す「固定効果」に加え、グループごとに異なる効果である「ランダム効果」を線形予測子に加えたモデルである。 17.7 確認問題 問１ carパッケージに入っているカナダにおける職業の威信度に関する調査データPrestigeを使う。102業種に関する調査結果が入っている。 library(car) head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof prestigeを応答変数、education, income及び womenを予測変数、typeをランダム効果（切片）としたマルチレベルモデルで解析せよ。応答変数が従う確率分布は、正規分布を用いるものとする。 各予測変数が応答変数に及ぼす効果について述べよ（その予測変数が1単位変化すると、応答変数がどう変化するか）。 なお、変数の意味は以下の通りである。 prestige：職業威信度（値が高いほど威信度が高い） education：在職者の平均教育年数 income：平均所得（単位はドル） women：女性の割合 type：職業のカテゴリ（bc=ブルーカラー、wc=ホワイトカラー、prof=専門職） ヒント：正規分布を扱うマルチレベルの場合は、lme4パッケージのlmer()を使えば良い。なお、出力時にメッセージが出ても無視して良い（中心化せよという命令だが、無視して良い）。 "],["17-evaluation.html", "Chapter 18 モデルの評価 18.1 準備 18.2 信頼区間と予測区間 18.3 多重共線性 18.4 過学習 18.5 モデルの予測力の評価 18.6 確認問題", " Chapter 18 モデルの評価 この章では一般化線形モデル（マルチレベルモデルを含む）を扱う上での注意点について学んでいく。 そのモデルはデータを上手く予測できているかをどう評価するかについて学んでいく。 信頼区間と予測区間 多重共線性 過学習 18.1 準備 この章では、performanceパッケージを使う。 library(performance) 18.2 信頼区間と予測区間 モデルでパラメータの推定を行ったあとは、そのモデルがデータを上手く予測できているかを確認することも重要である。 具体的には、パラメータの信頼区間(confidence interval)とデータの予測区間 (predictive interval)をチェックする。 信頼区間とは、パラメータが分布する区間のことをいう。今回得られた標本を用いて係数を推定したが、標本の元となる母集団の係数の値はどのくらいか？その母集団の係数の予想の範囲が、信頼区間である。 予測区間とは、標本がどの範囲に分布するかを予測する範囲のことをいう。新たな標本を取ったときに、そのデータがどの範囲に分布するか。その予想の範囲が予測区間である。 Rには、線形モデルの推定結果から信頼区間と予測区間を算出してくれるpredict()関数が用意されている。先ほどの線形モデルの解析結果を使って、信頼区間と予測区間を求めてみよう。 18.2.1 信頼区間 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) result_conf = predict(result, interval = &quot;confidence&quot;, level = 0.95) interval = \"confidence\"とすると、信頼区間を求めてくれる。 level = に信頼区間の幅を入力する（デフォルトで0.95だが、幅を変えたい場合は指定する）。 head(result_conf) ## fit lwr upr ## 1 2.376565 2.188121 2.565009 ## 2 2.004878 1.792226 2.217531 ## 3 1.633192 1.393955 1.872428 ## 4 1.447348 1.194160 1.700536 ## 5 2.190722 1.990526 2.390917 ## 6 2.934095 2.775149 3.093040 uprが95%信頼区間の上限、lwrが95%信頼区間の下限に当たる。 求めた信頼区間を図示してみよう plot_conf = cbind(iris, result_conf) #実測値のデータと予測値のデータを結合する。 ggplot2::ggplot() + ggplot2::geom_point(data = plot_conf, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_line(data = plot_conf, aes(x = Sepal.Length, y = fit)) + ggplot2::geom_ribbon(data = plot_conf, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 18.2.2 予測区間 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) new = data.frame(Sepal.Length = seq(4, 8, 0.1)) #0.1刻みで4から8まで範囲の数値ベクトルを入れたデータを仮に作る result_pred = predict(result, newdata = new, interval = &quot;prediction&quot;, level = 0.95) #newdataに先ほど作成した仮のデータを入れる。 head(result_pred) #仮データの数値に対応する予測区間が求められる ## fit lwr upr ## 1 0.3322885 -1.4165179 2.081095 ## 2 0.5181318 -1.2277203 2.263984 ## 3 0.7039751 -1.0390829 2.447033 ## 4 0.8898184 -0.8506063 2.630243 ## 5 1.0756617 -0.6622915 2.813615 ## 6 1.2615050 -0.4741389 2.997149 interval = \"prediction\"と入力する。 予測区間を図示してみよう。 plot_pred = data.frame(Sepal.Length = seq(4, 8, 0.1), result_pred) #予測区間のデータを作成する ggplot2::ggplot() + ggplot2::geom_point(data = iris, aes(x = Sepal.Length, y = Petal.Length)) + ggplot2::geom_line(data = plot_pred, aes(x = Sepal.Length, y = fit)) + ggplot2::geom_ribbon(data = plot_pred, aes(x = Sepal.Length, ymax = upr, ymin = lwr), alpha = 0.4) 実際のデータが予測区間の範囲に収まっているならば、そのモデルは概ねよくデータを予測できていることを示している。 18.3 多重共線性 予測変数同士が非常に強く相関しあっている場合、予測変数の係数の推定結果が信頼できなくなる恐れがある。この問題は、多重共線性(multicollinearity)と呼ばれる。 サンプルデータを使って確認してみよう。Rには多重共線性の例としてlongleyというサンプルデータがある。 head(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population Year Employed ## 1947 83.0 234.289 235.6 159.0 107.608 1947 60.323 ## 1948 88.5 259.426 232.5 145.6 108.632 1948 61.122 ## 1949 88.2 258.054 368.2 161.6 109.773 1949 60.171 ## 1950 89.5 284.599 335.1 165.0 110.929 1950 61.187 ## 1951 96.2 328.975 209.9 309.9 112.075 1951 63.221 ## 1952 98.1 346.999 193.2 359.4 113.270 1952 63.639 まず、このデータに入っている変数間の相関を確認してみよう。 cor(longley) ## GNP.deflator GNP Unemployed Armed.Forces Population ## GNP.deflator 1.0000000 0.9915892 0.6206334 0.4647442 0.9791634 ## GNP 0.9915892 1.0000000 0.6042609 0.4464368 0.9910901 ## Unemployed 0.6206334 0.6042609 1.0000000 -0.1774206 0.6865515 ## Armed.Forces 0.4647442 0.4464368 -0.1774206 1.0000000 0.3644163 ## Population 0.9791634 0.9910901 0.6865515 0.3644163 1.0000000 ## Year 0.9911492 0.9952735 0.6682566 0.4172451 0.9939528 ## Employed 0.9708985 0.9835516 0.5024981 0.4573074 0.9603906 ## Year Employed ## GNP.deflator 0.9911492 0.9708985 ## GNP 0.9952735 0.9835516 ## Unemployed 0.6682566 0.5024981 ## Armed.Forces 0.4172451 0.4573074 ## Population 0.9939528 0.9603906 ## Year 1.0000000 0.9713295 ## Employed 0.9713295 1.0000000 Employedを応答変数、GNP.deflatorを予測変数としたモデル（model01）と、Employedを応答変数、GNPを予測変数とした線形モデル（model02）でそれぞれ解析してみよう。 model01 = lm(data = longley, Employed ~ 1 + GNP.deflator) summary(model01) ## ## Call: ## lm(formula = Employed ~ 1 + GNP.deflator, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.68522 -0.44820 -0.07106 0.57166 1.61777 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.18917 2.12919 15.59 3.06e-10 *** ## GNP.deflator 0.31597 0.02083 15.17 4.39e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8706 on 14 degrees of freedom ## Multiple R-squared: 0.9426, Adjusted R-squared: 0.9385 ## F-statistic: 230.1 on 1 and 14 DF, p-value: 4.389e-10 model02 = lm(data = longley, Employed ~ 1 + GNP) summary(model02) ## ## Call: ## lm(formula = Employed ~ 1 + GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77958 -0.55440 -0.00944 0.34361 1.44594 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.843590 0.681372 76.09 &lt; 2e-16 *** ## GNP 0.034752 0.001706 20.37 8.36e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6566 on 14 degrees of freedom ## Multiple R-squared: 0.9674, Adjusted R-squared: 0.965 ## F-statistic: 415.1 on 1 and 14 DF, p-value: 8.363e-12 次に、Employedを応答変数、GNPとGNP.deflatorの両方を予測変数として入れて解析をしてみよう。 model03 = lm(data = longley, Employed ~ 1 + GNP.deflator + GNP) summary(model03) ## ## Call: ## lm(formula = Employed ~ 1 + GNP.deflator + GNP, data = longley) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.81315 -0.54330 0.05572 0.27894 1.40590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.94504 7.44945 7.644 3.67e-06 *** ## GNP.deflator -0.08511 0.12374 -0.688 0.5037 ## GNP 0.04391 0.01343 3.269 0.0061 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6693 on 13 degrees of freedom ## Multiple R-squared: 0.9685, Adjusted R-squared: 0.9637 ## F-statistic: 200 on 2 and 13 DF, p-value: 1.727e-10 それぞれの予測変数の係数を見てみると、一つずつ予測変数として入れたときと比べて値が変わっており、p値も低くなっている。 GNPとGNP.deflator同士は相関係数0.99とかなり強く相関している。このように、強く相関し合う変数を入れると係数の効果について信頼できる結果が得られなくなってしまう。 なぜ強く相関しあっている変数を入れるとまずいのか？モデルから考えてみよう。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x_{1} + \\beta_{2} x_{2} \\\\ \\tag{6} \\end{equation} \\] 2つの予測変数\\(x_{1}\\)と\\(x_{2}\\)が強く相関している場合、つまり\\(x_{1}=x_{2}\\)だとすると、式(6)は以下のように置き換えることができる。 \\[ \\begin{equation} \\mu = \\alpha + (\\beta_{1} + \\beta_{2}) x_{1} \\\\ \\tag{7} \\end{equation} \\] \\((\\beta_{1} + \\beta_{2})\\)について、パラメータ\\(\\beta_{1}\\)と\\(\\beta_{2}\\)の組み合わせは無限に考えられる。このように、強く相関する予測変数を入れると２つの予測変数のパラメータについて推定することが難しくなってしまう（パラメータの信頼区間が大きくなってしまう）。 18.3.1 多重共線性の評価 多重共線性の評価として、VIF(variance inflation factor)という指標がよく用いられる。一般的に、\\(VIF &gt; 10\\)の場合は、多重共線性を疑った方が良いといわれている。VIFの高い変数同士のうちどちらか一方を予測変数から除くといった対処をして、解析し直してみるのが良い。 performanceパッケージのcheck_collinearity()関数を使えば、VIFを確認することができる。 performance::check_collinearity(model03) ## # Check for Multicollinearity ## ## High Correlation ## ## Term VIF VIF 95% CI adj. VIF Tolerance Tolerance 95% CI ## GNP.deflator 59.70 [30.69, 117.04] 7.73 0.02 [0.01, 0.03] ## GNP 59.70 [30.69, 117.04] 7.73 0.02 [0.01, 0.03] 18.4 過学習 以下のプログラムを実行して、サンプルデータdを作成しよう。 set.seed(10) N = 10 x = seq(1,N,1) y = runif(N, min = 1, max = 5) d = data.frame(x = x, y = y) str(d) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ x: num 1 2 3 4 5 6 7 8 9 10 ## $ y: num 3.03 2.23 2.71 3.77 1.34 ... ggplot2::ggplot() + ggplot2::geom_point(data =d, aes(x=x, y = y)) このデータについて、以下の線形モデルを当てはめ、パラメータを推定しよう。図に線形モデルの直線及び信頼区間を図示するところまでやってみる。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x\\\\ \\tag{8} \\end{equation} \\] result_1 = lm(data = d, y ~ 1 + x) newdat = data.frame(x = x) conf.int_1 = predict(result_1, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_1) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 直線はほとんど観測値から外れており、当てはまりが悪いようである。 そこで、予測変数を増やして検討してみる。lm()では、予測変数\\(x\\)のn乗を含む多項式のモデルを考慮することも可能である。例えば、以下は3次の多項式の例である。 \\[ \\begin{equation} \\mu = \\alpha + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}\\\\ \\tag{9} \\end{equation} \\] n次式のモデルは多項式回帰(polynomial regression)と呼ばれる。 lm()では、I()の中に書くかたちでn次の予測変数を入れることができる。 result_3 = lm(data = d, y ~ 1 + x + I(x^2) + I(x^3)) 同じく、3次の多項式による予測の結果を図で確認しよう。 newdat = data.frame(x = x) conf.int_3 = predict(result_3, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_3) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 9次の式でも推定してみよう。 result_9 = lm(data = d, y ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9)) newdat = data.frame(x = x) conf.int_9 = predict(result_9, newdata = newdat, interval = &quot;confidence&quot;, level = 0.95) conf_dat = data.frame(d, newdat, conf.int_9) ggplot2::ggplot() + ggplot2::geom_point(data = conf_dat, aes(x = x, y = y))+ ggplot2::geom_line(data = conf_dat, aes(x = x, y = fit)) + ggplot2::geom_ribbon(data = conf_dat, aes(x = x, ymin = lwr, ymax = upr), alpha =0.2) 線は全てのデータ点を通っている。当然ながら、データの観測値の分だけパラメータがあれば、そのモデルはデータ点を全て通る線を引くことができる。現在のデータ点全てを予測することができる。 しかし、そのモデルは現在のデータを全て当てられても、将来得られる未知のデータを当てられるとは限らない。予測変数を多くすると現在のデータには当てはまるが、当てはまりすぎて未知のデータの予測力が低下してしまうことを、過学習(overfitting)という。 複雑なモデルが現在のデータによく当てはまるのは、ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし、複雑なモデルは現在のデータに当てはまっても、未知のデータにもうまく当てはまるとは限らない。 理想的なモデルは、「予測力が高く、かつ予測変数ができるだけ少なくてシンプルなモデル」となる。 18.5 モデルの予測力の評価 モデルの予測力を評価するために使われる指標について説明する。 18.5.1 決定係数 線形モデルでは、データに対する回帰分析のモデルの予測力を表す指標として、決定係数（R-squared）がある。 サンプルデータattitudeを例に見てみよう。 model_01 = lm(data=attitude, rating ~ 1 + complaints + learning) summary(model_01)$r.squared #r.squaredで決定係数のみを取り出すことができる。 ## [1] 0.7080152 これは、モデルから求めた予測値と実測値の分散が、実際のデータの分散に占める割合を意味する指標である。つまり、そのモデルでどれだけ全データの分散を説明できているかを意味する。 \\[ R^2 = \\sum_{i=1}^{n} \\frac {(y_{i}-\\mu_{i})^2}{(y_{i}-\\bar{y})^2} \\tag{10} \\] ただし、決定係数は単純に、予測変数が増えるほど大きくなる（説明できる分散の量が増える）。 例えばattitudeデータ内の全ての変数を予測変数に使ってみる。 model_full = lm(data=attitude, rating ~ .) #線形予測子を入力するところにドットを入力すると、そのデータに含まれる全ての変数を予測変数として扱う summary(model_full)$r.squared ## [1] 0.732602 応答変数に影響を及ぼさない変数を含めても、決定係数は上昇してしまう。 決定係数は、「予測力が高く、シンプルなモデル」を探すには常に適切な指標であるとは言えない。 18.5.2 赤池情報量規準（AIC） モデルのシンプルさ（予測変数の少なさ）とモデルの予測力とのバランスを取った指標の一つとして、赤池情報量規準(Akaike inoformation criteria: AIC)がよく知られている。AICは以下の式で計算される。 \\[ AIC = -2 \\log L + 2k \\tag{11}\\\\ \\] \\(\\log L\\)は最大対数尤度、\\(k\\)はモデルのパラメータ数である。 第9章で、モデルのパラメータを推定する方法として「最尤法」を紹介した。最尤法は、モデルのもっともらしさ（データが生じる確率）を意味する「対数尤度」が最大となるときのパラメータを求める方法であった。最大対数尤度は、現在のモデルに対する当てはまりの良さを反映している。その最大対数尤度に対し、パラメータ数\\(k\\)に応じてペナルティ(penalty term)を加える。 AICの値が低いほど、モデルの予測力が高いと評価する。AICは余計なパラメータが多くなる（\\(k\\)が大きくなる）ほど大きい値を取る。つまり、データをうまく予測しつつ、かつパラメータ数を抑えてシンプルなモデルを探る目的にかなっている。 AIC()関数でモデルをカッコ内に入れると、AICを算出してくれる。さきほどのattitudeに当てはめた2つのモデルのAICを見てみよう。 AIC(model_full) ## [1] 210.4998 AIC(model_01) ## [1] 205.1387 model_fullよりもmodel_01のAICが低く、model_01の予想力の方が高いことを示している。 AIC以外にも、モデルを評価する情報量基準がいくつか提案されている。 例えば、ベイズ情報量規準(Bayesian Information Criterion: BIC)がある。 BICは以下の式で求められる。\\(\\log L\\)と\\(k\\)はそれぞれ対数尤度とパラメータ数であり、\\(n\\)はオブザベーション数（データの数）を意味する。AICと同様に、数値が小さいほどモデルによく当てはまっていると判断する。 \\[ BIC = -2 \\log L + k\\log(n) \\tag{12}\\\\ \\] BIC(model_01) ## [1] 210.7435 18.6 確認問題 Rで標準で入っているairqualityを使う。 prac_dat_3 = na.omit(airquality) #欠損値を除き、別の名前で保存する head(prac_dat_3) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 3-1 Ozoneを応答変数、Solar.R, Wind, Tempの3つを予測変数とした線形モデルを行う。そして、切片及び傾きの推定値を報告せよ。 3-2 3-1で行った線形モデルについて、決定係数を報告せよ（Multiple R-squared）。 3-3 以下の3種類の線形モデルの解析を行い、 モデル1: Ozoneを応答変数、Solar.R, Wind, Tempの3つを予測変数とした線形モデル モデル2: Ozoneを応答変数、Solar.R, Tempの2つを予測変数とした線形モデル モデル3: Ozoneを応答変数、Tempを予測変数とした線形モデル それぞれのモデルのAICを報告するとともに、3つのモデルのうち予測力が高いと考えられるものはどれかを報告せよ。 "],["18-Bayese.html", "Chapter 19 ベイズ統計 19.1 頻度主義統計学とベイズ統計学 19.2 ベイズの定理 19.3 ベイズ推定の例 19.4 MCMC 19.5 確認問題", " Chapter 19 ベイズ統計 これまでの章で扱ってきた帰無仮説検定に基づく統計手法は、頻度主義の枠組みに分類される。これに対し、ベイズ統計学という別の統計学の流派が存在する。この章では、ベイズ統計学の基礎について触れる。 19.1 頻度主義統計学とベイズ統計学 両者の違いは、パラメータ（母数）の値をどう捉えるかの違いである。 頻度主義統計学では、パラメータが定数（固定された一定の値）であるという仮説を立て、データを確率変数とみなしてデータが生じる確率を推定するという考え方を置く。例えば平均値の差の有無を調べる統計的仮説検定では、パラメータである2つの母集団の平均値が等しい（\\(\\mu_{1} = \\mu_{2}\\)）という前提を置き、帰無仮説のもとでデータよりもまれな結果が生じる確率であるp値を求めた。 頻度主義は、以下の式で表される尤度（あるパラメータ\\(\\theta\\)のもとでデータxが生じる確率）を求めることに相当する。 \\[ Pr(x|\\theta) \\] これに対し、ベイズ統計学ではパラメータを確率的に変動する値であると考える。データは固定された値であり、未知であるパラメータの分布を推定するのがベイズ統計学の考え方である。以下の式で表されるように、ベイズ統計は頻度主義に対しデータxのもとにおけるパラメータ\\(\\theta\\)が生じる確率を求めることに相当する。 \\[ Pr(\\theta|x) \\] ベイズ統計では、「母集団の平均値の分布が-1〜1に分布する確率は90%である」といった範囲を求める。データからパラメータの分布を推定する手法を、ベイズ推定と呼ぶ。 19.2 ベイズの定理 まず、ベイズの定理について整理する。ベイズの定理とは、条件付き確率を求めるための定理である。 条件付き確率とは、ある条件のもとである事象が生じる確率のことをいう。例えば、「事象Bが起こったときに事象Aが生じる」条件付き確率を\\(Pr(A|B)\\)と表現したとする。 ベイズの定理は、以下で表される。\\(\\bar{A}\\)はAの余事象（Aではない事象）とする。 \\[ Pr(A|B) = \\frac{Pr(B|A) Pr(A)}{Pr(B)}\\\\ = \\frac{Pr(B|A) Pr(A)}{Pr(B|A) Pr(A) + Pr(B|\\bar{A}) Pr(\\bar{A})} \\] 19.2.1 例題 ベイズの定理を使って、ある条件付き確率を求めてみよう。 ある感染症に感染している確率は0.1%だとする。 ある感染症の検査方法を受けると、99%の確率で感染者に対して陽性反応が出ることがわかっている。その一方、1%の確率で感染していない者に対して陽性反応が出ることがわかっている。 あなたがこの検査を受けたとき、陽性反応が出た。 あなたが本当にその感染症にかかっている確率は何%か？ 陽性と診断されたときに感染している確率を、ベイズの定理で解いてみる。 先程のベイズの定理の式について、A=感染、B=陽性に置き換えて考えると、 \\[ Pr(感染|陽性) = \\frac{Pr(陽性|感染) Pr(感染)}{Pr(陽性)}\\\\ = \\frac{Pr(陽性|感染) Pr(感染)}{Pr(陽性|感染) Pr(感染) + Pr(陽性|非感染) Pr(非感染)} \\] つまり、陽性と診断される確率（分母）のうち、感染かつ陽性である確率（分子）の占める割合が、「陽性と診断されたときに感染している確率」を意味する。 問題文より、 Pr(感染) = 0.001（感染している確率0.1%） Pr(非感染) = 0.999（感染していない確率[1- 0.1%]） Pr(陽性|感染) = 0.99（感染者が陽性と正しく診断される確率99%） Pr(陽性|非感染) = 0.01（非感染者が陽性と誤って診断される確率1%） これらを当てはめると、 \\[ Pr(感染|陽性) = \\frac{0.99 \\times 0.001}{0.99 \\times 0.001 + 0.01 \\times 0.999} = 0.09 \\] つまり、実際に感染している確率は約9%ということになる。 19.2.2 事前確率、事後確率 もう一度ベイズの定理をおさらいすると、 \\[ Pr(A|B) = \\frac{Pr(B|A) Pr(A)}{Pr(B)} \\] \\(Pr(A)\\)は事前確率(prior probability)、\\(Pr(B|A)\\)は尤度(likelihood)、\\(Pr(A|B)\\)は事後確率(posterior probability)という。ベイズ推定は、もとの事前確率について与えられたデータをもとに事後確率へと更新するプロセスである。 19.2.3 ベイズ統計 データxのもとにおけるパラメータ\\(\\theta\\)の分布は、ベイズの定理から以下のように求められる。 \\[ Pr(\\theta|x) = \\frac{Pr(x|\\theta) Pr(\\theta)}{Pr(x)} \\] \\(Pr(\\theta|x)\\)を事後分布(posterior distribution)、\\(Pr(\\theta)\\)を事前分布(prior distribution)、\\(Pr(x|\\theta)\\)を尤度(likelihood)と呼ぶ。 \\(Pr(x)\\)はデータの分布を意味しているが、確率変数を含まないので定数とみなすことができる（基準化定数や周辺尤度と呼ばれる）。したがって、この式は以下のようなかたちで表現されることもある。\\(\\propto\\)は「比例する」という意味である。 \\[ Pr(\\theta|x) \\propto Pr(x|\\theta) Pr(\\theta) \\] 事後分布はデータを得たあとで推定したパラメータの分布であり、事前分布と尤度の積に比例する。 19.3 ベイズ推定の例 具体的な例を使って、ベイズ推定を行ってみよう。 手元にゆがんだコインがあるとする。このコインの表が出る確率は不明なので、何回かコインを投げて推定することにする。このコインを10回投げたら、6回表が出た。このデータから、コインの表が出る確率\\(\\theta\\)の事後分布を推定しよう。 19.3.1 事前分布 まずは、コインを投げる前に考えるコインの表が出る確率\\(\\theta\\)である、\\(\\theta\\)の事前分布を設定する。しかし、事前分布をどう設定するかについては、研究者の恣意性が介入してしまう恐れがある。 仮説について情報がないときに設定する事前分布は無情報事前分布と呼ばれ、一様分布が無情報事前分布として採用される。一様分布は、どの値も生じる確率が一定であるという前提の分布であるので、恣意性を排除できる。 この例では、表が出る確率\\(\\theta\\)の事前分布を0から1の範囲の連続一様分布に設定する。連続一様分布は確率密度関数なので、縦軸は確率そのものを意味しないので注意（面積が確率を意味する。0から1までの範囲の面積が1となっている）。 \\[ Pr(\\theta) = 1 \\] theta_seq = seq(0,1,0.01) #0から1までの範囲で0.01刻みでベクトルを作る theta_prior = rep(1, length(theta_seq)) もちろん事前確率について仮説がある場合は、範囲を定めた一様分布あるいは別の確率分布を事前確率を設定しても良い。例えば、身長の分布を推定するときは、0cm ~ 250cmの範囲の一様分布を事前分布として設定するのは妥当であるといえる。 19.3.2 尤度 データから尤度を求める。コインで表が出た回数は二項分布に従うので、尤度は以下から求められる(nはコイン投げの総数、hは表が出た回数とする)。 \\[ Pr(x|\\theta) = {}_n\\mathrm{C}_h\\theta^{h}(1-\\theta)^{(n-h)} \\] theta_likelihood = dbinom(x=6, size=10, prob = theta_seq) 19.3.3 事後分布 ベイズの定理をもとに、事後分布を求める。先程の式より、 \\[ Pr(\\theta|x) \\propto Pr(x|\\theta) Pr(\\theta) \\] 尤度と事前分布をかけて事後分布を求める（正確には、更に基準化定数\\(Pr(x)\\)で割る）。 theta_posterior = theta_likelihood * theta_prior theta_posterior_std = theta_posterior/sum(theta_posterior) 19.4 MCMC パラメータが多くて複雑なモデルになると、解析的に事後分布の推定は困難になる。 事後分布をベイズ推定する方法として、コンピュータの乱数を使ってシミュレーションで事後分布を推定するマルコフ連鎖モンテカルロ法（Markov Chain Monte Carlo）が提案されている。 モンテカルロ法とは乱数を発生させて近似的に解を求める手法である（カジノで有名な土地モンテカルロに由来する）。マルコフ連鎖とは、ある状態に移る確率が現在の状態のみに依存する確率過程を意味する。例えば、今日晴れならば明日も晴れやすい（ただし、2日前の天気は明日の天気に影響しない）という過程を意味する。MCMCではパラメータの推定に乱数を生成して事後分布を評価し、その結果に応じて次の乱数を決めるというアルゴリズムでパラメータの分布を推定する手法である。MCMCはアルゴリズムの総称で、メトロポリス法やギブスサンプリング法など様々な手法が提案されている。 MCMCでは、コンピュータのシミュレーションでパラメータの事後分布を求める。近年、StanをはじめとしたMCMCを行うためのソフトウェアが開発されている。 19.4.1 MCMCの例 さきほどのコイン投げで表が出る確率\\(\\theta\\)の事後分布をMCMCで推定する方法を通して、MCMCの全体像について説明する。 MCMCでは、まずパラメータの初期値を適当に選び（例えば\\(\\theta_{1}\\)とする）、それを元に事後分布\\(Pr(\\theta_{1}|x)\\)に従う乱数を生成する。次に、\\(Pr(\\theta_{1}|x)\\)をある基準で評価した上で、その評価を元に新たなパラメータ\\(\\theta_{2}\\)を元に乱数を生成する。このようなシミュレーションを何回も繰り返す。何回も繰り返していくうち、生成される乱数はある分布に収束していく（定常分布と呼ばれる）。最終的に出来上がった定常分布をパラメータの事後分布として採用するというのが、（非常に大雑把な）MCMCの概要である。 例えば以下に、シミュレーションを2,000回行ったときのパラメータ\\(\\theta\\)の推定の推移を示している。最初の1,000回の推定結果は適当に選んだ初期値に依存するため、これらは切り捨てて（warmup期間と呼ばれる）、残りの1,000回のシミュレーション結果（MCMCサンプル）を事後分布として採用する。 事後分布をプロットしたのが、以下である。 この分布の中央値は0.59であった。10回中6回表が出たので、表が出る確率0.60と概ね一致している。また、分布の中央95%を占める部分の範囲(下位2.5%から上位97.5%)は、0.29から0.83であった。つまり、表が出る確率は95%の確率で0.29から0.83の範囲を取ることを意味する。ベイズ推定で求めたパラメータの分布の範囲は、信用区間(credible interval)と呼ばれる。 19.5 確認問題 ベイズの定理を使って、事後確率を求める方法を復習しよう。 問1 3つの袋それぞれに、玉が100個入っている。袋Aには玉100個のうち赤玉が30個、袋Bには玉100個のうち赤玉が70個、袋Cには玉100個のうち赤玉が50個入っていて、残りは黒玉である。3つの袋の中から一つをランダムに選んで、玉を1個取り出すとする。 今、あなたの目の前に赤玉が1個ある。 1-1 この赤玉が袋Aから取り出された確率を求めよ。 1-2 この赤玉が袋Bから取り出された確率を求めよ。 1-3 この赤玉が袋Cから取り出された確率を求めよ。 ヒント: 赤玉があるときに、それが袋A、袋B、もしくは袋Cから取り出された条件付き確率\\(Pr(A|赤)\\)を求める。例えば、1-1の場合は、赤玉が取り出される確率（袋Aから赤玉が取り出される確率、袋Bから赤玉が取り出される確率、袋Cから赤玉が取り出される確率の合計）のうち、袋Aから赤玉が取り出される確率が占める割合を求めれば良い。 問2 世の中に出回っている迷惑メールは全メール中80%であることがわかっている。 迷惑メールのうち80%には「無料」という単語が含まれている、普通のメールには「無料」が50%含まれている事がわかっている。「無料」を含むメールが送られてきたときに、このメールが迷惑メールである確率は何％か？ ヒント: 感染症の問題を復習しよう。 "],["19-Bayese_model.html", "Chapter 20 ベイズ統計モデリング 20.1 準備 20.2 Rによるベイズ統計モデリング 20.3 ベイズ統計モデリングのプロセス 20.4 brmsパッケージでの一般化線形モデル 20.5 brmsパッケージでのマルチレベルモデル 20.6 その他", " Chapter 20 ベイズ統計モデリング この章では、これまで学んできた一般化線形モデルなどの解析をベイズ統計の枠組みで行う方法について解説する。rstanとbrmsパッケージを使い、一般化線形モデルやマルチレベルモデルのパラメータの事後分布をMCMCで推定する。 この章で書かれている内容を行う前に、前の章で説明したベイズ統計の概要を理解しておくこと（前の章を読んでいるという前提で説明をする）。「事前分布」、「事後分布」、「MCMC」について、前の章で確認しておくこと。 20.1 準備 20.1.1 Rstanのインストール MCMCを行うために、Stanと呼ばれるプラットフォームが必要にある。RStanはRからStanを使うために開発されたインターフェースである。この章の内容の解析を行うためには、RStanパッケージのインストールが事前に必要となる。 RStan（rstan）のインストール方法については、「RStan Getting Started (Japanese)」https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese)のページを参照のこと。「Rtool」、「C++コンパイラ（MacならばXCode）」のインストールも必要になる。 20.1.2 brmsパッケージのインストール rstanをインストールできたら、brmsパッケージもインストールする。 Stanを使ってベイズ推定（MCMC）を行うためには、Stan言語で解析用のコードを書く必要があるが、brmsパッケージを使えばユーザーがコードを書く必要なく、線形のモデルやオプションを指定するだけで自動でStanコードを生成してMCMCを行ってくれる。 install.packages(&quot;brms&quot;) 20.1.3 パッケージのロード rstanとbrmsをロードする。 library(rstan) library(brms) また、計算の高速化のために、以下のプログラムも実行しておく。 #計算を高速化するオプション rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) 20.2 Rによるベイズ統計モデリング 回帰分析のパラメータ（傾きと切片）のベイズ推定を例として、brmsパッケージを使った解析の手順について確認していく。 第9章で、irisデータを用いて以下の回帰分析を行った。 result = lm(data = iris, Petal.Length ~ 1 + Sepal.Length) summary(result) ## ## Call: ## lm(formula = Petal.Length ~ 1 + Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 同じデータについてbrmsパッケージのbrm()を使って、MCMCでパラメータの事後分布を推定してみよう。glm()と同じ書き方で、線形予測子、確率分布、リンク関数を指定すれば良い。回帰分析なので、確率分布は正規分布(gaussian)、リンク関数は恒等リンク(identity)とする。 また、オプションのseedには常に一定の値を指定すること（ここでは1と指定した）。MCMCは乱数を使ったシミュレーションであるので、やり直すたびに微妙に異なる結果が得られる可能性にある。しかし、seedに同じ値を設定すれば、やり直しても同じ結果を再現することできる。 result_brms_lm = brms::brm(data = iris, Petal.Length ~ 1 + Sepal.Length, family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(result_brms_lm) ## Family: gaussian ## Links: mu = identity ## Formula: Petal.Length ~ 1 + Sepal.Length ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -7.10 0.51 -8.12 -6.07 1.00 3831 2967 ## Sepal.Length 1.86 0.09 1.69 2.03 1.00 3797 2942 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.87 0.05 0.78 0.98 1.00 4074 2871 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 最初に、応答変数が従う確率分布(Family)、リンク関数(Links)、線形予測子(Formula)、データ(Data)の情報が出力される。更に、Samplesの部分にMCMCの設定が表記されているが、これらの意味については後ほど説明する。 lm()の出力と同様に、Population-Level Effectsという部分に各パラメータ（切片と傾き）の推定結果が出力されている。Estimateが係数の事後分布の期待値を示している。lm()で解析したときとほぼ同じ値が推定されている。 他にも、l-95% CIとu-95% CIといった数値が出力されているが、これらの意味を理解するにはパラメータの事後分布を図示するとよい。plot()に出力結果を入れると、MCMCの結果を図で示してくれる。 plot(result_brms_lm) 左側に表示されているのが、パラメータの事後分布（MCMCでサンプリングされた事後分布に従う乱数の分布）である。右側は、トレースプロット(traceplot)と呼ばれるものである。トレースプロットの下の軸はMCMCの試行数を示しており、MCMCのサンプリング結果の推移を示している。 もう一度brm()の出力のl-95% CIとu-95% CIの値を確認し、図との関係を確認しよう。l-95% CIとu-95%は、パラメータの事後分布の下位5%点と上位95%点の値を示しており、この下位5%から上位95%の範囲は95% 信用区間(credible intervals)と呼ばれる。95%信用区間とは、95%の確率で真のパラメータの値が含まれる範囲を意味する。 例えば、切片（intercept）の事後分布のプロットを確認すると、だいたい-7が分布の中央に位置しており、推定された切片の事後分布の期待値（Estimate）と概ね一致しているのがわかる。また、-8から-6の範囲に事後分布の大部分が締めており、これも95%信用区間の推定結果と概ね一致している。 最尤推定法（最小二乗法）によるパラメータ推定であるlm()の出力結果とは異なり、ベイズ推定であるbrm()の結果ではp値が表示されていていない点に注目しよう。前の章でも述べたように、データを定数、パラメータを確率変数として考えるベイズ統計には、統計的仮説検定の枠組みで扱うp値（帰無仮説のもとでデータが得られる確率）という概念はない。 ベイズ推定による一般化線形モデルで係数の効果に意味があるかを議論したいのならば、パラメータの信用区間について注目する。brm()の出力結果で係数の信用区間にゼロが含まれているかどうかが、lm()のp値が示す結果（係数がゼロから有意に離れているか）と対応している。 20.3 ベイズ統計モデリングのプロセス brm()のプログラムの書き方を確認しながら、ベイズ統計モデリングの手順について確認していこう。 20.3.1 事前分布の設定 パラメータの事後分布を推定するためには、まずパラメータ（切片と傾き）の事前分布を設定する必要がある。何か仮説があって事前にパラメータの範囲を設けることに正当な理由があるのならば、任意の範囲を設定しても構わない。例えば、身長を予測するならば切片の事前分布として0cm - 300cmの一様分布を設定するというのは妥当であろう。それに対し、特に仮説がない、パラメータの事前分布について確信がない場合は、無情報事前分布(non-informative prior)を設定する。 特に仮説がなければ、brm()では何も設定する必要はない。自動で事前分布を無情報事前分布としてくれる（例えば傾きなどをフラットな一様分布に設定してくれる）。 get_prior()に、モデル、確率分布、リンク関数を指定すれば、設定される事前分布を推定の前に確認することができる。 brms::get_prior(data= iris, Sepal.Length ~ 1 + Sepal.Width, family = gaussian(link=&quot;identity&quot;)) ## prior class coef group resp dpar nlpar lb ub tag ## (flat) b ## (flat) b Sepal.Width ## student_t(3, 5.8, 2.5) Intercept ## student_t(3, 0, 2.5) sigma 0 ## source ## default ## (vectorized) ## default ## default 事前分布を任意に指定したい場合は、brm()のオプションとしてset_prior()で設定することができる。以下に、プログラムの例を示す。 result_brm_lm = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width, family = gaussian(link=&quot;identity&quot;), prior = c(set_prior(&quot;normal(0,10)&quot;, class = &quot;b&quot;), #傾きbの事前分布を平均0, 標準偏差10の正規分布に設定 set_prior(&quot;cauchy(0,5)&quot;, class = &quot;sigma&quot;) #正規分布の分散の事前分布を半コーシー分布に設定 ), seed = 1 ) brms::prior_summary(result_brm_lm) #prior_summaryに結果を入れると、設定した事前分布を確認することができる 20.3.2 MCMCの設定 brm()のオプションで、MCMCシミュレーションの設定を指定することができる。 result_brms = brms::brm(data = iris, Petal.Length ~ 1 + Sepal.Length, iter = 2000, warmup = 1000, chains = 4, seed = 1) iterで乱数生成の試行数、warmupでwarmup期間の数、chainsでマルコフ連鎖の数を指定する。前の章の内容をおさらいすると、MCMCではパラメータの事後分布に従う乱数を生成するシミュレーションを繰り返し、全シミュレーションの結果から作られた分布をパラメータの事後分布として採用する。iterで、乱数生成の繰り返し数を設定する（この例では、2,000試行に設定）。また、MCMCシミュレーションの最初の部分は、乱数の初期値による影響を大きく受けていて最終的に事後分布を作成する上で使い物にならない。そのため、最初の試行は切り捨てられる。warmupで、その切り捨てる期間を指定する（この例では、最初の1,000試行を切り捨てるように設定）。MCMCでは一般的に乱数生成を1からやり直して何セットか行い、事後分布を評価する。chainで、このセット数を設定する（この例では、4セットに設定）。最終的に得られるMCMCサンプル（シミュレーションの結果）は、(iter - warmup)*chains個になる。 iter, warmup, chainsの指定をしなければ、デフォルトで設定されている値(iter = 2000, warmup = 1000, chains = 4)でMCMCが実行される。 20.3.3 事後分布の評価 事前分布、MCMCの設定ができたら、brm()を実行してMCMCを行う。 シミュレーションが終わったら、結果を確認する。先ほど示したように、summary()で事後分布の期待値などの要約を確認するのももちろん、図でも確認する。plot()で簡単な図を作成することができる。 summary(result_brms_lm) plot(result_brms_lm) 他にも、MCMCの結果を図示するためのパッケージとして、bayesplotパッケージがある（brmsパッケージをインストールすると一緒にインストールされる）。 以下に、事後分布を図示する例をいくつか示す（parsで出力したいパラメータの値を任意に指定することも可能）。 library(bayesplot) bayesplot::mcmc_trace(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #トレースプロット bayesplot::mcmc_hist(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #事後分布（ヒストグラム） bayesplot::mcmc_dens(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;)) #事後分布（密度曲線） bayesplot::mcmc_intervals(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;), prob = 0.89, #太い線が意味する範囲（89%区間とした） prob_outer = 0.95#細い線が意味する範囲（95%区間とした） ) #パラメータの分布を線で示したグラフ bayesplot::mcmc_areas(result_brms_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;), prob = 0.89, #色が塗られた部分（89%区間とした） prob_outer = 0.95#細い線が意味する範囲（95%区間とした） ) #分布も一緒に示したグラフ bayesplot::mcmc_combo(result_brms_lm, combo = c(&quot;hist&quot;, &quot;dens&quot;), pars = c(&quot;b_Intercept&quot;, &quot;b_Sepal.Length&quot;))#mcmc_comboで、出力する図を複数指定することができる。 20.3.4 収束の評価 MCMCによる事後分布の推定は乱数を使ったシミュレーションなので、シミュレーション結果は毎回異なる。しかし、得られた結果が毎回かなり違うのならば、その結果は信用できないということになる。summary()の出力に表示されているRhatが、MCMCの結果が安定している（収束している）かを評価する指標として用いられる。Rhatが1.00を超えている場合は収束していないことが疑われるので、MCMCの設定やモデルの修正などの対処が必要になる（詳細については、参考文献リストに挙げたベイズ統計モデリングに関する解説書を参照のこと）。 20.3.5 モデルの予測評価 推定結果が、実際のデータをうまく予測できているかを評価する。ここでは、回帰直線の信用区間や予測区間を図示して確認する方法を示す。 pred_line = brms::conditional_effects(result_brms_lm, method = &quot;posterior_epred&quot;, prob=0.95 #95%信用区間を表示 ) plot(pred_line, points=TRUE) #points=TRUEで点と一緒に示す pred_line = brms::conditional_effects(result_brms_lm, method = &quot;posterior_predict&quot;, prob=0.95 #95%予測区間を表示 ) plot(pred_line, points=TRUE) #pointsで点と一緒に示す 20.4 brmsパッケージでの一般化線形モデル brm()で確率分布やリンク関数を変更すれば、ロジスティック回帰やポアソン回帰のベイズ推定も行うことができる。プログラムの書き方はglm()とほぼ同じである。 以下のプログラムの例ではリンク関数の指定も記述しているが、省略しても構わない（デフォルトでその確率分布に対して適切なリンク関数が設定されている）。 20.4.1 ロジスティック回帰 第12章でロジスティック回帰の練習に使ったサンプルデータを使って、パラメータの事後分布の推定を行ってみよう。以下に第12章に示したサンプルデータの作成プログラムを再掲する。 library(MASS) dat = biopsy dat$y = ifelse(dat$class == &quot;malignant&quot;, 1, 0) #classがbenignならばゼロ、それ以外なら1という変数yを作る dat$x = dat$V1 #V1という変数をxという名前に変える head(dat) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class y x ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 5 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 5 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 3 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 6 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 4 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 8 確率分布（family）にはベルヌーイ分布(bernoulli)、リンク関数（link）にはlogitを指定する。 result_brms_logistic = brms::brm(data = dat, y ~ 1 + x, family = bernoulli(link=&quot;logit&quot;), seed = 1) summary(result_brms_logistic) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + x ## Data: dat (Number of observations: 699) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -5.21 0.38 -6.01 -4.49 1.00 2156 2007 ## x 0.95 0.07 0.81 1.10 1.00 2370 2346 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 20.4.2 ポアソン回帰 第12章のサンプルデータを使って、brm()でポアソン回帰を行ってみる。 set.seed(1) N= 50 x = rnorm(n=N, mean = 2, sd=1) lambda = exp(0.01+ 0.6*x) y = rpois(n=N, lambda = lambda) dat = data.frame(y=y, x=x) head(dat) ## y x ## 1 3 1.373546 ## 2 3 2.183643 ## 3 1 1.164371 ## 4 17 3.595281 ## 5 5 2.329508 ## 6 1 1.179532 確率分布（family）にはポアソン分布(poisson)、リンク関数（link）にはlogを指定する。 result_brms_poisson = brms::brm(data = dat, y ~ 1 + x, family = poisson(link=&quot;log&quot;), seed = 1) summary(result_brms_poisson) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + x ## Data: dat (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.70 0.28 -1.26 -0.17 1.00 1464 1687 ## x 0.90 0.11 0.70 1.11 1.00 1595 1976 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 20.4.3 負の二項回帰 サンプルデータを使って、brm()で負の二項回帰を行ってみる。 d = warpbreaks #別の名前(d)で保存する d$A &lt;- ifelse(d$wool == &quot;A&quot;, 1, 0) #Aなら1, Bなら0のダミー head(d) ## breaks wool tension A ## 1 26 A L 1 ## 2 30 A L 1 ## 3 54 A L 1 ## 4 25 A L 1 ## 5 70 A L 1 ## 6 52 A L 1 確率分布（family）には負の二項分布(negbinomial)、リンク関数（link）にはlogを指定する。 result_brms_negbin = brms::brm(data = d, breaks ~ 1 + A, family = negbinomial(link = &quot;log&quot;), seed = 1) summary(result_brms_negbin) ## Family: negbinomial ## Links: mu = log ## Formula: breaks ~ 1 + A ## Data: d (Number of observations: 54) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.23 0.09 3.06 3.40 1.00 3304 2674 ## A 0.21 0.12 -0.03 0.44 1.00 3150 2910 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 6.79 1.65 4.07 10.48 1.00 3163 2654 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 20.4.4 順序ロジスティック回帰 第13章のサンプルデータを使って、brm()で順序ロジスティック回帰を行ってみる。 ###サンプルデータの作成 Sleep = c(6,1,5,2,5,6,2,6,2,5,6,2,5,3,5,3,3,7,2,7,6,1,2,1,7,1,1,7,5,3) Score = c(3,3,3,2,3,3,5,5,2,2,2,3,4,1,3,2,3,5,1,4,4,3,3,3,4,1,3,3,3,2) Score = factor(Score, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;), ordered = TRUE) sample_ordered = data.frame(Score = Score, Sleep = Sleep) head(sample_ordered) ## Score Sleep ## 1 3 6 ## 2 3 1 ## 3 3 5 ## 4 2 2 ## 5 3 5 ## 6 3 6 確率分布（family）にcumulative、リンク関数（link）にはlogitを指定する。 result_brms_cum = brms::brm(data = sample_ordered, Score ~ 1 + Sleep, family = cumulative(link = &quot;logit&quot;), seed = 1) summary(result_brms_cum) ## Family: cumulative ## Links: mu = logit ## Formula: Score ~ 1 + Sleep ## Data: sample_ordered (Number of observations: 30) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -0.79 0.85 -2.57 0.80 1.00 3034 2855 ## Intercept[2] 0.77 0.74 -0.69 2.29 1.00 3928 3263 ## Intercept[3] 3.20 0.97 1.38 5.23 1.00 2810 2563 ## Intercept[4] 4.48 1.13 2.45 6.82 1.00 2888 2695 ## Sleep 0.46 0.18 0.11 0.84 1.00 2813 2578 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 20.4.5 多項ロジスティック回帰 第13章のサンプルデータを使って、brm()で多項ロジスティック回帰を行ってみる。 ###サンプルデータの作成 set.seed(1) Male = c(rep(0:1, 25)) Grade = rnorm(n=50, 5, 2) Faculty = c(rep(&quot;Literature&quot;, 15), rep(&quot;Economics&quot;, 20), rep(&quot;Physical&quot;, 15)) sample_mnl = data.frame(Faculty = Faculty, Male = Male, Grade = Grade) head(sample_mnl) ## Faculty Male Grade ## 1 Literature 0 3.747092 ## 2 Literature 1 5.367287 ## 3 Literature 0 3.328743 ## 4 Literature 1 8.190562 ## 5 Literature 0 5.659016 ## 6 Literature 1 3.359063 確率分布（family）としてカテゴリカル分布（categorical）、logitを指定する。 result_brms_mnl = brms::brm(data = sample_mnl, Faculty ~ 1 + Male + Grade, family = categorical(link = &quot;logit&quot;), seed = 1) summary(result_brms_mnl) ## Family: categorical ## Links: muLiterature = logit; muPhysical = logit ## Formula: Faculty ~ 1 + Male + Grade ## Data: sample_mnl (Number of observations: 50) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## muLiterature_Intercept -0.41 1.30 -3.10 2.09 1.00 4645 ## muPhysical_Intercept -0.84 1.32 -3.46 1.70 1.00 4350 ## muLiterature_Male -0.12 0.72 -1.56 1.24 1.00 4269 ## muLiterature_Grade 0.03 0.22 -0.40 0.49 1.00 4846 ## muPhysical_Male 0.18 0.72 -1.21 1.61 1.00 4464 ## muPhysical_Grade 0.09 0.22 -0.35 0.53 1.00 4390 ## Tail_ESS ## muLiterature_Intercept 3144 ## muPhysical_Intercept 2935 ## muLiterature_Male 3261 ## muLiterature_Grade 3169 ## muPhysical_Male 2987 ## muPhysical_Grade 2970 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 20.4.6 ゼロ過剰ポアソンモデル 第13章のサンプルデータを使って、brm()でゼロ過剰ポアソンモデルを当てはめてみる。 y = c(1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 4, 4, 0, 0, 0, 3, 0, 1, 1, 7, 0, 0, 5, 1, 4, 0, 2) Rain = c(0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0) Humidity = c(50, 50, 59, 58, 56, 59, 58, 51, 30, 56, 49, 48, 35, 45, 54, 64, 49, 54, 49, 36, 46, 46, 49, 61, 58, 48, 47, 57, 56, 43) Temperature = c(29, 30, 31, 30, 31, 30, 29, 30, 29, 31, 32, 30, 29, 31, 30, 32, 30, 31, 30, 29, 30, 28, 31, 30, 32, 30, 29, 31, 29, 29) d_zip = data.frame(y = y, Rain = Rain, Humidity = Humidity, Temperature = Temperature) head(d_zip) ## y Rain Humidity Temperature ## 1 1 0 50 29 ## 2 2 0 50 30 ## 3 0 1 59 31 ## 4 0 1 58 30 ## 5 2 0 56 31 ## 6 0 1 59 30 brm()では、ゼロ過剰ポアソンモデルを扱うためのzero_inflated_poissonという確率分布のfamilyが用意されている。bf()で、カウントデータを予測する線形予測子と、ゼロ過剰(zi)を予測する線形予測子のそれぞれを分けて入力する。リンク関数は、カウントデータの推定(link)にはlog、ゼロの推定（link_zi）にはlogitを指定する。 result_brms_zip = brms::brm(data = d_zip, bf(y ~ 1 + Humidity + Temperature, zi ~ 1 + Rain), family = zero_inflated_poisson(link = &quot;log&quot;, link_zi = &quot;logit&quot;), seed = 1) summary(result_brms_zip) ## Family: zero_inflated_poisson ## Links: mu = log; zi = logit ## Formula: y ~ 1 + Humidity + Temperature ## zi ~ 1 + Rain ## Data: d_zip (Number of observations: 30) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -13.59 7.31 -27.97 0.71 1.00 2448 2294 ## zi_Intercept -1.32 0.95 -3.63 0.06 1.00 1727 951 ## Humidity -0.06 0.05 -0.16 0.04 1.00 2268 2782 ## Temperature 0.57 0.24 0.10 1.04 1.00 2623 2585 ## zi_Rain 2.46 1.22 0.45 5.20 1.00 2233 1270 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). カウントデータの予測にポアソン分布ではなく負の二項分布を扱う場合には、zero_inflated_negbinomialが用意されている。 brms::brm(data = d_zip, bf(y ~ 1 + Humidity + Temperature, zi ~ 1 + Rain), family = zero_inflated_negbinomial(link = &quot;log&quot;, link_zi = &quot;logit&quot;), seed = 1) 20.5 brmsパッケージでのマルチレベルモデル 繰り返し測定を含むデータの場合は、マルチレベルモデルで個人差や集団差を統制する必要がある（マルチレベルモデルについては、第13章を参照のこと）。brm()では、マルチレベルモデルを扱うこともできる。 lme4パッケージのglmer()関数と同様の形式でランダム効果を加えることで、マルチレベルモデルの推定を行うことができる。 第13章で例として用いたirisデータを使って、マルチレベルモデルのベイズ推定を行ってみよう。 20.5.1 ランダム切片 グループごとに異なる切片（ランダム切片）を考慮する場合、式の中に(1|グループを意味する変数)というかたちでランダム切片を加える。 model_brm_lmm = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width + (1|Species),#(1|Species)をランダム切片として加える family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(model_brm_lmm) ## Family: gaussian ## Links: mu = identity ## Formula: Sepal.Length ~ 1 + Sepal.Width + (1 | Species) ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Multilevel Hyperparameters: ## ~Species (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.63 0.88 0.56 3.76 1.04 129 127 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.51 1.03 1.46 5.75 1.06 50 25 ## Sepal.Width 0.81 0.11 0.59 1.00 1.01 507 1882 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.44 0.03 0.39 0.49 1.05 73 158 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 上の例では、あやめの種（Species）ごとに切片（Intercept）が異なるという前提で推定を行っている。Group-Level Effectsに、ランダム切片の分散の推定結果が出力されている。 20.5.2 ランダム傾き グループごとに異なる傾き、すなわちランダム傾きをモデルに入れることもできる。式の中に(予測変数|グループを意味する変数)というかたちで加える。 model_brm_lmm_2 = brms::brm(data= iris, Sepal.Length ~ 1 + Sepal.Width + (Sepal.Width|Species), #(Sepal.Width|Species)を加える。Speciesごとに異なる切片とSepal.Widthに係る傾きを想定する。 family = gaussian(link=&quot;identity&quot;), seed = 1 ) summary(model_brm_lmm_2) ## Family: gaussian ## Links: mu = identity ## Formula: Sepal.Length ~ 1 + Sepal.Width + (Sepal.Width | Species) ## Data: iris (Number of observations: 150) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Multilevel Hyperparameters: ## ~Species (Number of levels: 3) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 1.24 0.97 0.07 3.78 1.00 906 ## sd(Sepal.Width) 0.38 0.34 0.02 1.28 1.00 831 ## cor(Intercept,Sepal.Width) 0.13 0.56 -0.91 0.97 1.00 1464 ## Tail_ESS ## sd(Intercept) 837 ## sd(Sepal.Width) 1336 ## cor(Intercept,Sepal.Width) 1915 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.42 0.81 1.74 5.20 1.00 832 928 ## Sepal.Width 0.80 0.24 0.26 1.30 1.01 875 628 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.44 0.03 0.40 0.50 1.00 1995 2409 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 上の例では、あやめの種（Species）ごとにSepal.Widthに係る傾きが異なるという前提で推定を行っている。 Group-Level Effectsの部分に、ランダム傾きとランダム切片の分散の推定結果が表示されている。同時に、ランダム効果同士（グループごとの傾きと切片）の相関の推定結果も出力される。 20.6 その他 20.6.1 Stanコードの出力 make_stancode()で、モデルのStanコードを出力することができる。モデルに直接修正を加えたいときに使える。 brms::make_stancode(data = iris, Petal.Length ~ Sepal.Length, family = gaussian(link=&quot;identity&quot;)) ## // generated with brms 2.23.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int&lt;lower=1&gt; Kc; // number of population-level effects after centering ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## matrix[N, Kc] Xc; // centered version of X without an intercept ## vector[Kc] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // regression coefficients ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0&gt; sigma; // dispersion parameter ## } ## transformed parameters { ## // prior contributions to the log posterior ## real lprior = 0; ## lprior += student_t_lpdf(Intercept | 3, 4.3, 2.5); ## lprior += student_t_lpdf(sigma | 3, 0, 2.5) ## - 1 * student_t_lccdf(0 | 3, 0, 2.5); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## target += normal_id_glm_lpdf(Y | Xc, Intercept, b, sigma); ## } ## // priors including constants ## target += lprior; ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept - dot_product(means_X, b); ## } 20.6.2 より深く学ぶには このように、brm()を使えばStanに関するプログラミングの知識が特になくても、簡単にMCMCで一般化線形モデルのパラメータ推定を行うことができる。ただし、この章の内容は、あくまでMCMCの練習に過ぎない。特にマルチレベルモデルのような複雑なモデルの推定は、推定結果が収束しないなど、うまく行かないケースに直面することも多い。 本格的にMCMCによる一般化線形モデルのベイズ推定を行うとなると、事前分布やMCMCの設定についてもっと深い知識が必要になる。より深く学ぶには、StanのWebサイトあるいはRとStanの使い方に関する解説書などを参照してほしい（付録の参考文献リストに示している）。 "],["20-Appendix_References.html", "A 確率分布 A.1 連続型確率分布 A.2 離散型確率分布 B R Markdownの使い方 C 参考資料 C.1 文献 C.2 Webサイト", " A 確率分布 ここでは、代表的な確率分布とその確率分布を扱うRの関数を説明する。 A.1 連続型確率分布 正規分布 指数分布 対数正規分布 t分布 カイ二乗分布 F分布 コーシー分布 正規分布 正規分布(normal distribution)は、平均\\(\\mu\\)、標準偏差\\(\\sigma\\)を持つ確率分布である。ガウス分布（gaussian distribution）とも呼ばれる。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] Rでは、正規分布を扱うための関数としてnormが用意されている。 #mean = 平均、sd = 標準偏差 dnorm(x = seq(-3,3, 0.1), mean = 0, sd = 1) qnorm(p = 0.95, mean = 0, sd = 1) pnorm(q = 1.64, mean = 0, sd = 1) rnorm(n = 10, mean = 0, sd = 1) norm_1 = data.frame(x = seq(-4,4, 0.1), f = dnorm(x = seq(-4,4, 0.1), mean = 0, sd = 1), par = &quot;mean = 0, sd = 1&quot;) norm_2 = data.frame(x = seq(-4,4, 0.1), f = dnorm(x = seq(-4,4, 0.1), mean = 1, sd = 1), par = &quot;mean = 1, sd = 1&quot;) norm_3 = data.frame(x = seq(-4,4, 0.1), f = dnorm(x = seq(-4,4, 0.1), mean = 0, sd = 2), par = &quot;mean = 0, sd = 2&quot;) norm = rbind(norm_1, norm_2, norm_3) ggplot() + ggplot2::geom_line(data = norm, aes(x = x, y = f, colour = par, linetype = par)) + labs(y = &quot;f(x)&quot;, x = &quot;x&quot;, colour = &quot;mean, sd&quot;, linetype = &quot;mean, sd&quot;) + theme_classic() 指数分布 指数分布(exponential distribution)は連続型確率分布であり、パラメータは\\(\\lambda\\)である。指数分布の確率変数は、ゼロ以上の正の値のみを取りうる。 \\[ f(x) = \\lambda \\exp(-\\lambda x)\\\\ x \\sim \\text{Exponential}(\\lambda)\\\\ \\] Rでは、指数分布を扱うための関数としてexpが用意されている。 #rate = パラメータ dexp(x = seq(0,3, 0.1), rate = 1) qexp(p = 0.95, rate = 1) pexp(q = 3.0, rate = 1) rexp(n = 10, rate = 1) exp_1 = data.frame(x = seq(0,3, 0.1), f = dexp(seq(0,3, 0.1), rate = 0.5), rate = 0.5) exp_2 = data.frame(x = seq(0,3, 0.1), f = dexp(seq(0,3, 0.1), rate = 1), rate = 1) exp_3 = data.frame(x = seq(0,3, 0.1), f = dexp(seq(0,3, 0.1), rate = 2), rate = 2) exp = rbind(exp_1, exp_2, exp_3) ggplot() + ggplot2::geom_line(data = exp, aes(x = x, y = f, colour = factor(rate), linetype = factor(rate))) + labs(y = &quot;f(x)&quot;, x = &quot;x&quot;, colour = &quot;rate&quot;, linetype = &quot;rate&quot;) + theme_classic() ある一定期間の間に平均して\\(\\lambda\\)回生じるイベントの時間間隔は、指数分布に従う。災害が生じてから次の災害が生じるまでの期間、店に客が来てから次の客が来るまでの期間などは指数分布に従う。 対数正規分布 対数正規分布(log-normal distribution)は連続型確率分布であり、正規分布と同様に平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の２つのパラメータを持つ分布である。名前の通り、正規分布とは深い関係にある。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}x}\\exp\\left(-\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right)\\\\ x \\sim \\text{logNormal}(\\mu, \\sigma)\\\\ \\] Rでは、対数正規分布を扱うための関数としてlnormが用意されている。 dlnorm(x = seq(0, 5, 0.1), meanlog = 0, sdlog = 1) qlnorm(p = 0.95, meanlog = 0, sdlog = 1) plnorm(q = 5.18, meanlog = 0, sdlog = 1) rlnorm(n = 10, meanlog = 0, sdlog = 1) lnorm_1 = data.frame(x=seq(0,5, 0.1), f = dlnorm(seq(0, 5, 0.1), meanlog = 0, sdlog = 1), par = &quot;mean=0, sd = 1&quot;) lnorm_2 = data.frame(x=seq(0,5, 0.1), f = dlnorm(seq(0, 5, 0.1), meanlog = 0.5, sdlog = 1), par = &quot;mean=0.5, sd = 1&quot;) lnorm_3 = data.frame(x=seq(0,5, 0.1), f = dlnorm(seq(0, 5, 0.1), meanlog = 0, sdlog = 2), par = &quot;mean=0, sd = 2&quot;) lnorm = rbind(lnorm_1, lnorm_2, lnorm_3) ggplot() + ggplot2::geom_line(data = lnorm, aes(x = x, y = f, colour = factor(par), linetype = factor(par))) + labs(y = &quot;f(x)&quot;, x = &quot;x&quot;, colour = &quot;par&quot;, linetype = &quot;par&quot;) + theme_classic() 対数正規分布は、右の方に長い裾を描く分布である。対数正規分布に従うものとしては、年収の分布などが知られている。 正規分布との関係 ランダムな変数を足し合わせた物は正規分布に従うのに対し、ランダムな変数をかけ合わせたものは対数正規分布に従う。 #サイコロを7回振って、それぞれの値で掛け算をする。これを10,000回行ったときの出目の合計値の分布 sample.prod = sapply(c(1:10000), function(x) {prod(round(runif(n = 7,min = 1,max = 6),0))} ) #prod()はカッコ内のベクトルの要素をかけ合わせる関数 hist(sample.prod) 「ランダムな変数をかけ合わせたもの」の対数を取ってその分布を確認すると、正規分布のかたちになることがわかる。 sample.prod_2 = log(sample.prod) hist(sample.prod_2) 掛け算の対数を取るということは、足し算に直すことと同じである。つまり、対数を取ることで「ランダムな変数をかけ合わせたもの」が「ランダムな変数を足し合わせたもの」に変換される。「ランダムな変数を足し合わせたもの」は、中心極限定理により正規分布に従う。したがって、対数正規分布の対数を取ったものは正規分布になる。 t分布 t分布(Student’s t distribution)は、自由度\\(v\\)をパラメータとする連続型確率分布である。 標本平均\\(\\bar{X}\\)、不偏標本分散\\(S^2\\)、サンプルサイズ\\(n\\)から、以下の統計量\\(t\\)を計算する（母集団平均\\(\\mu\\)は\\(\\bar{X}\\)と等しいと仮定する）。 \\[ t = \\frac{\\bar{X}-\\mu}{\\sqrt{S^2n}} \\] \\(t\\)は、以下の自由度\\(v\\)を持つt分布に従う（\\(v=n-1\\)とする）。 $$ f(t) = (1+t{2}/v){-(v+1)/2}\\ x (v)\\ $$ Rでは、t分布を扱うための関数としてtが用意されている。 #dfが自由度 dt(x = seq(-3, 3, 0.01), df = 30) qt(p = 0.95, df = 30) pt(q = 1.69, df = 30) rt(n = 10, df = 30) 以下に、\\(df=10\\), \\(df=50\\), \\(df=100\\)のt分布の例を示す。 t分布は、自由度\\(v\\)（サンプルサイズ）によって形状が変化する。 dat_t5 = data.frame(x = seq(-3, 3, 0.01), y = dt(x = seq(-3, 3, 0.01), df = 5), df = 5) dat_t10 = data.frame(x = seq(-3, 3, 0.01), y = dt(x = seq(-3, 3, 0.01), df = 10), df = 10) dat_t50 = data.frame(x = seq(-3, 3, 0.01), y = dt(x = seq(-3, 3, 0.01), df = 50), df = 50) dat_t = rbind(dat_t5, dat_t10, dat_t50) ggplot() + geom_line(data = dat_t, aes(x = x, y = y, linetype = factor(df))) + xlim(-3,3) + labs(x = &quot;y&quot;, y = &quot;density&quot;, linetype = &quot;df&quot;) + theme_classic() カイ二乗分布 標準正規分布（平均=0, 標準偏差=1の正規分布）\\(Z\\)の二乗を\\(v\\)回足し合わせた分布は、自由度\\(v\\)を持つカイ二乗分布(Cahi-Squared distribution)に従う。 \\[ \\chi^2 = \\sum_{k=1}^v Z_{k}^2 \\] Rでは、カイ二乗分布を扱うための関数としてchisqが用意されている。 dchisq(x = seq(0, 5, 0.01), df = 2) qchisq(p = 0.95, df = 2) pchisq(q = 5.99, df = 2) rchisq(n = 10, df = 2) 以下に、自由度を1, 5, 10に変えたカイ二乗分布を示す。 dat_chisq_1 = data.frame(x = seq(0, 5, 0.01), y= dchisq(x = seq(0, 5, 0.01), df = 1), df = 1) dat_chisq_5 = data.frame(x = seq(0, 5, 0.01), y= dchisq(x = seq(0, 5, 0.01), df = 5), df = 5) dat_chisq_10 = data.frame(x = seq(0, 5, 0.01), y= dchisq(x = seq(0, 5, 0.01), df = 10), df = 10) dat_chisq = rbind(dat_chisq_1, dat_chisq_5, dat_chisq_10) ggplot() + geom_line(data = dat_chisq, aes(x = x, y = y, linetype = factor(df))) + xlim(0,5) + ylim(0,1) + labs(x = &quot;y&quot;, y = &quot;density&quot;, linetype = &quot;df&quot;) + theme_classic() F分布 自由度がそれぞれ\\(v_{1}\\)と\\(v_{2}\\)である2つのカイ二乗分布の比として表される以下の\\(F\\)は、自由度\\(v_{1}\\)と\\(v_{2}\\)を持つF分布(F distribution)に従う。F分布のパラメータは、2つの自由度である。 \\[ F = \\frac{\\chi^2_{v_{1}}/v_{1}}{\\chi^2_{v_{2}}/v_{2}} \\] Rでは、F分布を扱うための関数としてfが用意されている。 df(x = seq(0, 5, 0.01), df1 = 10, df2 = 10) qf(p = 0.95, df1 = 10, df2 = 10) pf(q = 2.98, df1 = 10, df2 = 10) rf(n = 10, df1 = 10, df2 = 10) 以下に、自由度\\(v_{1}\\)を10に固定して、自由度\\(v_{2}\\)をそれぞれ、10, 50, 100に変えたF分布を示す。 dat_f_10 = data.frame(x = seq(0, 5, 0.01), y= df(x = seq(0, 5, 0.01), df1 = 10, df2 = 10), df2 = 10) dat_f_50 = data.frame(x = seq(0, 5, 0.01), y= df(x = seq(0, 5, 0.01), df1 = 10, df2 = 50), df2 = 50) dat_f_100 = data.frame(x = seq(0, 5, 0.01), y= df(x = seq(0, 5, 0.01), df1 = 10, df2 = 100), df2 = 100) dat_f = rbind(dat_f_10, dat_f_50, dat_f_100) ggplot() + geom_line(data = dat_f, aes(x = x, y = y, linetype = factor(df2))) + xlim(0,5) + ylim(0,1) + labs(x = &quot;y&quot;, y = &quot;density&quot;, linetype = &quot;df2&quot;, title = &quot;df1 = 10&quot;) + theme_classic() コーシー分布 コーシー分布(Cauchy distribution)は形状が正規分布に似ているが、裾が広いことが特徴である。つまり、極端な値が出る確率が正規分布よりも大きい。パラメータは、\\(l\\)(location)と\\(s\\)(scale)の2つである。 $$ f(x) = (1+(){2}){-1}\\ x (l, s)\\ $$ Rでは、コーシー分布を扱うための関数としてcauchyが用意されている。 dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 1) qcauchy(p = 0.95, location = 0, scale = 1) pcauchy(q = 6.31, location = 0, scale = 1) rcauchy(n = 10, location = 0, scale = 1) 以下に、\\(l=0\\)かつ\\(s=1\\)、\\(l=0\\)かつ\\(s=3\\)のコーシー分布の例を示す。 dat_cauchy_1 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 1), scale = 1) dat_cauchy2 = data.frame(x = seq(-6, 6, 0.01), y= dcauchy(x = seq(-6, 6, 0.01), location = 0, scale = 3), scale = 3) dat_cauchy = rbind(dat_cauchy_1, dat_cauchy2) ggplot() + geom_line(data = dat_cauchy, aes(x = x, y = y, linetype = factor(scale))) + xlim(-6,6) + labs(x = &quot;y&quot;, y = &quot;density&quot;, linetype = &quot;scale&quot;) + theme_classic() 中心極限定理との関係 コーシー分布は、極端な値が存在することにより、その分布の特徴を平均や標準偏差などで捉えることができない。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) #location = 0, scale = 1のコーシー分布から乱数を100個生成する mean(d_cauchy) ## [1] -0.02417088 median(d_cauchy) ## [1] 0.2940343 sd(d_cauchy) ## [1] 6.896175 コーシー分布には中心極限定理を適用できない。 コーシー分布から乱数を100個作って足し合わせることを5回やり、分布を確認する。足し合わせても、正規分布に近似しないことがわかる。 d_cauchy = rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) + rcauchy(n = 100, location = 0, scale = 1) ggplot() + geom_histogram(data = NULL, aes(x = d_cauchy)) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick ## better value `binwidth`. A.2 離散型確率分布 二項分布 ポアソン分布 負の二項分布 幾何分布 超幾何分布 二項分布 二項分布(binomial distribution)は、ある事象が生じる確率（成功確率）を\\(q\\)としたとき、試行回数\\(n\\)回のうち成功する回数\\(x\\)が従う確率分布である。パラメータは、成功確率\\(q\\)と試行回数の\\(n\\)である。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] コイントスで、表が出るのを成功、裏が出るのを失敗とする。二項分布は、全試行中表が出る回数の確率を表している。 Rでは、二項分布を扱うための関数としてbinomが用意されている。 #size = 全試行回数, prob = 成功試行が生じる確率 dbinom(x = 0:10, size = 10, prob = 0.5) qbinom(p = 0.95, size = 10, prob = 0.5) pbinom(q = 8, size = 10, prob = 0.5) rbinom(n = 10, size = 10, prob = 0.5) 以下は、全試行回数\\(n = 10\\)とし、\\(q=0.2\\), \\(q=0.5\\), \\(q=0.6\\)としたときの二項分布である。 binom_1 = data.frame(x = 0:10, p = dbinom(0:10, size = 10, prob = 0.2), q = 0.2) binom_2 = data.frame(x = 0:10, p = dbinom(0:10, size = 10, prob = 0.5), q = 0.5) binom_3 = data.frame(x = 0:10, p = dbinom(0:10, size = 10, prob = 0.6), q = 0.6) binom = rbind(binom_1, binom_2, binom_3) ggplot() + ggplot2::geom_bar(data = binom, aes(x = factor(x), y = p, fill = factor(q)), color = &quot;black&quot;, stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(y = &quot;P(x)&quot;, x = &quot;number of success&quot;, fill = &quot;q&quot;, title = &quot;number of trials = 10&quot;) + theme_classic() 第6章も参照のこと。 負の二項分布 負の二項分布(negative binomial distribution)は、ある事象が生じる確率（成功確率）を\\(q\\), 成功回数を\\(r\\)としたとき、 失敗回数\\(x\\)を確率変数とした場合に\\(x\\)が従う確率分布である。パラメータは、成功確率\\(q\\)と成功回数\\(r\\)である。 （二項分布とは異なり、試行回数が定まっていない） \\[ P(x) = {}_{x+r-1}\\mathrm{C}_{r-1} q^{r}(1-q)^{x}\\\\ x \\sim \\text{Negative Binomial}(r, q) \\] コイントスを例として考える。表が出るのを成功、裏が出るのを失敗とする。３回表が出たら投げるのをやめるとした場合、３回表が出るまでに裏が出る回数の確率は負の二項分布に従う。 Rでは、負の二項分布を扱うための関数としてnbinomが用意されている。 #size = 成功回数, prob = 成功試行が生じる確率 dnbinom(x = 0:10, size = 5, prob = 0.5) qnbinom(p = 0.95, size = 5, prob = 0.5) pnbinom(q = 18, size = 5, prob = 0.5) rnbinom(n = 10, size = 5, prob = 0.5) 以下は、成功回数\\(r = 3\\)とし、\\(q=0.2\\), \\(q=0.5\\), \\(q=0.6\\)としたときの負の二項分布である。 nbinom_1 = data.frame(x = 0:10, p = dnbinom(0:10, size = 3, prob = 0.2), q = 0.2) nbinom_2 = data.frame(x = 0:10, p = dnbinom(0:10, size = 3, prob = 0.5), q = 0.5) nbinom_3 = data.frame(x = 0:10, p = dnbinom(0:10, size = 3, prob = 0.6), q = 0.6) nbinom = rbind(nbinom_1, nbinom_2, nbinom_3) ggplot() + ggplot2::geom_bar(data = nbinom, aes(x = factor(x), y = p, fill = factor(q)), color = &quot;black&quot;, stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(y = &quot;P(x)&quot;, x = &quot;number of failure&quot;, fill = &quot;q&quot;, title = &quot;number of success = 3&quot;) + theme_classic() 幾何分布 幾何分布 (geometric distribution)は、成功確率を\\(q\\)としたとき、失敗が\\(x\\)回繰り返し続いたあとで初めて成功するまでの\\(x\\)が従う確率分布である。 コイントスを例とすると、裏（失敗）が何回か続いて初めて初めて表（成功）が出るときの確率である。 \\[ P(x) = q(1-q)^{x}\\\\ x \\sim \\text{Geometric}(q)\\\\ \\] Rでは、幾何分布を扱うための関数としてgeomが用意されている。 #prob = 成功試行が生じる確率 dgeom(x = 0:10, prob = 0.5) qgeom(p = 0.95, prob = 0.5) pgeom(q = 4, prob = 0.5) rgeom(n = 10, prob = 0.5) 以下は、\\(q=0.2\\), \\(q=0.5\\), \\(q=0.6\\)の幾何分布である。 geom_1 = data.frame(x = 0:10, p = dgeom(0:10, prob = 0.2), q = 0.2) geom_2 = data.frame(x = 0:10, p = dgeom(0:10, prob = 0.5), q = 0.5) geom_3 = data.frame(x = 0:10, p = dgeom(0:10, prob = 0.6), q = 0.6) geom = rbind(geom_1, geom_2, geom_3) ggplot() + ggplot2::geom_bar(data = geom, aes(x = factor(x), y = p, fill = factor(q)), color = &quot;black&quot;, stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(y = &quot;P(x)&quot;, x = &quot;x&quot;, fill = &quot;q&quot;) + theme_classic() 超幾何分布 超幾何分布 (hypergeometric distribution)について、次のような場合を考える。つぼの中に玉が入っていて、白玉が\\(m\\)個、黒玉が\\(n\\)個入っている。そのつぼの中から玉を\\(k\\)個取り出した時、取り出した玉のうち白玉の個数\\(x\\)が従う確率分布が超幾何分布である。超幾何分布は、非復元抽出（玉を取り出したらつぼに戻さないで、引き続き玉を取り出す）によるサンプリングに基づく確率分布である。 \\[ P(x) = \\frac{{}_m C_x \\quad {}_n C_{k-x}}{{}_{m+n} C_k}\\\\ x \\sim \\text{HyperGeometric}(m,n,k)\\\\ \\] Rでは、超幾何分布を扱うための関数としてhyperが用意されている。 # m = 白玉, n = 黒玉, k = 取り出す玉の数 dhyper(x = 0:5, m = 5, n = 5, k = 5) qhyper(p = 0.95, m = 5, n = 5, k = 5) phyper(q = 4, m = 5, n = 5, k = 5) rhyper(nn = 10, m = 5, n = 5, k = 5) 以下は、\\(m=10\\), \\(n=30\\), \\(k=10\\)の超幾何分布である。 hyper = data.frame(x = 0:10, p = dhyper(x = 0:10, m = 10, n = 30, k = 10)) ggplot() + ggplot2::geom_bar(data = hyper, aes(x = factor(x), y = p), color = &quot;black&quot;, stat = &quot;identity&quot;) + labs(y = &quot;P(x)&quot;, x = &quot;x&quot;) + theme_classic() Fisherの正確検定との関係 超幾何分布は、フィッシャーの正確検定(Fisher’s exact test)でp値を求めるときに使われる。フィッシャーの正確検定は、2つのカテゴリカル変数の間に有意な関連があるかを検定するのに使われる。 つぼから白玉と黒玉を取り出す例で考えると、それぞれの頻度は以下のクロス表で表すことができる。 玉の色 取り出した玉 つぼの中の玉 合計 白玉 x m - x m 黒玉 k - x n - (k - x) n 合計 k m + n - k m + n 例えば、白玉が10個(\\(m=10\\))と黒玉が30個(\\(n=30\\))入っているつぼから玉を10個( \\(k=10\\))取り出すとする。ここで、取り出した白玉が6個(\\(x = 6\\))だったとする。クロス表でまとめると以下になる。 玉の色 取り出した玉 つぼの中の玉 合計 白玉 6 4 10 黒玉 4 26 30 合計 10 30 40 このとき、取り出した玉のうち白玉の個数が\\(x\\)個である確率は、先ほどの、\\(m=10\\), \\(n=30\\), \\(k=10\\)の超幾何分布に従う。この幾何分布を帰無仮説として、今回の結果である\\(x=6\\)よりも珍しい結果が生じる確率(p値)を求める。 さきほどの\\(m=10\\), \\(n=30\\), \\(k=10\\)の超幾何分布のグラフでいえば、\\(p(x = 6), p(x = 7), p(x = 8), p(x = 9),p(x = 10)\\)の合計がp値を意味することになる（\\(p(x = 8)\\)以下である確率の合計）。直接計算すると、以下のとおりである。 p = dhyper(x = 0:10, m = 10, n = 30, k = 10) p[7] + p[8] + p[9] + p[10] + p[11] ## [1] 0.00738754 Rにあるfisher.test関数でも計算することができる。 tab = matrix(c(6, 4, 4, 26), nrow = 2, ncol = 2) #クロス表を作成する fisher.test(tab)#fisher.testの中に表のデータを入れる ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tab ## p-value = 0.007388 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.44215 68.65968 ## sample estimates: ## odds ratio ## 9.002545 ポアソン分布 ポアソン分布 (Poisson distribution)は、0以上の整数\\(x\\)（\\(x = 0, 1, 2, 3, ...\\)）が従う確率分布である。パラメータは\\(\\lambda\\)のみで、\\(\\lambda\\)は分布の平均と分散を表す。つまり、ポアソン分布は平均と分散が等しい分布である。 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ \\] Rでは、ポアソン分布を扱うための関数としてpoisが用意されている。 #lambda = パラメータ（平均・分散） dpois(x = 0:10, lambda = 2) qpois(p = 0.95, lambda = 2) ppois(q = 5, lambda = 2) rpois(n = 10, lambda = 2) 以下に、\\(\\lambda = 1\\), \\(\\lambda = 2\\), \\(\\lambda = 3\\)のポアソン分布を示す。 pois_1 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=1), lambda=1) pois_2 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=2), lambda=2) pois_3 = data.frame(x=seq(0,10), p=dpois(seq(0,10), lambda=3), lambda=3) pois = rbind(pois_1, pois_2, pois_3) ggplot() + ggplot2::geom_bar(data = pois, aes(x=factor(x), y=p, fill=factor(lambda)), stat=&quot;identity&quot;, color = &quot;black&quot;, position = &quot;dodge&quot;) + labs(y= &quot;P(x)&quot;, x = &quot;x&quot;, fill = &quot;lambda&quot;) + theme_classic() 二項分布との関係 二項分布の成功確率パラメータを\\(p=\\lambda/n\\)とする。試行回数である\\(n\\)を大きくすると、二項分布はパラメータ\\(\\lambda\\)を持つポアソン分布に近似する。すなわち、生じる確率が小さいイベントはポアソン分布に従う。 ポアソン分布については、第7章も参照のこと。 B R Markdownの使い方 RStudioならば、R Markdownの機能を使うことで、自分が行った分析のコードや出力をドキュメント形式で残しておくことができて便利である。 以下に、R Markdownの簡単な使い方について示す。 New Fileから「R Markdown」を選択する 「New R Markdown」というウィンドウが出てくる。「Document」を選んで「OK」を選ぶ。Title（ドキュメントのタイトル）やAuthor（著者名）の情報を入力しても構わない。 R Markdownドキュメントがサンプルコードとともに表示される。 試しにこのサンプルコードをHTMLファイルで出力してみよう。 「Knit」のプルダウンから、「Knit to HTML」を選ぶ。 ドキュメントのプレビューが出力される。 R Markdownファイルを保存する。 「.Rmd」という拡張子のファイルで保存される。「.Rmd」ファイルをRStudioで開けば、編集することができる。 このように、R Markdownに記した文章、Rのコード、出力結果（分析結果、グラフなど）をドキュメントで保存することができる。 R Markdownの使い方の詳細については、他の資料を参照のこと。 RmarkdownのCheatsheetsにも、Rmarkdownのチートシートが掲載されている。「Translations」のところに日本語訳もある。 https://www.rstudio.com/resources/cheatsheets/ C 参考資料 C.1 文献 C.1.1 統計学の解説書 東京大学教養学部統計学教室 編 (1991). 統計学入門 東京大学出版会 南風原朝和 (2002). 心理統計学の基礎：統合的理解のために 有斐閣 大久保街亜・岡田謙介 (2012). 伝えるための心理統計：効果量・信頼区間・検定力 勁草書房 阿部真人(2021). データ分析に必須の知識・考え方 統計学入門：仮説検定から統計モデリングまで重要トピックを完全網羅 ソシム C.1.2 Rを用いた統計解析の解説書 山田剛史・杉澤武俊・村井潤一郎 (2008). Rによるやさしい統計学 オーム社 石田基広 監修、奥村晴彦 (2016). Rで楽しむ統計 共立出版 金明哲 (2016). 定性的データ分析（Useful R 5） 共立出版 嶋田正和・阿部真人 (2017). Rで学ぶ統計学入門 東京化学同人 C.1.3 データ・ビジュアライゼーション Nordmann E, McAleer P, Toivo W, Paterson H, DeBruine LM. Data Visualization Using R for Researchers Who Do Not Use R. Advances in Methods and Practices in Psychological Science. 2022;5(2). doi:10.1177/25152459221074654 C.1.4 一般化線形モデル、マルチレベルモデルなどの解説書 久保拓哉 (2012). データ解析のための統計モデリング入門：一般化線形モデル・階層ベイズモデル・MCMC 岩波書店 金明哲 編、粕谷英一 (2012). 一般化線形モデル（Rで学ぶデータサイエンス 10） 共立出版 Brown VA. An Introduction to Linear Mixed-Effects Modeling in R. Advances in Methods and Practices in Psychological Science. 2021;4(1). doi:10.1177/2515245920960351 C.1.5 ベイズ統計モデリングの解説書 石田基広 監修、松浦健太郎 (2016). StanとRでベイズ統計モデリング 共立出版 馬場真哉 (2019). RとStanではじめるベイズ統計モデリングによるデータ分析入門（実践Data Science シリーズ） 講談社 McElrearth R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd Ed.) Boca Raton, FL: CRC Press C.2 Webサイト RStudio Cheatsheets（代表的なパッケージの使い方などが簡単にまとめられた”チートシート”を入手できる） https://rstudio.com/resources/cheatsheets/ tidyverseの公式ページ https://www.tidyverse.org Stan User’s Guide https://mc-stan.org/users/documentation/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
