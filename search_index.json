[
["index.html", "心理学データ解析法Ⅰ 2019 はじめに 0.1 この授業について", " 心理学データ解析法Ⅰ 2019 帝京大学文学部心理学科 堀田結孝 2019-06-16 はじめに 0.1 この授業について 応用的なデータ解析を学ぶ。具体的には，以下の解析を学ぶ。 回帰分析 一般化線形モデル 一般化線形混合モデル データ解析の基礎をRの演習を通して学ぶ。この演習で学ぶ知識は，特殊実験演習や卒論など自身の今後の研究にも活かせる知識である。 0.1.1 授業の進め方 R を使って演習を行う。自分でキーボードからプログラムを直接入力して解析を行う。自分で使えるPCを持っているなら，Rをインストールしておくと良い。復習用に使おう。 インストールは，www.r-project.orgから可能。 また，RStudioもインストールしておくことを推奨する。様々な面で操作しやすい。無料版をインストールすればよい。インストールは，www.rstudio.comから可能。 0.1.2 授業資料の配布 LMSから配布する。 0.1.3 練習問題 各回，最後に練習問題を出す。宿題にするときもある。授業日から1週間以内の期限で提出する。提出は帝京大学Web File Severを介して行う。提出先のリンクはそのつど伝える。 0.1.4 授業の前にやること マシンの電源を入れ，インターネットにつなげられる状態にする。ネットワークシステムにアカウントとパスワードを入力する。 R (ver. 3)を起動しておく。デスクトップにある「アプリケーション」フォルダから，「R 3.5.3」を選ぶ。バージョン2ではなく，バージョン3の方なので注意。 0.1.5 この資料について 読み方 章の中の各セクションのタイトルに（＊）がついている箇所は応用項目です（試験範囲外です）。より理解を深めるための内容です。読み飛ばしても章の大筋の理解に支障はないかと思います。 使用上の注意 この資料は帝京大学文学部心理学科「心理データ解析法Ⅰ」向けに作成したものです。もちろん，履修者ないしは帝京大学の学生はダウンロードして使って構いません。 特殊実験実習や卒論のデータ分析の際にも参考にしてください。 ただし，無断で譲渡，頒布，転載等（インターネット上も含む）するなど，著作権の侵害にあたる行為は固く禁止します。 "],
["section-1.html", "Chapter 1 Ｒの使い方 1.1 Ｒを使う前の準備 1.2 プログラムの練習 1.3 データの扱い方 1.4 外部パッケージの利用 1.5 Rを終わらせる 練習問題", " Chapter 1 Ｒの使い方 まずはRの操作になれる。以下を行う。 準備 プログラムに慣れる データの読み書き パッケージのインストール Rを終わらせる 1.1 Ｒを使う前の準備 1.1.1 ファイルの場所の指定 Rを起動すると，「R Console」という画面が表示されるはず。 「File」から「Change dir」を選ぶ。 デスクトップを選んでおく。 「ローカルディスク」→「ユーザー」→「SETUP」→「デスクトップ」を選ぶ（帝京大学841のマシンの場合）。 これで「作業ディレクトリ（working directory）」をデスクトップに設定した。Rで作成したファイルは作業ディレクトリとして設定した場所に保存される。また，作業ディレクトリに保存したファイルをRから読み込むこともできる（後述）。 もちろん，デスクトップ以外の場所も作業ディレクトリに設定することができる。この授業ではとりあえずデスクトップに設定しておく。 帝京大学のマシンは一度電源を切るとデスクトップに保存したファイルが自動で削除されてしまう。必ず，自分のUSBメモリなどに保存しておくこと。 コンソールに以下のプログラムを直接書き込んでもOK。 #作業ディレクトリとしてデスクトップを設定する（帝京の情報処理教室の場合） setwd(&quot;C:/Users/SETUP/Desktop&quot;) #正しく設定されたかを確認する getwd() 1.1.2 スクリプトファイルの作成 「R Console」に直接プログラムを書き込んでも良いが，復習のためにもプログラムは別のファイルに保存しておいて方が良い（コンソールに入力したプログラムは保存されない）。 「File」から「New Script」を選ぶ。何も書かれていないファイル（R Editor）が開かれる。 名前をつけて保存する。「File」から「Save as..」を選び，名前をつけて保存する（自分のUSBファイルにでも保存しておく）。拡張子が「.R」のファイルとして保存される。 スクリプトに，試しに以下のプログラムを入力する。 1 + 1 ## [1] 2 プログラムを選択し，Ctrl+Rで実行する（「Run line or selection」を選んでも可）。すると，「R Console」にプログラムの結果が出力される。 一旦スクリプトを閉じる（閉じる前に，Cntrl+Sで保存しておく）。 Rの「File」から「Open script」を選び，さっき閉じたスクリプトのファイルを選ぶ。すると，さっき作成したプログラムが表示される。 このように，プログラムをスクリプトファイルとして残しておくと便利なので，この授業でも実行したプログラムは基本スクリプトファイルとして保存しておくこと。 1.1.3 tidyverseのインストール Rを使いやすくするために，tidyverseパッケージをインストールしておく。 install.packages(&quot;tidyverse&quot;) 「Please select a CRAN mirror …」というのが表示されたら，Japan (Tokyo)を選んで「OK」を押す。 インストールしただけではパッケージは使えない。使う前にロードする必要がある。library()で，括弧内に使いたいパッケージ名を入力する。 library(&quot;tidyverse&quot;) 通常なら，一度インストールすれば今後はlibrary()でロードするだけで使うことができる。毎回インストールする必要はない。ただし，帝京大学のマシンは，ログアウトするたびに設定がリセットされてしまうので，毎回この作業を行う必要があるので注意。 1.2 プログラムの練習 1.2.1 簡単な計算 まずは簡単な計算を通して，Rが動くことを確かめる。 2 + 3 ## [1] 5 2 - 3 ## [1] -1 2 * 3 ## [1] 6 2 / 3 ## [1] 0.6666667 +, -, *, /で四則計算ができる（それぞれ，キーボードのどの位置にあるかを確認する！）。 なお，#（シャープ）はコメントとして使うことができる（#から改行までの内容は実行されない）。 2 / 3 #これは割り算 ## [1] 0.6666667 1.2.2 変数 次は，変数について理解する。以下のプログラムを入力する。 x = 5 + 8 ##以下でも可 #x &lt;- 5 + 8 x ## [1] 13 13が出力されることを確認する。このように，変数を使って数値を代入することが可能。 以下のプログラムも実行してみよう。 y = x - 2 y ## [1] 11 なお，Rは小文字と大文字を区別するので注意。たとえば，x（小文字）と入力して実行すると結果が出力されるが，X（大文字）では出力されない（変数が作られていないので）。今後プログラムがうまく動かない場面に直面したら，単に変数の記入ミスではないかをまず確認すること。 1.3 データの扱い方 1.3.1 データフレーム エクセルなどから自分で作ったファイルをデータとして読み込むことももちろん出来る。 ただ，その前にRを使って簡単なサンプルデータを作って，データフレームについて理解する。以下のプログラムを入力する。 x = c(1, 2, 3) y = c(4, 5, 6) x ## [1] 1 2 3 y ## [1] 4 5 6 c()はベクトルを作るための関数。ベクトルとは，Rで数値や文字の並びを表現するデータの型である。 更に，以下のプログラムを実行する z = data.frame(x, y) z ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 Rではデータ解析の際には，データをデータフレーム（data.frame）という行列で表現する。data.frame()はデータフレームを作るための関数である。 以下のように，データフレーム$変数名 で，データフレームの特定の列を参照することが出来る。 z$x ## [1] 1 2 3 データフレームに新たに変数を加えることも出来る。 z$x2 &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) z$y2 &lt;- z$x + z$y z ## x y x2 y2 ## 1 1 4 A 5 ## 2 2 5 B 7 ## 3 3 6 C 9 1.3.2 tibble（＊） データフレーム以外に，tibbleというデータ形式もある。 dat = tibble::tibble(x = c(1, 2, 3), c(4, 5, 6)) dat ## # A tibble: 3 x 2 ## x `c(4, 5, 6)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 z_tibble = as_tibble(z) #既に作ったデータフレームをtibble形式に変えることもできる。 z_tibble ## # A tibble: 3 x 4 ## x y x2 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 4 A 5 ## 2 2 5 B 7 ## 3 3 6 C 9 tibbleにはデータフレームとは異なる，以下のメリットがある。 コンソールにはデータ全てではなく，最初の10行程度が表示される。行数が多すぎるデータでも，見にくくならない。 変数の型の種類も表示される。 この授業でも基本的には，tibble形式を使う。ただし，データフレーム形式も覚えておくこと（tibbleはtidyverseパッケージを読み込んでいないと使えない。データフレームはデフォルトで使える）。 1.3.3 外部データの読み込み 実際には上述のようなR上でデータを作ることはなく，多くの場合は外部にあるファイルを読み込むこととなる。 データを用意するときの注意点！ データファイルはCSV形式にすること。CSVとは，値をカンマで区切った形式のテキストファイルをいう。 データファイル内では，漢字や仮名など日本語を入れない（全角を入れない）。どうしても入れなければならない場合は，文字コードを「UTF-8」にする。 サンプルデータを読み込んでみよう。read_csv()関数で読み込むことが出来る。 sample_data = read_csv(&quot;1_sample.csv&quot;) sample_data 1.3.4 データの出力 write_excel_csv()を使う。以下のように，保存したいデータフレームの名前，出力後のファイルの名前を入力する。 write_excel_csv(sample_data, &quot;1_sample_2.csv&quot;) 1.4 外部パッケージの利用 パッケージとは，Rの機能を拡張するためにインターネットからインストールして使うものである。 この授業でも外部パッケージをインストールして使う場合がある。具体的には今後，ggplot2, lme4, lmerTestパッケージを使う予定。 先にも説明したように，帝京のマシンでは毎回ログインするたびにインストールし直す必要があるので注意（自宅のPCならば，一度インストールすれば，もう毎回インストールし直す必要ない）。 1.4.1 インストールされているパッケージの確認 library()で，現在自分のマシンのRにインストールされているパッケージの一覧が出る。 library() #標準でインストールされているライブラリもいくつかある。 1.5 Rを終わらせる そのまま閉じてよい。コンソールに q() と打ち込んでも良い。 q() 「Save workspace image?（作業スペースを保存しますか？）」が表示されるが，これは「いいえ」にする。 練習問題 問１ Rを使って以下のa, bを計算し、aとbの式どちらの方が答えが大きいかを確認せよ。 a: 1 × 1 × 2 × 2 × 3 × 4 × 5 × 9 × 8 × 7 × 6 b: 9 × 8 × 7 × 6 × 1 × 7 × 5 × 1 × 3 × 2 × 1 問２ Rを使って、3の10乗の値を求めよ。 ヒント：aのx乗は^を使って求める（a^x） 問３ サンプルデータ「1_sample.csv」をデータフレームとして読み込み，データフレーム上の変数X, Yを用いて，Z = X - 3Y の新しい変数Zをデータフレームに追加せよ。 "],
["section-2.html", "Chapter 2 平均値，標準偏差など 2.1 尺度水準 2.2 母集団と標本 2.3 代表値 2.4 データの散らばり 2.5 データの要約 練習問題", " Chapter 2 平均値，標準偏差など これまで学んだ統計の基礎について復習する。以下について復習する。 尺度水準 母集団と標本 代表値 データの散らばり 2.1 尺度水準 変数は以下のように分類される。 2.1.1 質的変数 分類や種類を意味するデータ。数量化して計算することはできない。 名義尺度 性別（男，女），血液型，出身地など。順序関係がない（男性&lt;女性といった関係はないように）。 順序尺度 「優，良，可，不可」といった成績，「1. 賛成，2. どちらでもない，3. 反対」といった尺度など。順序関係を持つが，間隔は定義されない。 2.1.2 量的変数 分類や種類を意味するデータ。計算することができる。 間隔尺度 データの間隔に意味があるもの。ゼロが何もない状態を意味するものでないもの。セ氏温度など（0℃以下も-1℃があるように，ゼロは何もない状態を意味しない）。 差には意味があるが，比率については意味を持たない。例えば，「10℃と20℃の差は10℃である」とはいえるが，「20℃は10℃の2倍の熱さである」とは言えない。 比率尺度 データの間隔に意味があるもの。ゼロがなにもない状態を意味するもの。身長，体重，絶対温度など。 間隔を比率で表現できる。例えば，「体重100キロの人は体重50キロの人より2倍重い」といえる。 2.1.3 離散変数と連続変数 また，離散変数と連続変数という区別もある。 離散変数とは，小数の間隔を持たない変数。例えば，個数。1個, 2個，3個と数えるが，1.1個, 1.2個などは存在しない。 連続変数とは，小数の間隔を持つ変数。例えば，身長。150cmから151cmの間には小数で表現できる範囲が存在する。 2.1.4 量的変数と質的変数の区別 特に重要な分類は，変数が「質的変数」か「量的変数」かである。これによって適用すべき解析の方法が定まるので，変数が質的か量的かについては常に意識すること。 量的変数はデータの間隔があるので，平均値や分散を算出することが出来る。 一方，質的変数は平均値や分散を算出することが出来ない。ゆえに，データの要約の際には質的変数の場合は頻度や割合を算出することとなる（男性が何人か，何%かなど）。 2.2 母集団と標本 日本人の性質を調べる目的の研究をするにしても，日本人全員を研究対象とするわけにはいかない。日本人の中から何名かを選び出して研究をするというのが普通である。 このように分析の対象となる全体の集団を母集団（population），母集団から選びだしたものを標本（sample）と呼ぶ。 以下のRプログラムを実行して，母集団と標本の関係を理解しよう。まず，乱数（ランダムの数字）を10,000個作り，populationという変数に入れる。次に，sample()を使って，populationからランダムに10個選び出し，sampleという変数に入れる。sample()を何回も繰り返して，出力される標本（sample）の中身が毎回異なることを確認する。 population = rnorm(10000) population sample = sample(population, 10) sample 2.3 代表値 代表値とはデータの分布を捉えるための統計量の総称をいう。平均値，中央値などがある。 以下のサンプルデータを使って，Rを使って代表値を求めてみよう。 X = c(1, 3, 2, 6, 5, 7, 3, 2, 8, 4) 2.3.1 平均値 \\(n\\)個の数値\\(X_{1}, X_{2}, \\cdots, X_{n}\\)からなるデータの平均値\\(\\bar{X}\\)は，以下の数式で求める。 \\[ \\bar{X} = \\frac{X_{1} + X_{2} + \\cdots + X_{n} }{n}\\\\ \\] 簡単に表現すると以下にようになる。 \\[ \\bar{X} = \\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\\\ \\] R で平均を求めるには，mean()を使う。 mean(X) ## [1] 4.1 再び，先程の母集団と標本のシミュレーションに戻り，母集団の平均と標本の平均を比較しよう。 母集団の平均は，母平均（population mean）という。標本の平均は標本平均（sample mean）という。 当たり前だが，母平均（\\(\\mu\\)）は一定の値を取る（\\(\\mu\\)は「ミュー」と読む）。 ただし，標本平均（\\(\\bar{X}\\)）は標本を取るたびに異なる値が出る。母平均と近い値が出ることもあれば，離れた値が出ることもある。 population = rnorm(10000) mean(population) sample = sample(population, 10) mean(sample) 2.3.2 中央値 データを小さい順から並べた場合，つまり，\\(X_{1} \\le X_{2} \\le ... \\le X_{n}\\)と並べた場合に，中央に位置する値を中央値という。 データの個数を\\(n\\)とした場合，\\(n\\)が奇数の場合は\\(X_{(n+1)/2}\\)，\\(n\\)が偶数の場合は\\((X_{n/2}+X_{n/2+1})/2\\)が中央値となる。， Rでは，median()を使えば求められる。 median(X) ## [1] 3.5 X = c(1, 3, 2, 6, 5, 7, 3, 2, 8, 4) これを小さい順に並べ替えると， X = c(1, 2, 2, 3, 3, 4, 5, 6, 7, 8) となる。端っこの数値を取っていくと残るのは，このデータ数は偶数なので，(3, 4)が残る。すなわち，このデータの場合は，これら2つの値の平均が中央値ということになる。 2.3.3 平均値と中央値 平均値と中央値のどちらを代表値として使えばよいのだろうか？それは，データの分布に依存してくる。データが正規分布（釣鐘型）ならば，平均値がデータの代表的な値を意味する。しかし，極端な値が多い分布の場合ならば中央値のほうが全体の代表を表す数値となる（詳しくは練習問題を参照）。 2.4 データの散らばり 2.4.1 分散と標準偏差 データを要約する際には，代表値だけではなく，データの散らばりを意味する値を報告する必要がある。分散や標準偏差は，データの散らばりを意味する値である。 ここでは，分散は\\(\\sigma^2\\)，標準偏差は\\(\\sigma\\)と表現することにする。 ※\\(\\sigma\\)は，「シグマ」と読みます。 分散\\(\\sigma^2\\)は以下の式で計算される。\\(\\mu\\)は母集団の平均値とする。 標準偏差\\(\\sigma\\)は，分散\\(\\sigma^2\\)の平方根である。 \\[ \\sigma^2 = \\frac{1}{n-1}\\sum_{k=1}^{n}(X_{k}-\\mu)^2\\\\ \\] Rでは，var()とsd()でそれぞれ，分散と標準偏差を求めることが出来る。 var(X) ## [1] 5.433333 sd(X) ## [1] 2.330951 2.4.2 その他の散らばり具合を意味するもの（＊） データの散らばり具合を意味するものとしては，他にも四分位数（quantile）がある。四分位数とは，データを小さい順に並べて四分割したときの区切りとなる数値をいう。最小値，25%分位点，50％分位点（中央値），75%分位点，最大値に分割する。 Rならば，quantile()で求めることが出来る。 quantile(X) ## 0% 25% 50% 75% 100% ## 1.00 2.25 3.50 5.75 8.00 2.5 データの要約 Rのsummary()を用いると，主要な統計量が出力される。平均値と四分位数が表示される。summary()でデータの全体像を知る習慣をつけておこう。 summary(X) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.25 3.50 4.10 5.75 8.00 練習問題 宿題にはしない。各自確認しておくことをすすめる。 サンプルデータ「2_sample_1.csv」を使って，以下の分析をせよ。このデータには，ある学校の生徒100人の身長が記録されている（架空のデータである）。 ヒストグラムで身長(height)の分布を確認せよ（ヒント：hist()で作れる。hist(データフレーム名$変数)で出力される）。 身長の平均値，中央値，標準偏差，四分位数を求めよ。 サンプルデータ「2_sample_2.csv」を使って，以下の分析をせよ。このデータには，ある大学の卒業生100人の年収が記録されている（架空のデータである）。 ヒストグラムで年収(income)の分布を確認せよ。 年収の平均値，中央値，標準偏差，四分位数を求めよ。 練習問題1と2で分析したデータそれぞれについて，平均と中央値のずれについて確認し，データの代表値として平均値と中央値のどれが適切か（あるいはどれでも良いか）を理由とともに述べよ。 "],
["section-3.html", "Chapter 3 データの可視化 3.1 相関 3.2 データの可視化（＊） 練習問題 参考文献", " Chapter 3 データの可視化 相関の復習とデータの可視化を学ぶ。 相関係数 グラフの種類 グラフの作り方 必要なパッケージのインストール及びロードをする。 install.packages(&quot;tidyverse&quot;, &quot;MASS&quot;) #tidyverseはデータの操作に便利な様々なパッケージを含んでいる（ggplot2, dplyrなど複数のパッケージのセット） library(tidyverse, MASS) Rにはサンプルデータが予めいくつか入っている。グラフ作成の練習として，その中からいくつかのデータを使う。head()を使うと，データの最初の数行だけが表示される。 head(iris) irisは，フィッシャーの有名なアヤメのデータ。それぞれ50個体の萼（がく）片と花弁の長さと幅が入っている。 他にもたくさんある。library(help = &quot;datasets&quot;)で確認できる。 3.1 相関 フィッシャーのアヤメのデータを使って，相関の復習を行う。がく片と花弁の長さの関係を見てみる。 まずは散布図を作る。とりあえず何も考えずに以下のプログラムを実行してみよう。 #「ggplot2::」はなくてもOK。これは，「ggplot2パッケージに含まれているqplot()関数を使います」という意味。 ggplot2::qplot(data = iris, Sepal.Length, Petal.Length) ggplot2::qplot(data = iris, Petal.Length, Sepal.Width) 3.1.1 ピアソンの積率相関係数 変数\\(x\\)と変数\\(y\\)の積率相関係数\\(r\\)は，以下の式で求められる。 \\[ r = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}} \\] \\(\\sigma_{xy}\\)は\\(x\\)と\\(y\\)の共分散で，\\(\\sigma_{xy} = \\sum^n_{i=1}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\)である（\\(\\bar{x}\\)は\\(x\\)の標本平均）。\\(\\sigma_{x}\\)と\\(\\sigma_{y}\\)は\\(x\\)と\\(y\\)それぞれの標本標準偏差である。 変数\\(X\\)と変数\\(Y\\)の相関係数\\(r\\)が \\(r &gt; 0\\)のとき，「変数\\(X\\)と変数\\(Y\\)の間に正の相関関係がある」という。\\(r &lt; 0\\)のときは，「変数\\(X\\)と変数\\(Y\\)の間に負の相関関係がある」という。正の相関関係は，一方の変数の量が増えればもう一方の変数も増えるという関係のことを言う。負の相関関係は，一方が増えればもう一方が減るという関係のことを言う。 試しに，様々な相関係数の場合の散布図を見てみよう。 r = 0.5 #rの値を変えてプロットを確認してみよう x = rnorm(1000, mean=0, sd=1) #平均0，標準偏差1の正規分布から1,000個乱数を作る。 y = r*x + sqrt(1-r^2)*rnorm(1000, mean=0, sd=1) cor.test(x, y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 20.361, df = 998, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4964219 0.5841188 ## sample estimates: ## cor ## 0.541743 qplot(x, y) + ggtitle(r) 相関係数の絶対値の大きさが，その相関関係の強さを意味する。一般的に， \\(.20 &lt; |r| &lt; .40\\) くらいを「 弱い相関」，\\(.40 &lt; |r| &lt; .70\\)くらいを「強い相関」， \\(|r| &gt; .70\\) くらいを「かなり強い相関」と表現する。 しかし，分野にもよるし，相関係数の強弱の判断はいわば「研究者の主観」である（社会調査においては，\\(|r| = .10\\)でも意味のある相関として扱う場合もある）。また，相関係数が有意かどうかにもよる。 Rでは，相関係数を求めるための関数cor.test()がある。相関を求めたい二つの変数を入れれば，その相関係数と相関係数の有意性検定（母集団の相関係数がゼロから離れているか）の結果が出力される。 cor.test(iris$Sepal.Length, iris$Petal.Length) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Petal.Length ## t = 21.646, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8270363 0.9055080 ## sample estimates: ## cor ## 0.8717538 cor.test(iris$Petal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Petal.Length and iris$Sepal.Width ## t = -5.7684, df = 148, p-value = 4.513e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.5508771 -0.2879499 ## sample estimates: ## cor ## -0.4284401 3.1.2 スピアマンの順位相関係数 Rでは，cor.test()でスピアマンの順位相関係数も求めることが出来る。オプションでmethod = &quot;spearman&quot;と入れれば良い（methodに何も指定しないとデフォルトでピアソンの積率相関係数が算出される）。 cor.test(iris$Sepal.Length, iris$Petal.Length, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: iris$Sepal.Length and iris$Petal.Length ## S = 66429, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8818981 cor.test(iris$Petal.Length, iris$Sepal.Width, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: iris$Petal.Length and iris$Sepal.Width ## S = 736637, p-value = 0.0001154 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.3096351 3.1.3 外れ値の影響 データに極端な値がある場合，ピアソンの相関係数では結果が歪む恐れがある。 library(MASS) #MASSパッケージにあるサンプルデータAnimalsを使う head(Animals) #28種の動物の体重と脳重量のデータ ## body brain ## Mountain beaver 1.35 8.1 ## Cow 465.00 423.0 ## Grey wolf 36.33 119.5 ## Goat 27.66 115.0 ## Guinea pig 1.04 5.5 ## Dipliodocus 11700.00 50.0 qplot(data=Animals, body, brain) cor.test(Animals$body, Animals$brain) #Pearsonの相関係数を求める ## ## Pearson&#39;s product-moment correlation ## ## data: Animals$body and Animals$brain ## t = -0.027235, df = 26, p-value = 0.9785 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3776655 0.3684700 ## sample estimates: ## cor ## -0.005341163 極端に体重が大きくかつ脳重量の小さい種が影響している。 相関係数を鵜呑みにするのではなく，必ず散布図を確認する。 対処法としては， 外れ値を除去して相関を見る。 スピアマンの順位相関係数を見る。順位相関はデータを順位（1位,2位,3位,…）に変換して相関係数を出したものであり，これにより外れ値の影響が調整される。 Animal_2 = dplyr::filter(Animals, body &lt; 10000) qplot(data=Animal_2, body, brain) cor.test(Animal_2$body, Animal_2$brain) #Pearsonの相関係数を求める ## ## Pearson&#39;s product-moment correlation ## ## data: Animal_2$body and Animal_2$brain ## t = 2.8764, df = 24, p-value = 0.008308 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1479923 0.7471395 ## sample estimates: ## cor ## 0.5063195 cor.test(Animals$body, Animals$brain, method=&quot;spearman&quot;) #外れ値除去前のデータでSpearmanの相関係数を求める ## ## Spearman&#39;s rank correlation rho ## ## data: Animals$body and Animals$brain ## S = 1036.6, p-value = 1.813e-05 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7162994 3.1.4 相関関係と因果関係 因果関係とは，原因と結果の関係である（X -&gt; Y：XがYの原因である）。相関関係はあくまで，一方の変数の増減ともう一方の変数の増減に関係があることである（X &lt;-&gt; Y）。 XとYとの間の相関係数を求めて有意な相関があったとしても，両者の間に因果関係があるとは結論付けることはできない。 因果関係がないのに相関関係があるように見える例（中室・津川(2017)より）。 アイスクリームの売上が高いと水死者が増える（第3の要因：「暑さ」という共通の要因）。 警察官が多い街ほど犯罪が多い（逆の因果：犯罪が多いところにたくさん警察官を配置している）。 大気中の二酸化炭素量が増えるほど海賊が増える（偶然）。 3.2 データの可視化（＊） 平均値や相関係数を計算する前に，グラフを作ってデータの分布を確かめる習慣をつけよう（データの可視化）。 以下では，Rのggplot2パッケージを使って代表的なグラフを作成してみた。ggplot2を使ったグラフの作成方法については覚える必要はない。あくまで，データを可視化する手段として，どのようなグラフがよく使われるかを例として挙げる。 3.2.1 散布図（＊） p &lt;- ggplot(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() p 3.2.2 ヒストグラム（＊） p &lt;- ggplot(data = cars, aes(x=dist)) + geom_histogram() p 3.2.3 箱ひげ図（＊） 最小値，第一分位点，中央値，第三分位点，最大値を示す（外れ値は点で示される）。 p &lt;- ggplot(data = InsectSprays, aes(x=spray, y=count)) + geom_boxplot() p 3.2.4 棒グラフ（＊） #サンプルデータをつくる: あやめの種ごとのがくの幅の平均 iris_mean &lt;- data.frame( Sepecies = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;), Width = c(3.43, 2.77, 2.97) ) p &lt;- ggplot(data = iris_mean, aes(x=Sepecies, y=Width)) + geom_bar(stat = &quot;identity&quot;) p 3.2.5 折れ線グラフ（＊） #サンプルデータをつくる: 10日間の気温の変化 temperature &lt;- data.frame( Days = 1:10, Celsius = c(17.2, 17.5, 18.1, 18.8, 19.0, 19.2, 19.7, 20.2, 20.5, 20.1) ) p &lt;- ggplot(data = temperature, aes(x=Days, y=Celsius)) + geom_line() p 3.2.6 応用（＊） p &lt;- ggplot(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point(aes(shape = Species)) p &lt;- p + xlab(&quot;Length of Sepal&quot;) + ylab(&quot;Length of Petal&quot;) p p &lt;- ggplot(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() p &lt;- p + facet_wrap(~Species) p p &lt;- ggplot(data = ChickWeight, aes(x=weight)) + geom_histogram() + facet_wrap(~Diet) p 3.2.7 qplot関数を使った簡単なグラフの作り方 ggolot2パッケージのqplot()でも手っ取り早くグラフを作成することが出来る。とりあえずデータの傾向を見たいだけなら，qplot()を使うのが良い。x軸もしくはy軸に指定したい変数を入れればできるので，これくらいは覚えておくと良い。 x軸とy軸に表示する変数を2つ入れれば散布図，x軸の変数1つだけならヒストグラムがデフォルトで表示される。geomでグラフの種類を指定することも可能。 qplot(data = iris, x = Sepal.Length, y = Petal.Length) qplot(data = ChickWeight, x = weight) qplot(data = InsectSprays, x = spray, y = count, geom=&quot;boxplot&quot;) gglot2パッケージがなくても，Rで標準で入っているplot()でもグラフを作成することができる（ただし，見た目は良くない）。 plot(iris$Sepal.Length, iris$Petal.Length) hist(ChickWeight$weight) boxplot(InsectSprays$count~InsectSprays$spray) 今後もわからないことが出てきたら，関数のヘルプを参照すること（英語だが）。あるいはインターネットで検索する。 #関数の前に?を入れて実行すると，ヘルプが表示される。あるいはhelp()で囲む ?qplot help(qplot) 練習問題 問１ Rにサンプルデータとして入っているChickWeightを使ってグラフを作ってみよう。 head(ChickWeight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 ChickWeightには，ひよこを対象とした実験データが入っている。食事を変えると体重が変化するかどうかを調べた。まず，ChickWeightと入力して，データの中身を確認する。 日数（Time）と体重（weight）の間の関係を散布図で確認する。 体重（weight）の分布をヒストグラムで確認する。 食事の種類（Diet）ごとに体重（weight）の分布を箱ひげ図を作って確認する。 問２ ある学校の生徒を対象に行った調査の結果，YouTubeの視聴時間と学校での成績の間の相関係数を測ったところ，\\(r = - .56\\)であった。この調査の結果から，「YouTubeを見るほど，学力が下がる」という結論を出せるか？別の解釈が可能だと思うのならば，その解釈を述べよ。また，「YouTubeを見るほど，学力が下がる」ということを調べるためにはどうすれば良いと思うかを述べよ。 参考文献 中室牧子・津川友介 (2017). 「原因と結果」の経済学ーデータから真実を見抜く思考 ダイヤモンド社 "],
["section-4.html", "Chapter 4 確率分布 4.1 確率変数と確率分布 4.2 二項分布 4.3 正規分布 4.4 その他の確率分布（＊） 練習問題", " Chapter 4 確率分布 確率分布とは何かについて学ぶ。 確率変数と確率分布 二項分布 正規分布 その他の確率分布 今回の演習でも，tidyverseパッケージを使うので，予めロードしておく。また，set.seed(1234)というプログラムを実行しておく。 set.seed()はいわゆる「乱数の種」というものを設定するための関数。乱数の種を同じ値に設定しておくと，乱数を再現することが出来る（このテキストに書いてあるのと同じ結果が再現できる）。 install.packages(&quot;tidyverse&quot;) library(tidyverse) set.seed(1234) 4.1 確率変数と確率分布 サイコロを1個投げるとする。それぞれの目が出る確率は1/6である。それぞれの目をX（1, 2, 3, 4, 5, 6），それぞれの目が出る確率をP(X)とする。 XとP(X)を以下の表1で示す。 表1 このとき，Xを確率変数と呼ぶ。その値と対応する確率が存在する変数のことをいう。表1のように，確率変数とその変数が取り得る確率の分布を確率分布という。 確率分布には様々な種類がある。今回は特に，その中でも二項分布と正規分布について扱う。 ※なお，上のサイコロの目の例のように，どの確率変数Xについても常に一定値の確率を取る確率分布は一様分布(uniform distribution)と呼ばれる。 4.2 二項分布 4.2.1 二項分布の基本 コインをn回投げる。表が出る確率を\\(q\\)とすると, 裏が出る確率は\\((1-q)\\)となる。n回中，表がx回出る確率P(x)は，理論的には以下の式で算出される。 \\[ P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)} \\] xを確率変数とした場合，上記の式の確率に従う確率分布を二項分布という。 つまり二項分布は，生じる事象が2つのカテゴリに分けられる場合に当てはまる確率分布である。コインを投げたときに出る面が「表か裏」か，学生の中から選んだ人の性別が「男か女」か，ある意見について「賛成か反対」かなど。このような事象が生じる確率は，理論的には二項分布に従う。 コインを投げる場合の例に戻る。例えば，表が出る確率qを0.5として，10回投げたときに表が6回出る確率を計算してみよう。Rならば，dbinom()関数を使えば簡単に計算できる。 #xは確率変数（コインの例での），sizeは試行回数（コインの例のnに相当）， dbinom(x=6, size=10, prob=0.5) ## [1] 0.2050781 #上の式n, x, pに実際に値を入れて計算する場合。dbinom()関数を使った場合と結果が一致することを確認しよう。 choose(10, 6) * 0.5^6 * (1 - 0.5) ^4 ## [1] 0.2050781 二項分布は以下のように表現されることもある。 \\[ x \\sim Binomial(n, q) \\] Binomialのカッコ内のn, qのように，確率分布を構成する変数のことをパラメータと呼ぶ。 Binomialは二項分布，\\(\\sim\\)は「従う」という意味である。つまりこの式は，「xは，nとqをパラメータとする二項分布に従う」ということをいっている。 4.2.2 二項分布の期待値と分散 表が出る回数xが0〜10回の場合全てについて，それぞれが生じる確率を計算すると以下のようになる。 dbinom(x=0:10, size=10, prob=0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 グラフにすると以下のようになる。横軸をx, 縦軸をP(x)とする。 グラフからもわかるように，表が出る確率が0.5のコインを10回投げたときに，最も出やすいのは10回中5回であることがわかる。10回中1回，10回中10回出るケースはほとんどまれであることがわかる。 この図からも，確率分布には最も出やすい変数（平均値。確率分布の場合は期待値と呼ぶ）と分散が存在することがわかる。 二項分布の期待値\\(E(x)\\)と分散\\(Var(x)\\)は，以下の式から計算できる。 \\[ E(x) = nq\\\\ Var(x) = nq(1-q) \\] 表が出る確率が0.5（i.e., q=0.5）のコインを10回(i.e., n=10)投げた場合における，表が出る回数xの期待値と分散を計算してみよう。 #E(x) = nq 10*0.5 ## [1] 5 #Var(x) = nq(1-q) 10*0.5*(1-0.5) ## [1] 2.5 n=1のときは，ベルヌーイ分布と呼ばれる（例えばコインを1回だけ投げる場合）。 4.3 正規分布 4.3.1 正規分布の基礎 統計学で用いられる確率分布の中でも有名なのは正規分布である。正規分布は，平均\\(\\mu\\)，標準偏差\\(\\sigma\\)をパラメータとする確率分布で，釣鐘型（ベル・カーブ）状の分布を描く。 平均\\(\\mu\\)，標準偏差\\(\\sigma\\)とする正規分布の確率密度関数f(x)は，以下の式から計算される。 \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\] この式自体を覚える必要はない。 これは確率分布ではなく，確率密度関数と呼ばれるものである。確率分布は縦軸が横軸の値それぞれが生じる確率を意味しているが，確率密度関数の縦軸は確率そのものを意味しない。確率密度関数の面積が確率を意味する。すなわち，確率密度関数の面積全てを合計した値は，1となる。 または，以下のように表現することもある。 \\[ x \\sim Normal(\\mu, \\sigma) \\] 平均\\(\\mu\\)を0，標準偏差\\(\\sigma\\)を1とする正規分布について考えてみる。 Rならば，dnorm関数で確率密度関数を出力することができる。 x = seq(-3, 3, 0.05) y = dnorm(x=x, mean=0, sd=1) y これをグラフにすると以下のようになる。 x=0からx=1の範囲が生じる確率は，その範囲に対応するグラフの面積となる。 #Rならば，pnorm関数でxが-∞からqまでの範囲の確率を求めることができる #以下は，平均0，標準偏差1の正規分布で 0までの確率を求めている。正規分布の半分に相当するので，0.5である。 pnorm(q=0, mean=0, sd=1) ## [1] 0.5 #特定の範囲を求めたい場合は以下のように使えば良い。例えば以下は，xが1から2の範囲の確率である。 x = pnorm(q=2, mean=0, sd=1) - pnorm(q=1, mean=0, sd=1) x ## [1] 0.1359051 以下が平均\\(\\mu\\)と標準偏差\\(\\sigma\\)の値をそれぞれ変えた場合の正規分布である。 正規分布は平均\\(\\mu\\)を対象として左右対称のかたちを描く。 統計学では様々な変数について，「変数が正規分布に従う」という仮定を置くことが多い。体重や身長なども経験的に正規分布を描く。正規分布は以降でも述べるように確率変数xが連続量の場合の確率分布であるが，離散値のデータについても正規分布を仮定して分析することもよくある（テストの点数，質問紙の回答得点など）。 しかし，世の中には正規分布に従わない変数もある(例えば年収などは釣鐘型の分布にならない)。データ分析の前に事前に実際のデータの分布を見て，左右対称でないなど明らかに正規分布を仮定できない変数については分析のやりかたを考える必要がある。 平均0, 標準偏差1の正規分布は標準正規分布と呼ばれる。 二項分布\\(Binomial(n, p)\\)の\\(n\\)が十分に大きい場合，二項分布は正規分布\\(Normal(np, \\sqrt{np(1-p)})\\)に近似する。例えば，以下にn=50のときの二項分布を示す。 #n=50のとき plot &lt;- data.frame(x=0:50, p=dbinom(x=0:50, size=50, prob=0.5)) p &lt;- ggplot(data=plot, aes(x=factor(x), y=p)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;P(x)&quot;) + xlab(&quot;x&quot;) p 4.3.2 中心極限定理（＊） 「母集団の確率分布が平均\\(\\mu\\)，標準偏差\\(\\sigma\\)を持つ確率分布に従う\\(X\\)について，その平均値\\(\\bar{X_{n}}\\)はnが大きくなれば，平均\\(\\mu\\)，標準偏差\\(\\sigma\\)の正規分布に収束する」というのが中心極限定理である。つまり，元の分布がどのような分布でも，そこから平均値を計算することを何回も繰り返すとその分布は正規分布のかたちになるということを意味する。 シミュレーションで確認する。6面のサイコロを100回振る実験を行うとする。 X &lt;- round(runif(100,min=1,max=6),0) mean(X) ## [1] 3.24 それぞれの目が出る確率は1/6で一定である。すなわち，サイコロが出る目は一様分布に従う（正規分布ではない）。一様分布の平均値は，最大値をa, 最小値をbとすると，(a+b)/2。すなわち，サイコロの例の場合の平均値は理論的には(1+6)/2=3.5となる。 サイコロを100回振って平均値を求める。この平均値を求めるのを，1,000回繰り返し行う。求めた平均値1,000個の分布を見てみると。。 sample.means &lt;- sapply(c(1:1000), function(x) {mean(round(runif(100,min=1,max=6),0))} ) qplot(sample.means) 正規分布に近似する。1,000回よりももっと回数を増やすと，より正規分布っぽいかたちになる。 このように，元の母集団の分布がたとえ正規分布でなくても，その標本平均は正規分布に近似する。 #サイコロを7回振ってその合計を求める。これを10,000回行ったときの出目の合計値の分布 Y &lt;- round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) +round(runif(10000,min=1,max=6),0) qplot(Y) 中心極限定理が成り立つため，たとえ元の変数が正規分布に従ってなくても，平均化したものを使えば正規分布を前提とした統計的仮説検定を行なっても大きな問題はない。 複数の質問項目（順序尺度）をまとめて平均化した心理尺度を分析に使う。 選挙への投票（「した」もしくは「しなかった」の二値）者の割合を県ごとに算出して，県を単位として分析に使う。 4.4 その他の確率分布（＊） 二項分布のように，確率変数xが離散変数の場合の確率分布は，離散確率分布という。 正規分布のように，確率変数xが連続変数の場合は，連続確率分布という。 他にも確率分布は以下のようなものがある。 4.4.1 離散確率分布 ポアソン分布 \\[ P(x) = \\frac{\\lambda^x\\exp(-\\lambda)}{x!}\\\\ x \\sim Poisson(\\lambda)\\\\ \\] xは0以上の整数（0, 1, 2, 3, …）とする。 ポアソン分布のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，平均と分散が等しい分布である。 以下に，パラメータ\\(\\lambda\\)をそれぞれ変えた場合のポアソン分布を示す。 一定の期間中にランダムで生じる事象はポアソン分布に従う。具体的な例としては，1日の間に届くメールの件数，営業時間中に来る客の数など。 二項分布\\(Binomial(n, p)\\)の\\(n\\)が十分大きく，かつ\\(p\\)が小さい場合は平均を\\(np\\)とするポアソン分布に近似する。つまり，めったに起こらない事象はポアソン分布に従う。例えば，1年間の間に生じる交通事故の件数など（365日それぞれで0.1%で生じるとした場合）。歴史的に有名な例は，「ドイツ軍で1年間で馬に蹴られて死亡した兵士の数」がポアソン分布に従うというもの。 4.4.2 連続確率分布 t分布 \\[ P(x) = \\frac{\\Gamma((v+1)/2)}{\\Gamma(v/2)\\sqrt{\\pi v}\\sigma}\\left(1+\\frac{1}{v}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)^{-(v+1)/2}\\\\ x \\sim Student\\_t(v, \\mu, \\sigma)\\\\ \\] パラメータは3つ。vは自由度と呼ばれる。 自由度の小さいt分布は分布の裾が長くなる。すなわち，外れ値を含んだ分布となる。 自由度\\(\\infty\\)のt分布は正規分布と一致する。 連続確率分布には他にも対数正規分布，指数分布などがあるが，とりあえずは正規分布を知っておけば良い。 練習問題 問１ あなたは野球部の監督で，自分のチームの勝率はこれまでの練習の経験から32%だとわかっている。 これから遠征で，全部で10試合を行う予定である。 勝つ試合の回数を確率変数nとし，nとそれぞれのnに対応する確率を表で示せ。 平均して何試合勝つことができるかを述べよ。 問２ ある学校で小学6年生の身長を測ったところ，平均は150.2 cmで標準偏差が3.5 cmであった。 身長152 cmから155 cmの児童の割合はいくらか。 身長158 cmを超える児童は何割いるか。 ヒント：pnorm関数を使おう。なお，全ての範囲の確率の合計は1である。 "],
["section-5.html", "Chapter 5 統計的帰無仮説検定 5.1 統計的帰無仮説検定の考え方 5.2 第1種の過誤と第2種の過誤 5.3 統計的帰無検定の問題 練習問題", " Chapter 5 統計的帰無仮説検定 統計的帰無仮説検定の考え方について理解する。 * 統計的帰無仮説検定の考え方（p値とは何か？） * 第1種の過誤と第2種の過誤 * 多重検定 今回もtidyverseパッケージを使う。予めインストールの上，ロードをしておく。 install.packages(&quot;tidyverse&quot;) library(tidyverse) 5.1 統計的帰無仮説検定の考え方 以降では，前回学んだ二項分布を用いた統計的帰無仮説検定（二項検定）を例としながら，統計的帰無仮説検定の考え方について理解していく。 5.1.1 二項分布の復習 コインを10回投げて表が出た回数Xをカウントしていく。”理論的”には，Xは，コインを投げる回数nと表が出る確率qをパラメータとする二項分布に従う。 \\[ \\begin{equation} P(x) = {}_n\\mathrm{C}_xq^{x}(1-q)^{(n-x)}\\\\ x \\sim Binomial(n, q) \\end{equation} \\] plot &lt;- data.frame(x=0:10, p=dbinom(x=0:10, size=10, prob=0.5)) ggplot(data=plot, aes(x=factor(x), y=p)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;P(x)&quot;) + xlab(&quot;x&quot;) 5.1.2 統計的帰無仮説検定 ”理論的には”，表が出る回数Xは上の図のようになる（平均は\\(nq = 10*0.5\\) = 5回）。 では，実際にコインを10回投げて表が出る回数をカウントしみたところ，表が2回しか出なかった。 この実験結果から，「このコインには歪みがあって，片一方の面だけが出やすい」と言ってもよいのか？ これを検討するために，表と裏それぞれが出る確率が等しいコインを投げる場合（すなわち，\\(q=0.5\\)）との比較を行い，今回の実験結果がどれくらいまれな事象と言えるのかを比較する。 このとき，研究者が検証したい仮説を対立仮説（alternative hypothesis），対立仮説を検証するために比較の対象とする「偏りを仮定しない」仮説のことを帰無仮説(null hypothesis)という。 では，今回の帰無仮説となる\\(Binomial(n=10, q=0.5)\\)の分布を見てみよう。理論的には，表がx回出る確率pは，それぞれ以下のようになる。 d = data.frame(x=0:10, q=dbinom(x=0:10, size=10, prob=0.5)) d ## x q ## 1 0 0.0009765625 ## 2 1 0.0097656250 ## 3 2 0.0439453125 ## 4 3 0.1171875000 ## 5 4 0.2050781250 ## 6 5 0.2460937500 ## 7 6 0.2050781250 ## 8 7 0.1171875000 ## 9 8 0.0439453125 ## 10 9 0.0097656250 ## 11 10 0.0009765625 表もしくは裏が出る回数が2回以下の場合の確率を計算すると， round(d$q[1] + d$q[2] + d$q[3] + d$q[9] + d$q[10] + d$q[11], 2) ## [1] 0.11 となる。このことから，「フェアなコインを投げたら片一方の面だけが出る回数が2回以下の確率は 0.11 だ。こんなことはめったに起こらないから，このコインは歪んでいて片面だけが出やすい」と結論づけてよいのだろうか？ 「帰無仮説の前提のもとで，今回の結果が得られる確率」のことをp値（ピーち）と呼ぶ。 つまるところ，上述の問題は「\\(p = 0.11\\) は非常に小さい確率と評価してもよいのか」という問題である。「p値を小さい，もしくは大きい」と評価する基準となる確率を有意水準という。 一般的に有意水準には0.05（5%）が使われる。なお，5％を基準とすることについては特に明確な根拠はない（研究の世界で合意されているからという以上の理由はない）。 帰無仮説（フェアなコインを投げる）の前提のもとでは，表が出る回数が2回以下の確率は 0.11 であり，5％より大きかった。すなわち，このコインはゆがんでいると結論付ける訳にはいかない，ということになる。 ちなみに，今回のように「コインが表か裏かに関わらず，一方の面だけが出やすい」という対立仮説を検討する場合の検定は，両側検定という。仮に，今回の仮説で表と裏を区別するとして「表が出にくい」つまり「表が出る回数が2回以下の確率」を対象とする場合，このような検定を片側検定という。二項分布は左右対称の分布なので，両側p値は片側p値の2倍の値である(厳密には左右対称ではないのであくまで近似値)。多くの場合，両側検定を使うのが一般的。 Rならば，二項検定はbinom.test()で出来る。 #alternative = c(&quot;two.sided&quot;)は，両側検定をするためのオプション。alternativeを指定しなければ，出てくる結果はデフォルトで両側検定になる。 binom.test(x = 2, n = 10, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 #alternative = c(&quot;less&quot;)は，片側検定をするためのオプション。2/10よりも低い確率を求めよということ。 binom.test(x = 2, n = 10, p = 0.5, alternative = c(&quot;less&quot;)) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.05469 ## alternative hypothesis: true probability of success is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5069013 ## sample estimates: ## probability of success ## 0.2 2集団間の平均値の比較（t検定など）も，基本的に同じような考え方である。平均値に差がないと仮定したときの理論分布と比べて，実際に得られた値がどれくらい珍しいのかを検討する。 5.2 第1種の過誤と第2種の過誤 「帰無仮説が真なのに，帰無仮説を棄却してしまう誤り」のことを，第1種の過誤（type Ⅰ error）という。 * 本当は差がないのに，”差がある”と判断してしまう誤り。 * 第1種の過誤を犯す確率を\\(\\alpha\\)と表現する。 * \\(\\alpha\\)は有意水準の値そのものである（\\(\\alpha=0.05\\)）。 「帰無仮説が偽なのに，帰無仮説を採択してしまう」誤りのことを，第2種の過誤（type Ⅱ error）という。 * 本当は差があるのに，”差がない”と判断してしまう誤り。 * 第2種の過誤を犯す確率を\\(\\beta\\)と表現する。 「帰無仮説が偽であるときに，正しく帰無仮説を棄却する確率」のことを”検定力”という。 * 差があるときに，”差がある”と正しく判断できる確率。 * 検定力は\\(1 - \\beta\\)で求められる（全体の確率から第２種の過誤を犯す確率を引いたもの）。 表1 \\(\\alpha\\)と\\(\\beta\\)はトレード・オフの関係にある。第Ⅰ種のエラーを避けようと思い有意水準を小さくすれば（例えば\\(\\alpha=0.001\\)とするなど），帰無仮説の棄却が厳しくなり，逆に第Ⅱ種のエラーを犯してしまう確率も高くなる（帰無仮説が偽であるにもかかわらず，棄却しない）。 5.3 統計的帰無検定の問題 5.3.1 p値 p値は標本数に依存する。標本数が多くなるほどp値は小さくなる。 例えば先程のコイン投げの例で，100回コインを投げて20回表が出た場合で（表が出る確率は0.2で先程の例と等しい），「フェアなコインと比べてこのコインはゆがんでいるか」を検討する。 binom.test(x = 20, n = 100, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 20 and 100 ## number of successes = 20, number of trials = 100, p-value = ## 1.116e-09 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.1266556 0.2918427 ## sample estimates: ## probability of success ## 0.2 そこで，p値ではなく，差の大きさそのものを表す指標である”効果量”というものが扱われている（詳しくは次回）。 5.3.2 多重比較の問題（＊） 統計的検定を繰り返すと，有意な結果が生じる確率が増える。例えば，5%水準で10回検定を行えば，少なくとも1回は帰無仮説を誤って棄却してしまう確率が0.4 になる。 #全ての確率から，「10回検定を行って全て正しい判断を行う」確率を差し引いたものが，少なくとも1回は誤った判断をしてしまう確率 1 - (1 - 0.05)^10 ## [1] 0.4012631 このように複数回検定を行うことを多重比較という。有意な結果が出やすくなってしまうので，p値を調整する必要がある。 有名な多重比較補正は，ボンフェローニの補正である。10回検定を行うので，有意水準を元の0.05から0.05/10を基準とするといったかたちで調整する。 0.05/10 #つまり，p &lt; 0.005でなければ「有意である」と結論しないという保守的な立場を取る ## [1] 0.005 練習問題 問１ 以下について，説明できるようにしておく。 「帰無仮説」とはなにか？ 「p値」とはなにか？ 「第Ⅰ種の過誤」と「第Ⅱ種の過誤」とはなにか？ 問２ コインを100回投げて、表が51回出た。有意水準を5%として統計的仮説検定をした場合，「このコインは歪みがない」という結論を出せるか？ コインを10,000回投げて、表が5,100回出た。有意水準を5%として統計的仮説検定をした場合，「このコインは歪みがない」という結論を出せるか？ "],
["section-6.html", "Chapter 6 ｔ検定，分散分析 6.1 t検定 6.2 分散分析 6.3 まとめ 練習問題", " Chapter 6 ｔ検定，分散分析 これまで学んできた様々な統計的検定について復習をする。 * t検定 * 分散分析 理論については深く掘り下げない（統計学の基礎授業で学んでいるので）。どの分析も前回学んだ「データが帰無仮説の分布に従うとしたときに，今回得られたデータが得られる確率がどれくらいまれか」を検討している点で共通している。様々な統計的検定の復習を通して，帰無仮説検定の考え方（p値とは何か）について理解する。 必要なパッケージのインストールと乱数の種の指定をする。 installed.packages(&quot;tidyverse&quot;) library(tidyverse) set.seed(1234) 6.1 t検定 連続量の変数を扱う検定の場合，t検定を使うのが一般的である。 6.1.1 一標本のt検定 まずサンプルデータとして，平均0, 標準偏差1の正規分布からランダムに10個サンプルしたデータXを作る。 それぞれの平均と標準偏差を求めてみる。 set.seed(1234) X = rnorm(n = 10, mean = 0, sd = 1) X ## [1] -1.2070657 0.2774292 1.0844412 -2.3456977 0.4291247 0.5060559 ## [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378 mean(X) ## [1] -0.3831574 sd(X) ## [1] 0.9957875 このデータXから，帰無仮説「母集団の平均\\(\\mu = 0\\)」を検定する。 #mu=0は入力しないでもOK t.test(X, mu=0) ## ## One Sample t-test ## ## data: X ## t = -1.2168, df = 9, p-value = 0.2546 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -1.0955009 0.3291861 ## sample estimates: ## mean of x ## -0.3831574 t値，自由度(df)，p値が出力される。 t値は以下の式から計算された統計量である。 \\[ t = \\frac{\\bar{X} - \\mu}{\\sqrt{s^2/n}} \\] Xが\\(N(\\mu, \\sigma^2)\\)に従う場合，tは自由度\\(n-1\\)のt分布に従う。 式にサンプルデータの平均と分散を入れてt値がt.test()で求めたものと一致するかを確かめてみよう。 t = (mean(X) - 0) / sqrt(var(X)/10) t ## [1] -1.216776 Rならば，t.test()を使えばt値とp値，更に母集団の平均\\(\\mu\\)の95%信頼区間を出力してくれる。 出力結果から，\\(N(\\mu, \\sigma^2)\\)から今回観測されたデータX以上のtが得られる確率は0.25であるということを示している。この確率はまれといえるか（5%未満か）について結論を下す。 6.1.2 二標本の差のt検定（対応なし） set.seed(1234) Value = c(rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 10, mean = 1, sd = 1)) Treatment = c(rep(&quot;X&quot;, 10), rep(&quot;Y&quot;, 10)) sample_data = data.frame(Treatment = Treatment, Value = Value) sample_data %&gt;% group_by(Treatment) %&gt;% summarise(Mean = mean(Value), SD = sd(Value), N = length(Value)) ## # A tibble: 2 x 4 ## Treatment Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 X -0.383 0.996 10 ## 2 Y 0.882 1.07 10 Xのサンプル数をm, Yのサンプル数をnとする。 \\[ t = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{s^2_{X}/m+s^2_{Y}/n))}} \\] XとYは同じ\\(N(\\mu, \\sigma^2)\\)に従う時，t値は自由度\\(m+n-2\\)のt分布に従う。 つまり，同じ母集団からXとYが抽出されたと仮定した時（\\(\\bar{X} - \\bar{Y}\\)），今回のデータが得られる確率がどのくらいまれであるかを検討する。 #2つの標本の母分散が等しいと仮定できない場合 #ウェルチの検定(Welch&#39;s t-test)と呼ばれる検定 t.test(data = sample_data, Value ~ Treatment, paired = F) ## ## Welch Two Sample t-test ## ## data: Value by Treatment ## t = -2.7404, df = 17.914, p-value = 0.01349 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.2351190 -0.2948544 ## sample estimates: ## mean in group X mean in group Y ## -0.3831574 0.8818293 #2つの標本の母分散が等しいと仮定する場合 #var.equal = Tを指定する（デフォルトでvar.equal = Fとなる） t.test(data = sample_data, Value ~ Treatment, paired = F, var.equal = T) ## ## Two Sample t-test ## ## data: Value by Treatment ## t = -2.7404, df = 18, p-value = 0.01344 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.2347852 -0.2951882 ## sample estimates: ## mean in group X mean in group Y ## -0.3831574 0.8818293 等分散を仮定した場合しない場合いずれも，それぞれの平均の間に有意な差があるといえる。 論文などにt検定の結果を報告するときは一般的に，「t(17.9)=2.74, p = 0.01」と書く。つまり，t値，自由度，p値を報告する。 一般的に2つの標本の母分散は不明であるので，それらが等しいかどうかも不明である。なので，等分散を仮定しないt検定をしておくほうが保守的である。 6.1.3 二標本の差のt検定（対応あり） t.test(data = sample_data, Value ~ Treatment, paired = T) ## ## Paired t-test ## ## data: Value by Treatment ## t = -2.5283, df = 9, p-value = 0.03233 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.3968339 -0.1331395 ## sample estimates: ## mean of the differences ## -1.264987 #なお，等分散を仮定する場合はvar.equal = Tを指定する。 t.test(data = sample_data, Value ~ Treatment, paired = T, var.equal = T) ## ## Paired t-test ## ## data: Value by Treatment ## t = -2.5283, df = 9, p-value = 0.03233 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.3968339 -0.1331395 ## sample estimates: ## mean of the differences ## -1.264987 6.2 分散分析 t検定で比較できるのは2群までである。3群以上の間で平均値の比較を行いたい場合は，分散分析（ANOVA）を行う。 6.2.1 一要因の分散分析 #サンプルデータ Y1 = c(1, 2, 3) Y2 = c(5, 7, 5) Y3 = c(5, 4, 2) Value = c(Y1, Y2, Y3) Treatment = factor(c(1, 1, 1, 2, 2, 2, 3, 3, 3)) sample_data2 = data.frame(Treatment = Treatment, Value = Value) sample_data2 ## Treatment Value ## 1 1 1 ## 2 1 2 ## 3 1 3 ## 4 2 5 ## 5 2 7 ## 6 2 5 ## 7 3 5 ## 8 3 4 ## 9 3 2 それぞれ3つのグループ（X = 1, 2, or 3）で変数Yを取ったとする。 グループごとの平均値などは，以下のようになっている。これら3つのグループの間で平均値に差があるのかを検定する。 sample_data2 %&gt;% dplyr::group_by(Treatment) %&gt;% dplyr::summarise(Mean = mean(Value), SD = sd(Value), N = length(Value)) ## # A tibble: 3 x 4 ## Treatment Mean SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 2 1 3 ## 2 2 5.67 1.15 3 ## 3 3 3.67 1.53 3 result_anova = aov(data = sample_data2, Value ~ Treatment) summary(result_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 20.222 10.111 6.5 0.0315 * ## Residuals 6 9.333 1.556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #以下でも同じ結果が出る oneway.test(data = sample_data2, Value ~ Treatment, var.equal = TRUE) ## ## One-way analysis of means ## ## data: Value and Treatment ## F = 6.5, num df = 2, denom df = 6, p-value = 0.03149 #lm関数でも分散分析表を出せる（ただし，lm関数を使った場合ではTukeyHSD()は使えないので注意。多重比較をしたい場合はaov()を使う。 result_anova_2 = lm(data = sample_data2, Value~Treatment) anova(result_anova_2) ## Analysis of Variance Table ## ## Response: Value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 2 20.2222 10.1111 6.5 0.03149 * ## Residuals 6 9.3333 1.5556 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ここでは分散分析の理論の詳細は省略する。 簡単に説明すると，分散分析ではグループ間の分散がグループ内の分散と比べて大きい確率を求めて，群の間で平均値に差があるかを検定する。 回帰分析（正確には線形モデルと呼ばれる統計解析の枠組み）でも，分散分析と同様の目的の検定を行うことができる。 分散分析は要因の数やそれぞれの要因の対応ありなしによって，やり方が複雑になる（例えばテストの成績に対して，男女，学年，科目によってどう差があるかを検討するなど）。線形モデルならば，もっとシンプルに行うことができる。 分散分析の結果を論文で報告するときは，「F(2, 6) = 2.26, p = .19」といったように報告する。 anova()では，逐次平方和（Type Ⅰ SS）が出力される（要因の各セルのデータ数にかたよりがある場合，独立変数を投入した順序（x1, x2とx2, x1）で平方和の値の計算結果が異なる場合がある）。調整平方和（Type Ⅱ，Type Ⅲ）を出したいときは，carパッケージのAnova()など別のパッケージが必要になる。 6.2.2 多重比較 「3つのグループの間で平均値に差がない」という帰無仮説が棄却された場合，どのグループの間に有意な差があるのかを検定する必要がある。このサンプルデータの場合，グループ１とグループ２，グループ２とグループ３，グループ１とグループ３との間，計３つの組み合わせで平均値の比較を行う。つまり，t検定を3回行って条件間の比較をする。 ただし，第5回でも述べたように，統計的検定を繰り返し行う（多重比較）と第一種のエラーを犯す確率が増える。t検定で出てきたp値をそのまま報告するのではなく，多重比較の問題を考慮した上でp値に補正をかける（p値を過大に評価し直す）必要がある。 多重比較の補正法には，いくつかの方法がある。 * ボンフェローニ（Bonferroni）の方法: p値を比較の回数分かけて過大に評価する * チューキー(Tukey)の方法 * ホルム(Holm)の方法 多重比較の補正を行うときは，pairwise.t.test関数を使う。各群間の比較について，補正後のp値が出力される。 pairwise.t.test(sample_data2$Value, g = sample_data2$Treatment, p.adjust.method = &quot;bonferroni&quot;) #gがグループを意味する変数，p.adjust.methodに補正方法を指定する。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: sample_data2$Value and sample_data2$Treatment ## ## 1 2 ## 2 0.034 - ## 3 0.458 0.291 ## ## P value adjustment method: bonferroni pairwise.t.test(sample_data2$Value, g = sample_data2$Treatment) #オプションに何も指定しないと，ホルムの方法の結果が出力される。ボンフェローニの方法は保守的すぎるので，他の方法の方が好まれる。 ## ## Pairwise comparisons using t tests with pooled SD ## ## data: sample_data2$Value and sample_data2$Treatment ## ## 1 2 ## 2 0.034 - ## 3 0.194 0.194 ## ## P value adjustment method: holm よく使われるチューキーの方法を使う場合は，TukeyHSD()が用意されている。 TukeyHSD(result_anova) #TukeyHSD()に，分散分析表の結果を入れる ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Value ~ Treatment, data = sample_data2) ## ## $Treatment ## diff lwr upr p adj ## 2-1 3.666667 0.5420889 6.791244 0.0263939 ## 3-1 1.666667 -1.4579111 4.791244 0.3023895 ## 3-2 -2.000000 -5.1245777 1.124578 0.2019189 6.3 まとめ 統計学の基礎の復習として，これまで学んだ統計解析（＋α）の復習をしてきた。いずれも考え方は共通していて，「データから求めた統計量が帰無仮説に従うとしたときに，今回のデータよりもまれな事象が得られる確率（p値）」を求めている。 更に，これまで学んできた統計手法は統計モデルというかたちで包括的に理解することができる。詳しくはまた後日学ぶ「一般化線形モデル」の回で扱う。t検定，分散分析なども，「一般化線形モデル」の中に含まれる。「一般化線形モデル」を理解すれば，様々なデータに対して適切な分析を行うことができるようになる。 次回はp値を基準として結論を出す帰無仮説検定が抱える問題について扱う。 練習問題 問１ 以下のプログラムを読み込む。 あるサプリメントにダイエットの効果があるかを検討するために，10名を対象に実験を行った。それぞれの参加者にサプリメントを投与する前(before)と投与した後（after）で体重を測定した（架空のデータである）。 before = c(37.93, 52.77, 60.84, 26.54, 54.29, 55.06, 44.25, 44.53, 44.36, 41.1) after = c(57.84, 50.02, 53.36, 65.97, 79.39, 63.35, 57.33, 51.33, 52.44, 101.24) Value = c(before, after) Treatment = c(rep(&quot;before&quot;, 10), rep(&quot;after&quot;, 10)) Subject = c(c(1:10), c(1:10)) sample_1 = data.frame(Subject = Subject, Treatment = Treatment, Value = Value) sample_1 ## Subject Treatment Value ## 1 1 before 37.93 ## 2 2 before 52.77 ## 3 3 before 60.84 ## 4 4 before 26.54 ## 5 5 before 54.29 ## 6 6 before 55.06 ## 7 7 before 44.25 ## 8 8 before 44.53 ## 9 9 before 44.36 ## 10 10 before 41.10 ## 11 1 after 57.84 ## 12 2 after 50.02 ## 13 3 after 53.36 ## 14 4 after 65.97 ## 15 5 after 79.39 ## 16 6 after 63.35 ## 17 7 after 57.33 ## 18 8 after 51.33 ## 19 9 after 52.44 ## 20 10 after 101.24 投与前と投与後それぞれについて，体重の平均値及び標準偏差を求めて報告せよ。 このサプリメントの投与により体重が変化したかについてt検定（等分散を仮定しない）で検討し，結果について報告するとともに結論を述べよ。 問２ 以下のプログラムを読み込む。 ある教授法に児童の学力向上の効果があるかを検討した。学校Bにはその教授法を実施し，学校Aには何もしなかった。その後，学校Aと学校Bそれぞれ10人の生徒に学力テストを行った。A，Bそれぞれが学校A，Bそれぞれの生徒の成績である（架空のデータである）。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(48, 40, 43, 56, 69, 53, 47, 41, 42, 91) Value = c(A, B) Treatment = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) sample_2 = data.frame(Treatment = Treatment, Value = Value) sample_2 ## Treatment Value ## 1 A 38 ## 2 A 53 ## 3 A 61 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 48 ## 12 B 40 ## 13 B 43 ## 14 B 56 ## 15 B 69 ## 16 B 53 ## 17 B 47 ## 18 B 41 ## 19 B 42 ## 20 B 91 学校Aと学校Bそれぞれについて，テストの得点の平均値及び標準偏差を求めて報告せよ。 この教授法に成績向上があったかどうかについてt検定（等分散を仮定しない）で検討し，結果について報告するとともに結論を述べよ。 問３ 以下のプログラムを読み込む。 A県, B県, C県のそれぞれで学力テストを行った。各県で10名の生徒の成績を集計し，県(prefecture)ごとの得点（score）である（架空のデータである）。A，Bそれぞれが学校A，Bそれぞれの生徒の成績である。 Value = c(38, 40, 58, 27, 54, 55, 44, 45, 44, 41, 44, 38, 41, 51, 62, 49, 44, 39, 40, 79, 61, 55, 56, 65, 53, 46, 66, 50, 60, 51) Prefecture = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10), rep(&quot;C&quot;, 10)) sample_3 = data.frame(Prefecture = Prefecture, Value = Value) sample_3 ## Prefecture Value ## 1 A 38 ## 2 A 40 ## 3 A 58 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 44 ## 12 B 38 ## 13 B 41 ## 14 B 51 ## 15 B 62 ## 16 B 49 ## 17 B 44 ## 18 B 39 ## 19 B 40 ## 20 B 79 ## 21 C 61 ## 22 C 55 ## 23 C 56 ## 24 C 65 ## 25 C 53 ## 26 C 46 ## 27 C 66 ## 28 C 50 ## 29 C 60 ## 30 C 51 A県，B県，C県の成績の平均値及び標準偏差は以下の通りである。 Prefecture Mean SD N A 44.6 9.215928 10 B 48.7 12.858633 10 C 56.3 6.600505 10 県によって学力に違いがあるかについて一元配置分散分析で検討し，結果について報告するとともに結論を述べよ。 県の間で学力に有意差が見られた場合は，どの県とどの県との間に有意差があるかをホルムの方法で確認し，結果について報告せよ。 "],
["section-7.html", "Chapter 7 統計的帰無仮説検定の問題 7.1 p値の問題 7.2 効果量 7.3 検定力 7.4 有意水準，効果量，検定力，標本数の関係 7.5 信頼区間 練習問題 参考文献", " Chapter 7 統計的帰無仮説検定の問題 以下の4つの関係を理解し，統計的帰無仮説検定が抱える問題点について理解する。 * 有意水準 * 効果量 * 検定力 * サンプルサイズ * 信頼区間 tidyverseとpwrパッケージをインストールして使う。 install.packages(&quot;pwr&quot;, &quot;tidyverse&quot;) library(pwr) library(tidyverse) 7.1 p値の問題 7.1.1 第Ⅰ種の過誤 第Ⅰ種の過誤（\\(\\alpha\\)）とは，「帰無仮説が真なのに，帰無仮説を棄却する誤り」であった。第Ⅰ種の過誤を犯す確率は，有意水準として設定する確率である。 有意水準は一般的に5%が用いられる。第Ⅰ種の過誤を犯す確率を少なくするためにできるだけ小さい確率ということで，慣習として5%とされている（20回に1回くらいの誤りは許す）。 7.1.2 p値と標本数の関係 しかし，p値は標本数に依存する。標本数が多くなるほどp値は小さくなる。 例えばコイン投げて表が出る回数をカウントする実験を行う。10回コインを投げて2回表が出た場合，100回コインを投げて20回表が出た場合それぞれについて（表が出た割合はどちらも0.2），二項検定を行う。 「フェアなコインと比べてこのコインはゆがんでいるか」を検討する。 binom.test(x = 2, n = 10, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 2 and 10 ## number of successes = 2, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.02521073 0.55609546 ## sample estimates: ## probability of success ## 0.2 binom.test(x = 20, n = 100, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: 20 and 100 ## number of successes = 20, number of trials = 100, p-value = ## 1.116e-09 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.1266556 0.2918427 ## sample estimates: ## probability of success ## 0.2 「有意ではない（\\(p &gt; 0.05\\)）」というのは「差がない」ということを意味しない（差自体は存在する：今回の実験結果0.2とフェアなコインの結果0.5）。少ない標本数では，珍しい結果が生じることもありえる。意味のある差かどうかが，標本数が少なすぎて判断できないということである。 逆に，実質意味のない差であっても，標本数が多ければ統計的に有意な差（\\(p &lt; .05\\)）は得られる。例えば，10,000回コインを投げて，表が出た回数が4,900回だった場合（表が出る割合は0.49），このコインがフェアなコインよりも歪んでいるかを検定してみると，有意（\\(p &lt; .05\\)）な結果が得られる。 n = 10000 x = 0.49 * n binom.test(x = x, n = n, p = 0.5, alternative = c(&quot;two.sided&quot;)) ## ## Exact binomial test ## ## data: x and n ## number of successes = 4900, number of trials = 10000, p-value = ## 0.04659 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4801563 0.4998496 ## sample estimates: ## probability of success ## 0.49 しかし，0.49と0.50の差，すなわち1%の違いを意味のある差と判断してよいのか？ 7.1.3 p-hacking（＊） 方法次第で，「有意な結果」を導くことが可能。このようなインチキはpハッキングと表現されることがある。 * たくさんの質問項目について個別に検定を行い，有意な結果だけについて議論する。 * たくさん実験を行って，有意な結果だけを報告する。 * 被験者を少しずつ追加していく度に検定を行い，有意になったら追加するのをやめる。 有意な結果が得られた研究を報告している論文があった。その論文の研究を追試してみたところ，有意な効果は見られなかった。しかし，「追試のやりかたが悪かったからだ」などと批判され，既存の研究に対してネガティブなデータは公表されにくい。これは，出版バイアス（publiation bias）と呼ばれる。 7.2 効果量 上述のように，p値は「帰無仮説のもとで今回のデータ以上にまれな結果が得られる確率」を意味し，効果の大きさそのものを意味する指標ではない。有意だった（あるいは有意でなかった）からといって，差が大きい（ない）とは限らない。 効果の大きさを示す指標は，効果量（effect size）と呼ばれる。 7.2.1 Cohen’s d ここでは，Cohenのd（Cohen’s d）と呼ばれる指標を紹介する。 Cohen’s dは，以下の式で計算される。nはそれぞれの群の標本数，\\(\\bar{X}\\)はそれぞれの群の標本平均，\\(S_{1}^2\\)と\\(S_{2}^2\\)はそれぞれ2群の不偏分散とする。すなわち，\\(S_{P}\\)は2群をプールした上での標準偏差を意味する。 \\[ \\begin{equation} d = \\frac{|\\bar{X_{1}} - \\bar{X_{2}|}}{S_{P}} \\\\ S_{P} = \\sqrt{\\frac{(n_{1}-1)S_{1}^2 + (n_{2}-1)S_{2}^2}{n_{1}+n_{2}}} \\end{equation} \\] つまり，2群の標本平均の差が標準偏差の何倍大きいかを示す指標がdである。 7.2.1.1 例 60人の学生をそれぞれ2群に分け，学習課題を行わせた。課題の前に，実験群にはある訓練を，統制群には関係のない課題を行わせた。実験群と統制群の間で，学習課題の成績(Y)を比較する。 set.seed(1234) #実験群：平均60，標準偏差20の正規分布から，30個サンプルする。 Exp = round(rnorm(n=30, mean = 60, sd = 20), 0) #平均55，標準偏差20の正規分布から，30個サンプルする。 Cntr = round(rnorm(n=30, mean = 55, sd = 20), 0) sample_data = data.frame(Y = c(Exp, Cntr), X = c(rep(&quot;Exp&quot;, 30), rep(&quot;Cntr&quot;, 30))) それぞれ，群別に平均値と標準偏差を見てみる。 dplyr::group_by(sample_data, X) %&gt;% dplyr::summarise(mean = mean(Y), var = var(Y), sd = sd(Y), n = length(Y)) ## # A tibble: 2 x 5 ## X mean var sd n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Cntr 44 367. 19.2 30 ## 2 Exp 54.1 326. 18.0 30 t検定で2群の平均値の差が統計的に有意かを検討する。 t.test(data=sample_data, Y~X, paired = F) ## ## Welch Two Sample t-test ## ## data: Y by X ## t = -2.0947, df = 57.793, p-value = 0.0406 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.687396 -0.445937 ## sample estimates: ## mean in group Cntr mean in group Exp ## 44.00000 54.06667 効果量\\(d\\)を求める（上記の式に当てはまる数値を入れる）。 SP = sqrt(((30 - 1)*367 + (30 - 1)*326 ) / (30 + 30)) d = abs(44 - 54.1) / SP #abs()は絶対値を求める関数 d ## [1] 0.5518631 Cohen (1968)によると，効果量の評価は「小さい」，「中くらい」，「大きい」の目安がある。 だいたい，d = 0.2 が小さい，d = 0.5が中くらい，d = 0.8が大きい。 Cohen’s dは2つの標本の平均値の差の大きさを示す指標である。例えば，効果量0.2は以下のようになる。 curve(dnorm(x), lwd=2, xlim=c(-3,3), xlab=&quot;&quot;, ylab=&quot;&quot;, frame.plot= FALSE, yaxt=&quot;n&quot;, yaxs=&quot;i&quot;) curve(dnorm(x,mean=0.2), lwd=2, add=TRUE) segments(0, 0, 0, dnorm(0)) segments(0.2, 0, 0.2, dnorm(0)) 7.2.1.2 その他の効果量 Cohen’s d以外にも，効果量の指標が提案されている。詳しくは，大久保・岡田(2012)「伝えるための心理統計学」の第3章を参照。 7.3 検定力 7.3.1 第Ⅱ種の過誤 第Ⅱ種の過誤（\\(\\beta\\)）とは，「帰無仮説が偽なのに，帰無仮説を採択してしまう」誤りのことであった。 検定力とは，正しい判断の確率，つまり「帰無仮説が偽であるときに，正しく帰無仮説を棄却する確率」であった。 * 検定力は\\(1 - \\beta\\)で求められる（全体の確率から第２種の過誤を犯す確率を引いたもの） 検定力は有意水準（α=0.05）と違って特に基準が定まっているわけではないが，.80を水準として設定するのが良いとされている（5回に1回の誤りは許す:β=0.2）。 後述するように，統計的検定の検定力は標本数にも依存する。標本数が大きければ検定力は上がる（わずかな差でも”有意差”として検出してしまう）。標本数が少なければ検定力は下がる。 7.4 有意水準，効果量，検定力，標本数の関係 有意水準，効果量，検定力，標本数はそれぞれ関わり合っている。 * 標本数が増えれば有意な結果が出やすくなるというのは既に見たとおりである。 いたずらに標本数を増やせば意味のない差も検出されてしまう。無駄な労力にもなる。 これら4つのパラメータのうち，研究者が左右できるのは「標本数」だけである。 * 効果量は研究で明らかにしたいものそのもの。測定しなければわからない（事前に知ることはできない。先行研究からこれくらいだろうと予想することはできる）。 研究の前にあらかじめ「有意水準」，「効果量（の予想）」，「検定力」を決めておけば，取るべき標本数が定まる（事前の検定力分析）。また，データの取得後に，「有意水準」，「効果量」，「標本数」から，そのデータに対する「検定力」を調べることができる（事後の検定力分析）。 Rでは，pwrパッケージにある関数を用いて検定力の分析をすることができる。 #事前の検定力分析（標本数の設計） #2群間の差をt検定で検定する場合 #各群の標本数(n)，効果量（d: Cohen&#39;s d），有意水準（sig.level），検定力（power）のどれか３つを入れると，入れなかったものの結果が出力される。 pwr::pwr.t.test(d=0.5, power=0.8, sig.level=0.05, n=NULL) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group #事後の検定力分析 #２群それぞれの標本数(n1, n2)，効果量(d), 有意水準(sig.level)を入れると，検定力が求められる。 pwr::pwr.t2n.test(n1=10, n2=10, d=0.8, sig.level=0.05, power=NULL) ## ## t test power calculation ## ## n1 = 10 ## n2 = 10 ## d = 0.8 ## sig.level = 0.05 ## power = 0.3950692 ## alternative = two.sided 7.5 信頼区間 95%信頼区間(95% confidence interval)は，95%の確率で母数が含まれる範囲のことを言う。 例えば，母集団からランダムに20個の標本を抽出し，平均を求める。 set.seed(1234) sample_data2 = rnorm(n=20, mean=0, sd=1) sample_data2 ## [1] -1.20706575 0.27742924 1.08444118 -2.34569770 0.42912469 ## [6] 0.50605589 -0.57473996 -0.54663186 -0.56445200 -0.89003783 ## [11] -0.47719270 -0.99838644 -0.77625389 0.06445882 0.95949406 ## [16] -0.11028549 -0.51100951 -0.91119542 -0.83717168 2.41583518 mean(sample_data2) ## [1] -0.2506641 平均は-0.25であった。この20個のデータから，母集団の平均を推定する。 標本から得られた値をもとに母集団のパラメータを評価する手続きは，推定と呼ばれる。 もちろん，正確な母集団の平均値を当てることは難しい。そこで，母集団の平均値が入るだろうと予測される範囲を推定する。これが，信頼区間である。 7.5.1 信頼区間の求め方 例えば平均値の信頼区間は以下のように求める。 \\[ CI = \\bar{X} + ME\\\\ ME = SE \\times \\pm t_{95\\%} \\] MEは誤差範囲(margin of errors)である。つまり，標本から得られた値について正及び負の方向に誤差を加えた値が信頼区間の上限及び下限値となる。 誤差範囲には標準誤差（SE）を用いる。 データが正規分布に従うのならば，誤差範囲はt分布を使って求めるのが一般的。\\(t_{95\\%}\\)は，t分布の95％点に対応する（t分布の下位または上位2.5%に対応するtの値）。 #conf.levelの値を変えれば，信頼区間の範囲を任意で設定できる（デフォルトで0.95） t.test(sample_data2, conf.level = 0.95) ## ## One Sample t-test ## ## data: sample_data2 ## t = -1.1057, df = 19, p-value = 0.2826 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.7251406 0.2238125 ## sample estimates: ## mean of x ## -0.2506641 平均値の信頼区間ならば，Rではt.test()でも求めることができる。 7.5.2 信頼区間と統計的帰無仮説検定の関係 信頼区間は統計的帰無仮説検定と表裏一体である。 例えば「母集団の平均値は0ではない」という帰無仮説を評価する。すなわち，母集団の平均値の95％信頼区間にゼロが含まれていなければ，帰無仮説は棄却されることになる。 標本数を増やせば（標準）誤差が小さくなるので，信頼区間の範囲も狭くなる。 7.5.3 平均値以外の信頼区間の求め方（＊） 割合について求める場合は，Rならばbinom.test()で求められる。 #100人中，30人がある意見に賛成した場合。母集団の賛成率の信頼区間は？ #conf.levelの値を変えれば，信頼区間の範囲を任意で設定できる（デフォルトで0.95） binom.test(x=30, n=100, conf.level = 0.95) ## ## Exact binomial test ## ## data: 30 and 100 ## number of successes = 30, number of trials = 100, p-value = ## 7.85e-05 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.2124064 0.3998147 ## sample estimates: ## probability of success ## 0.3 もちろん，他の統計量についても，同じく信頼区間を評価することは可能。相関係数，回帰係数，効果量などについても求められる（詳しくは，大久保・岡田, 2012）。 信頼区間を評価することで，その推定値の正確さや範囲を評価することができる。 信頼区間を報告するときは一般的に，\\(CI = [-0.56, 0.36]\\)といったように表現する。 最近の心理学においても，信頼区間の報告が求められるようになっている。グラフのエラーバーの範囲にも信頼区間を載せることも多い（95%水準で有意差があるのかがわかりやすくなる） 練習問題 問１ 二群間で平均値をt検定で比較する場合，効果量が小さい，中くらい，大きいとき（それぞれ，\\(d=0.2, d=0.5, d=0.8\\)），それぞれで一群あたり何名程度の参加者を取ればよいか？有意水準を0.05, 検定力を0.8とした上で求めよ。 ヒント：Rのpwrパッケージのpwr.t.test()を使えば求まる。 問２ 小問1 以下のプログラムを読み込む。 あるサプリメントにダイエットの効果があるかを検討するために，10名を対象に実験を行った。それぞれの参加者にサプリメントを投与する前(before)と投与した後（after）で体重を測定した（架空のデータである）。 before = c(37.93, 52.77, 60.84, 26.54, 54.29, 55.06, 44.25, 44.53, 44.36, 41.1) after = c(57.84, 50.02, 53.36, 65.97, 79.39, 63.35, 57.33, 51.33, 52.44, 101.24) Value = c(before, after) Treatment = c(rep(&quot;before&quot;, 10), rep(&quot;after&quot;, 10)) Subject = c(c(1:10), c(1:10)) sample_1 = data.frame(Subject = Subject, Treatment = Treatment, Value = Value) sample_1 ## Subject Treatment Value ## 1 1 before 37.93 ## 2 2 before 52.77 ## 3 3 before 60.84 ## 4 4 before 26.54 ## 5 5 before 54.29 ## 6 6 before 55.06 ## 7 7 before 44.25 ## 8 8 before 44.53 ## 9 9 before 44.36 ## 10 10 before 41.10 ## 11 1 after 57.84 ## 12 2 after 50.02 ## 13 3 after 53.36 ## 14 4 after 65.97 ## 15 5 after 79.39 ## 16 6 after 63.35 ## 17 7 after 57.33 ## 18 8 after 51.33 ## 19 9 after 52.44 ## 20 10 after 101.24 対応のあるt検定で投与前と投与後の体重の比較をし，帰無仮説が棄却されるかについて結論を述べよ。 ヒント：対応のあるt検定のやりかたについては，第６回の資料を参照。 小問２ 以下のデータは，小問１のデータを横に並べ替えたものである。参加者１人につき１行，投与前（before）と投与後（after）も数値が入力されている。 sample_2 = data.frame(Subject = c(1:10), before = before, after = after) sample_2 ## Subject before after ## 1 1 37.93 57.84 ## 2 2 52.77 50.02 ## 3 3 60.84 53.36 ## 4 4 26.54 65.97 ## 5 5 54.29 79.39 ## 6 6 55.06 63.35 ## 7 7 44.25 57.33 ## 8 8 44.53 51.33 ## 9 9 44.36 52.44 ## 10 10 41.10 101.24 投与前と投与後の体重の差を求めて，体重の差の95%信頼区間を求めよ。 ヒント：投与前と投与後の差の変数を作る。つまり，beforeからafterを引き（逆でも可），diffという新しい変数を作る。新しく作った変数をt.test()に入れれば95%信頼区間が求まる。 参考文献 大久保街亜・岡田謙介 (2012). 伝えるための心理統計 勁草書房 "],
["section-8.html", "Chapter 8 回帰分析 8.1 回帰分析 8.2 重回帰分析 8.3 ダミー変数（独立変数が質的変数の場合） 練習問題", " Chapter 8 回帰分析 回帰分析の基礎について学ぶ。 * 回帰分析 * 最小二乗法 * 重回帰分析 install.packages(&quot;tidyverse&quot;) install.packages(&quot;car&quot;) library(tidyverse) library(car) 8.1 回帰分析 以下のプログラムを読み込み，サンプルデータを作る。 #架空のデータを作る set.seed(1234) N = 100 a = 10 b1 = 3 b2 = 0.5 x1 = rnorm(n = N) x2 = rnorm(n = N) e = rnorm(n = N, sd = 5) y = b1*x1 + b2*x2 + a + e sample_data = data.frame(x1 = x1, x2 = x2, y= y) head(sample_data) #サンプルデータの上数行を表示 ## x1 x2 y ## 1 -1.2070657 0.41452353 9.012199 ## 2 0.2774292 -0.47471847 14.078772 ## 3 1.0844412 0.06599349 14.213890 ## 4 -2.3456977 -0.50247778 6.215336 ## 5 0.4291247 -0.82599859 12.432780 ## 6 0.5060559 0.16698928 15.403974 qplot(data=sample_data, x1, y) #x1とyの散布図を示す 以下のことが知りたい。 新たに測定を行ったとき，x1 = 0.1 の値が得られた。この値から，y がどのような値になるのか？（新たなデータから，yを予測する） x1 が 1増えたら，y がどれくらい増えるのか？（x1の効果の強さを知りたい） 回帰分析(regresion analysis)は，「データの予測」と「効果の測定」を目的として行う。 回帰分析とは，以下の式により独立変数の値から従属変数の値を予測する解析である。以下のような式は，「回帰式」あるいは「線形予測子」と呼ばれる。 \\[ \\begin{equation} \\hat{y} = bx+a \\end{equation} \\] xを独立変数，yを従属変数とする。\\(\\hat{y}\\)は，yの予測値とする。傾きbと切片aをデータから求める。 実測値であるyと予測値である\\(\\hat{y}\\)の差が最も小さくなるときの，bとaを求める。 回帰分析はシンプルな直線の式から結果を予測する解析である。 切片aは，独立変数xがゼロのときの従属変数の予測値を表現している。 傾きは，回帰係数（regression coefficient）と呼ばれる。 回帰係数は独立変数が従属変数にもたらす効果の強さを意味する。つまり，効果量の一種ともいえる。 なお，回帰分析において独立変数を「説明変数」，従属変数を「目的変数，応答変数，ないしは被説明変数」という場合もある。 8.1.1 回帰分析の結果の解釈 回帰分析をする場合，Rではlm()を使う。 result = lm(data = sample_data, y ~ x1) #結果を別の変数で保存しておき，summary()関数で詳細な結果を出すことができる summary(result) ## ## Call: ## lm(formula = y ~ x1, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.3526 -3.3001 0.5401 2.6685 13.2098 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8529 0.4945 21.95 &lt; 2e-16 *** ## x1 3.3780 0.4889 6.91 4.94e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.886 on 98 degrees of freedom ## Multiple R-squared: 0.3276, Adjusted R-squared: 0.3207 ## F-statistic: 47.74 on 1 and 98 DF, p-value: 4.935e-10 係数（Coefficients）の結果を見る。 Interceptは切片を意味する。x1が独立変数の傾きを意味する。 Estimateが推定された切片または傾きの値である。 Std.Errorは推定された係数の標準誤差である。 t value及びPrは係数の有意性検定の結果を示している（それぞれt値，p値）。ここでは，「係数がゼロである」という帰無仮説を検定している。p値が極端に低い場合は，「求めた係数の値は有意にゼロから乖離している」と結論付けることができる。 傾きが意味することは，独立変数が1単位増えたら従属変数がどう変化するかということである。 係数がプラスならば，係数の値が増えると従属変数の値が増える関係にあることを意味する。 係数がマイナスならば，係数が値が増えると従属変数が減る関係にあることを意味する。 さっきの散布図に回帰分析から求めた回帰直線を引いてみよう。 p = qplot(data = sample_data, x1, y) p = p + stat_smooth(method = &quot;lm&quot;, se = FALSE) p 概ねデータに重なるかたちで直線が引かれている。 他にも出力に「Multiple R-Squared」とあるが，これは「決定係数」と呼ばれるもので，データの全ての分散のうち，今回の回帰分析によって説明できている割合を意味する。0から1の範囲を取り，1に近いほど今回の回帰分析の結果がデータを良く説明できていることを意味する。 \\[ \\begin{equation} R^2 = \\frac{\\sum^n_{i=1}(\\hat{y}_i-\\bar{y})^2}{\\sum^n_{i=1}(y_i-\\bar{y})^2} \\end{equation} \\] \\(y\\)は実測値，\\(\\bar{y}\\)は実測値yの平均値，\\(\\hat{y}\\)は回帰分析で求めたyの予測値を意味する。 8.1.2 最小二乗法（＊） では，回帰分析では傾きと切片の値をどう求めているのだろうか？ 実測値と予測値との差は残差（Residual value）と呼ばれる。残差が最も小さくなるときの傾き及び切片を，yを予測する上で最適な傾き及び切片の値として採用する（残差が最小となるときの式が，データをうまく説明できていると解釈できる）。 このような回帰式の切片及び傾きの求め方を，「最小二乗法」という。 ある条件のもとで結果を最小あるいは最大にする手法のことを，数学では最適化と呼ぶ。最小二乗法とは最適化手法の一種である。 当然ながら手計算では困難である。普通はコンピュータを使って計算する。Rには最適化を行うための関数optim()が用意されている。 あくまで参考までに，最小二乗法をoptim()で行ったプログラムを以下に示す。 y = sample_data$y x1 = sample_data$x1 residual = function(para){ b = para[1] a = para[2] hat_y = b*x1 + a #x1とパラメータa, bから，予測値hat_yを計算する sum((y - hat_y)^2) #予測値と実測値との差の二乗の合計値を求めたものをresidualという変数で保存 } optim(par = c(1, 1), residual) #特に何も指定しなければ，residualという変数を最小化せよという命令となる（par=c(1, 1)は，bとaを計算するときの初期値を1, 1にせよという命令。あまり深く考えなくて良い）。出力結果の「par」に，傾き(b)と切片(a)の推定結果が出る。lm()関数を使って計算した値と近似しているはず。 ## $par ## [1] 3.379037 10.852593 ## ## $value ## [1] 2339.32 ## ## $counts ## function gradient ## 67 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 最小二乗法以外にも，「最尤法（さいゆうほう）」という最適化手法でも同じ回帰係数を求めることができる。最尤法については，一般化線形モデルの回で説明する。 8.2 重回帰分析 独立変数が複数の場合の回帰分析は，重回帰分析(multivariant regression analysis)と呼ばれる。 独立変数が一つの場合は，単回帰分析と呼んで区別することもある。 例えば，独立変数が2つの場合の回帰式は以下のようになる。 \\[ \\begin{equation} \\hat{y} = b_{1}x_{1}+b_{2}x_{2} + a \\end{equation} \\] result_2 = lm(data=sample_data, y ~ x1 + x2) summary(result_2) ## ## Call: ## lm(formula = y ~ x1 + x2, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.812 on 97 degrees of freedom ## Multiple R-squared: 0.3544, Adjusted R-squared: 0.3411 ## F-statistic: 26.63 on 2 and 97 DF, p-value: 6.049e-10 例えば変数 X1 の回帰係数は，他の独立変数の効果が同じとした場合に X1 が1単位増えた時の y の変化量を意味している。 このように他の変数の影響を統制（control）した上で独立変数の効果を検討することができるのが，回帰分析のメリットの一つである。 8.2.1 多重共線性（＊） 独立変数同士が強く相関していると，回帰分析の結果は信用できない恐れがある。このような問題は「多重共線性（multicollinearity）」と呼ばれる。例えば，\\(x_{1}\\)と\\(x_{2}\\)が相関している，つまりほぼ\\(x_{1}=x_{2}\\)と言える場合，傾きの値もほぼ\\(b_{1}=b_{2}\\)となる。この場合，傾きの組み合わせが複数ありえるため（\\(b_{1}=1, b_{2}=2\\)や\\(b_{1}=2, b_{2}=1\\)），パラメータの値を推定することが出来ない。 独立変数同士の相関が0.8を超えている場合は，多重共線性を疑った方が良い。 多重共線性を疑う基準として，VIF(Variance inflation factor)と呼ばれる指標もある。VIFが10を超えていたら，多重共線性を疑った方が良いとされている。 \\[ \\begin{equation} VIF = \\frac{1}{1-R^2} \\end{equation} \\] 上の式の\\(R^2\\)は，ある独立変数を従属変数としたときの他の独立変数による重回帰分析での決定係数を意味する。 carパッケージのvif()を使うと，VIFを計算してくれる。 car::vif(result_2) #lm()関数で計算した回帰分析の結果を入れれば良い。 ## x1 x2 ## 1.000645 1.000645 8.2.2 決定係数 先に説明したように，決定係数は回帰分析によって説明されるデータの分散の割合，すなわち「回帰分析の精度の良さ」を示す指標として認識されている。 しかし，決定係数は基本的に独立変数を多く入れれば大きい値を取る傾向にある（意味のない変数を入れたとしても，データの分散を多少は説明できるようになる）。 この問題を解消するために，決定係数とは別に「情報量基準」という指標が用いられている。詳しくは「一般化線形モデル」の回で説明する。 8.3 ダミー変数（独立変数が質的変数の場合） 上の例は独立変数が量的変数の場合を用いたが，質的変数（性別，学生か否かなど）の場合でも可能である。解析の際には，変数を0か1の変数に変換する必要がある。ダミー変数（dummy variable）と呼ばれる。 #サンプルデータの作成 set.seed(1234) y = rnorm(100) x = round(runif(100),0) sampledata_2 = data.frame(y = y,x = x) head(sampledata_2) #sampledata_2の上数行だけを表示する。 ## y x ## 1 -1.2070657 1 ## 2 0.2774292 1 ## 3 1.0844412 0 ## 4 -2.3456977 1 ## 5 0.4291247 1 ## 6 0.5060559 1 result_3 = lm(data = sampledata_2, y ~ x) summary(result_3) ## ## Call: ## lm(formula = y ~ x, data = sampledata_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0686 -0.7651 -0.1993 0.6510 2.6929 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02642 0.14456 -0.183 0.855 ## x -0.25066 0.20047 -1.250 0.214 ## ## Residual standard error: 1.002 on 98 degrees of freedom ## Multiple R-squared: 0.0157, Adjusted R-squared: 0.005658 ## F-statistic: 1.563 on 1 and 98 DF, p-value: 0.2142 独立変数が一つで二値の場合，回帰係数の有意性検定の結果は，t検定を行った場合と一致する。つまり，独立変数が二値の単回帰分析でやっていることは，２群間の平均値の差の検定と同じである。その理由は，一般化線形モデルの回で説明する。 t.test(data = sampledata_2, y ~ x, paired = F, var.equal = T) #等分散を仮定した場合の検定。 ## ## Two Sample t-test ## ## data: y by x ## t = 1.2503, df = 98, p-value = 0.2142 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1471737 0.6484883 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.02641995 -0.27707724 練習問題 問１，問２いずれも宿題とする。なお，数値は小数点第３位まで報告せよ（小数点第４位以下は四捨五入）。 問１ Rにはattitudeというデータが入っている。ある金融機関の30部門で従業員に行ったアンケート調査の結果である。各部署ごとに，７つの質問項目について好意的な評価をした人の割合が示されている。 ちなみに，データのcomplaints, privileges, learning, raises, critical, advanceはそれぞれ，「従業員の不満への対応」，「特権を許さない」，「学習の機会」，「能力に応じた昇給」，「批判的すぎる」，「昇進」を意味する。 head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 ratingを従属変数，その他の変数（complaints, privileges, learning, raises, critical, advance）を独立変数とした重回帰分析を行い， 小問１ それぞれの独立変数の傾きの係数及び検定の結果を報告せよ（係数の値とp値だけでよい）。 ヒント：一般的に回帰係数の推定結果を論文などで報告するときは，(b=x.xxx, p=.xxx)とする。bは回帰係数を意味する。 小問２ 5%水準で有意な効果を持った独立変数を挙げ，それらの独立変数の増減によって従属変数がどう変化する傾向にあるかを報告せよ。 小問３ learningが１単位増えると，ratingはいくら変化するかを述べよ。 問２ 以下のプログラムを読み込み，サンプルデータ dat_q2 を作成する。 このデータには，A県とB県それぞれで10人の生徒を選んで学力テストを行った結果を示している（架空の調査である）。Prefectureが県，Value がテストの成績を意味する。 A = c(38, 53, 61, 27, 54, 55, 44, 45, 44, 41) B = c(55, 50, 52, 61, 70, 59, 55, 51, 52, 84) Value = c(A, B) Prefecture = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) dat_q2 = data.frame(Prefecture = Prefecture, Value = Value) dat_q2 ## Prefecture Value ## 1 A 38 ## 2 A 53 ## 3 A 61 ## 4 A 27 ## 5 A 54 ## 6 A 55 ## 7 A 44 ## 8 A 45 ## 9 A 44 ## 10 A 41 ## 11 B 55 ## 12 B 50 ## 13 B 52 ## 14 B 61 ## 15 B 70 ## 16 B 59 ## 17 B 55 ## 18 B 51 ## 19 B 52 ## 20 B 84 A県=1, B県=0のダミー変数を作り，そのダミー変数を独立変数，Value を従属変数とした回帰分析を行い， 小問１ A県=1を意味するダミー変数の傾きの推定値とp値を報告せよ。 ヒント：このダミー変数を作る場合には，以下のようにプログラムを書くと良い（今後のためにも覚えておくと良い）。 dat_q2$A = ifelse(dat_q2$Prefecture == &quot;A&quot;, 1, 0) #「もしPrefectureがAならば1，それ以外ならば0とする，新しい変数Aを作れ」という命令 ヒント：実はダミー変数を作らなくても，質的変数（i.e., Prefecture）を独立変数としてそのまま入れてもRは自動でダミー変数を作って計算してくれる。 小問２ t検定（等分散を仮定する）で，A県とB県の間で Value の平均値の比較を行い，A県とB県の間の平均値の差が5%水準で有意かどうかを述べよ（t値，自由度，p値も報告すること）。 ヒント：対応のあるt検定か対応のないt検定，どちらが適切か？ "],
["section-9.html", "Chapter 9 線形モデル 9.1 一般化線形モデル 9.2 一般化線形モデルと統計解析との関係 9.3 Rでの一般化線形モデルの方法 9.4 線形モデルに含まれる統計解析 練習問題", " Chapter 9 線形モデル 線形モデルについて学ぶ。 一般化線形モデル 線形モデル これまで学んできたいくつかの分析（t検定，分散分析，回帰分析）が，「線形モデル」という一つの枠組みで捉えることができることを理解する。 まず，一般化線形モデルの前に，線形モデルを学ぶ。線形モデルは，一般化線形モデルに含まれるものである。今後学ぶもの関係図を示すと，以下のようになる。 Figure: 一般化線形混合モデル，一般化線形モデル，線形モデルの関係図 9.1 一般化線形モデル 一般化線形モデルとは，データを線形の式で従属変数と独立変数の関係で表現したモデルのことをいう。 一般化線形モデルを理解するには，「線形予測子」，「誤差分布」，「リンク関数」の３つの要素を知っておく必要がある。 9.1.1 線形予測子 従属変数と独立変数を表した式。いわゆる，「回帰式」である。 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] xを独立変数（説明変数），yを従属変数（応答変数または目的変数）と呼ぶ。独立変数は1個でなくても構わない。また，独立変数は質的変数でも量的変数でも構わない（その両方でもOK）。 線形予測子から，従属変数の予測値(\\(\\hat{y}\\))を推定する。 9.1.2 誤差分布 従属変数の実際の値(\\(y\\))と従属変数の予測値(\\(\\hat{y}\\))の誤差が従う確率分布のことをいう。 9.1.2.1 確率分布の復習 確率分布の種類として，正規分布，二項分布，ポアソン分布などがあった。変数の種類によって，ある変数が従うだろうと仮定する確率分布が異なることを学んだ。 例えば，連続量ならば正規分布，二値の値ならば二項分布，カウントデータ（非負の離散値）ならばポアソン分布というように，変数の特徴によって仮定する確率分布が異なる。 9.1.3 リンク関数 線形予測子と従属変数との関係をリンクさせる関数。 恒等リンク（線形予測子を変形せずにそのまま利用すること），logリンク，logitリンク関数などがある（誤差分布によってどのリンク関数を指定するかは大体定まっている）。 恒等リンクの場合 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] logがリンク関数の場合 \\[ \\begin{equation} \\log\\hat{y} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] logitがリンク関数の場合 \\[ \\begin{equation} \\log\\frac{\\hat{y}}{1-\\hat{y}} = \\beta_{0} + \\beta_{1} x \\\\ \\end{equation} \\] 詳しくは次回以降で触れるが，線形予測子そのままだと，推定されるyの予測値は，-∞から∞の値をとり得る。これだと，従属変数の種類によっては不自然な場合もある（例えば従属変数が1個，2個といった頻度なのに-1個といった負の値が推定されてしまう，確率を推定したいのに，1.5や-1など0から1に収まらない値が推定されてしまうなど）。 そのため，リンク関数によって，推定される従属変数の予測値を正の値に限定したり（対数を求める），0から1の範囲に限定する（ロジットを取る）事を行う。 誤差分布とリンク関数については，次回で詳しく扱う。とりあえず今回は，一般化線形モデルはこれら3つの要素で構成されていると言うことを理解しておく。 9.2 一般化線形モデルと統計解析との関係 誤差分布やリンク関数の組み合わせによって，様々な解析を表現することが可能になる。 重回帰分析：誤差分布が正規分布，リンク関数には何も指定しない（線形式をそのまま使う） ロジスティック回帰：誤差分布が二項分布，リンク関数はロジット ポアソン回帰：誤差分布がポアソン分布，リンク関数はlog 対数線形分析：誤差分布がポアソン分布，リンク関数はlog（ポアソン回帰と同じだが，説明変数がカテゴリカルのみの場合） つまり，一般化線形モデルとは，ある特定の分析手法を指す言葉ではなく，「あらゆる統計解析を共通の枠組みから包括的に理解する統計解析の考え方」である。誤差分布やリンク関数をカスタマイズすることで，あらゆるデータの解析に対応させることができるといったイメージである。 今回は，「従属変数の誤差分布を正規分布，リンク関数には何も指定しない場合」に，どのような解析ができるかについて見ていく。 要は前回学んだ回帰分析の式をそのまま扱う。新しく学ぶことはなにもない。今回の目的は，「回帰分析の式により，t検定など他の分析も行うことができる」ことを理解することである。 「従属変数の誤差分布を正規分布，リンク関数には何も指定しない」場合の統計モデルのことを，「線形モデル(linear model)」と表現することもある。 正確には，「一般線形モデル（general linear model）」と呼ばれる事が多い。ただ，これだと一般”化”線形モデル（generalized linear model）と区別がつきにくいので，ここでは”線形モデル”と表現する。 9.3 Rでの一般化線形モデルの方法 Rには，一般化線形モデルで解析を行うためのglm()が用意されている。 前回の回帰分析の練習との時に用いたデータを使い，glm()で重回帰分析を行ってみる。 set.seed(1234) N = 100 a = 10 b1 = 3 b2 = 0.5 x1 = rnorm(n = N) x2 = rnorm(n = N) e = rnorm(n = N, sd = 5) y = b1*x1 + b2*x2 + a + e sample_data = data.frame(x1 = x1, x2 = x2, y = y) head(sample_data) #サンプルデータの上数行を表示 ## x1 x2 y ## 1 -1.2070657 0.41452353 9.012199 ## 2 0.2774292 -0.47471847 14.078772 ## 3 1.0844412 0.06599349 14.213890 ## 4 -2.3456977 -0.50247778 6.215336 ## 5 0.4291247 -0.82599859 12.432780 ## 6 0.5060559 0.16698928 15.403974 qplot(data = sample_data, x1, y) #x1とyの散布図を示す 今回のデータは正規分布に従うという前提を置くことにする。 \\[ \\begin{equation} \\hat{y} = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} \\\\ y \\sim Normal(\\hat{y}, \\sigma) \\end{equation} \\] まず，独立変数と切片及び傾きのパラメータから従属変数yの予測値を求める。予測値と実測値との間には誤差が生じると考えられる。その誤差は予測値を平均，\\(\\sigma\\)を標準偏差とする正規分布に従うだろうと考える。 result_glm = glm(data = sample_data, y ~ x1 + x2, family = gaussian(link = &quot;identity&quot;)) summary(result_glm) ## ## Call: ## glm(formula = y ~ x1 + x2, family = gaussian(link = &quot;identity&quot;), ## data = sample_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 23.15325) ## ## Null deviance: 3479.0 on 99 degrees of freedom ## Residual deviance: 2245.9 on 97 degrees of freedom ## AIC: 602.96 ## ## Number of Fisher Scoring iterations: 2 familyで指定しているのが誤差分布である。ここでは，gaussian（正規分布はガウス分布とも呼ばれる）としている。 linkで指定しているのが，リンク関数である。ここでは，identity（恒等リンク。要は何も変換しないで線形式そのまま使うという意味）。 なお，family及びlinkを特に何も指定しなければ，デフォルトでそれぞれgausian, identityとなる。 前回は重回帰分析を行うための関数はlm()であると学んだ。lm()を使った結果と比べてみよう。 result_lm = lm(data = sample_data, y ~ x1 + x2) summary(result_lm) ## ## Call: ## lm(formula = y ~ x1 + x2, data = sample_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.966 -3.367 0.530 2.912 13.825 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.8179 0.4874 22.196 &lt; 2e-16 *** ## x1 3.4026 0.4816 7.065 2.46e-10 *** ## x2 0.9416 0.4687 2.009 0.0473 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.812 on 97 degrees of freedom ## Multiple R-squared: 0.3544, Adjusted R-squared: 0.3411 ## F-statistic: 26.63 on 2 and 97 DF, p-value: 6.049e-10 全く同じ結果が出力されている。 lmはlinear modelの略で，lm()は線形モデル（誤差分布が正規分布で，リンク関数が恒等リンクの場合の一般化線形モデル）の解析を行うための関数である。 9.4 線形モデルに含まれる統計解析 線形モデルでできる解析は，重回帰分析に限らない。lm()の結果は説明変数がどのような種類かによって，いわゆる 独立変数が一つ以上で，独立変数が質的か量的変数かは問わない → 回帰分析 独立変数が一つで，独立変数が二値[0, 1]の変数 → t検定 独立変数が一つ以上で，独立変数がすべて質的変数 → 分散分析 更に共変量を加える → 共分散分析 と呼ばれる統計解析と対応する。 9.4.1 重回帰分析 既に学んだので省略。 9.4.2 t検定 二群間の差の検定を，対応のないt検定を行った場合と群をダミー変数とした単回帰分析の結果は一致する。 set.seed(1234) y = rnorm(100) x = round(runif(100),0) dat_t = data.frame(y = y, x = x) t.test(data = dat_t, y ~ x, paired = F, var.equal = T) #等分散を仮定する ## ## Two Sample t-test ## ## data: y by x ## t = 1.2503, df = 98, p-value = 0.2142 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.1471737 0.6484883 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.02641995 -0.27707724 summary(lm(data = dat_t, y ~ x)) ## ## Call: ## lm(formula = y ~ x, data = dat_t) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0686 -0.7651 -0.1993 0.6510 2.6929 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02642 0.14456 -0.183 0.855 ## x -0.25066 0.20047 -1.250 0.214 ## ## Residual standard error: 1.002 on 98 degrees of freedom ## Multiple R-squared: 0.0157, Adjusted R-squared: 0.005658 ## F-statistic: 1.563 on 1 and 98 DF, p-value: 0.2142 回帰分析の傾きの検定は，「傾きがゼロである」という帰無仮説の検定をしている。 傾きの係数が意味することは，独立変数xが1単位増えたときのyの変化量であった。傾きの検定は，「x=0 から x=1 に変化することによって， y が上昇（下降）するか（傾きがゼロではないか）」を検定している。 すなわち，「x=0とx=1の間にyの値に差があるかを検定している」のと同じである。 * t検定も，測定値の誤差が正規分布に従うという前提が置かれている。 このように，2群間の対応のないt検定も線形モデルの中に含まれる。 ただし，対応のあるt検定の場合は線形モデルでは表現できない。これについては，「一般化線形混合モデル」の回で扱う。 9.4.3 分散分析 分散分析は，独立変数が質的変数の場合（ダミー変数に変換する必要あり）の線形モデルである。 Rで分散分析をしたい場合は，lm()とanova()を使えば良い。各変数の主効果・交互作用効果について，F統計量が出力される。 9.4.3.1 1要因の分散分析 #サンプルデータ set.seed(1234) y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;) #要因1：A,かBかC dat = data.frame(y = y, x1 = x1) result_anova = aov(data = dat, y ~ x1) summary(result_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 2 3.972 1.9861 3.374 0.0806 . ## Residuals 9 5.298 0.5886 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 result_anova = lm(data = dat, y ~ x1) anova(result_anova) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 2 3.9721 1.98607 3.3741 0.08064 . ## Residuals 9 5.2976 0.58863 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(result_anova) ## ## Call: ## lm(formula = y ~ x1, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0913 -0.4933 0.2020 0.5015 1.0775 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.2544 0.3836 -3.270 0.00968 ** ## x1B 1.1751 0.5425 2.166 0.05849 . ## x1C 1.2613 0.5425 2.325 0.04512 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7672 on 9 degrees of freedom ## Multiple R-squared: 0.4285, Adjusted R-squared: 0.3015 ## F-statistic: 3.374 on 2 and 9 DF, p-value: 0.08064 lm()は変数が文字列でも，自動でダミー変数化してくれる。今回の結果で言うと，X1BとX1Cという変数が自動で作られており，要因1がAのときはX1B=0かつX1C=0，BのときはX1B=1かつX1C=0，CのときはX1B=0かつX1C=1となっている（ダミー変数の個数は「水準の数-1」）。 線形モデルの結果のsummaryを見てみると，Bのダミー変数の係数が正でかつ有意であった。このことから，条件Bにおいてyの値が顕著に高い傾向にあることがわかる。 ただし，これでわかるのはB条件が従属変数に及ぼす効果の強さのみである。BとAの間，あるいはBとCの間に有意な差があるかはわからない。条件間の差に興味があるのならば，多重比較を行う必要がある。 このように，単に各独立変数が従属変数に及ぼす効果を見たいときは，線形モデルの方が結果を直感的に理解しやすい。 ただし，自動で変数名がつけられて混乱するので，自分でダミー変数を作り直したほうが良い。 9.4.3.2 2要因以上の分散分析 独立変数の主効果だけではなく交互作用も考える。分散分析は独立変数（要因）の数が増えると複雑になるが，線形モデルならばモデルを立てるのも，結果を解釈するのもわかりやすい。 #サンプルデータ: 2x2の2要因配置 set.seed(1234) y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;) #要因1：A,かB x2 = c(&quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;) #要因2:MかF dat_anova = data.frame(y = y, x1 = x1, x2 = x2) #ダミー変数を作る dat_anova$x1_A = ifelse(dat_anova$x1 == &quot;A&quot;, 1, 0) dat_anova$x2_M = ifelse(dat_anova$x2 == &quot;M&quot;, 1, 0) dat_anova ## y x1 x2 x1_A x2_M ## 1 -1.2070657 A M 1 1 ## 2 0.2774292 B M 0 1 ## 3 1.0844412 A M 1 1 ## 4 -2.3456977 B M 0 1 ## 5 0.4291247 A M 1 1 ## 6 0.5060559 B M 0 1 ## 7 -0.5747400 A F 1 0 ## 8 -0.5466319 B F 0 0 ## 9 -0.5644520 A F 1 0 ## 10 -0.8900378 B F 0 0 ## 11 -0.4771927 A F 1 0 ## 12 -0.9983864 C F 0 0 result_anova2 = lm(data = dat_anova, y ~ x1_A + x2_M + x1_A:x2_M) #lm(data=dat_anova, y ~ x1_A*x2_M) #これでも同じ summary(result_anova2) ## ## Call: ## lm(formula = y ~ x1_A + x2_M + x1_A:x2_M, data = dat_anova) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.82496 -0.10544 0.01797 0.44476 1.02679 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.8117 0.5746 -1.413 0.195 ## x1_A 0.2729 0.8126 0.336 0.746 ## x2_M 0.2909 0.8126 0.358 0.730 ## x1_A:x2_M 0.3500 1.1493 0.305 0.768 ## ## Residual standard error: 0.9953 on 8 degrees of freedom ## Multiple R-squared: 0.1451, Adjusted R-squared: -0.1755 ## F-statistic: 0.4526 on 3 and 8 DF, p-value: 0.7226 anova(result_anova2) #分散分析表が出力される ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1_A 1 0.6018 0.60184 0.6076 0.4581 ## x2_M 1 0.6513 0.65134 0.6575 0.4409 ## x1_A:x2_M 1 0.0919 0.09188 0.0928 0.7685 ## Residuals 8 7.9247 0.99059 交互作用は独立変数同士をかけ合わせた項で表現される。 Rのlm()ならば，x1_A:x2_Mが交互作用項を意味する。x1_A*x2_Mとすれば，主効果と交互作用効果の項の両方が自動で入る。 ただし，ここで行っているb分散分析は，要因がどれも「対応なし」の場合である。「対応あり」の要因が含まれる分散分析は，lm関数ではできない。このような場合どうするかは，「一般化線形混合モデル」の回で扱う。 9.4.4 共分散分析 例えば，ある大学Aと大学Bで，体重測定を行ったとする。その結果，大学Aの学生の方が大学Bの学生よりも平均体重が大きかった。しかし，もしかしたら大学Aには身長が高い学生が多く，それにより体重にも違いが生じてしまったのかもしれない（体重と身長は正の相関関係にある）。 共分散分析（ANCOVA: analysis of covariance）は，データに影響を及ぼす別の変数の影響を統制した上で平均値の群間比較を行う分散分析である。要因に影響を及ぼす別の変数のことを，共変量と呼ぶ。 要は重回帰分析である。重回帰分析の利点の１つとして，他の変数の影響を統制した上での，ある独立変数が従属変数に及ぼす効果を見れる点があると学んだ。共分散分析では，共変量の効果を統制した上で，注目する独立変数の効果を見ている。 #サンプルデータ # 2x2の2要因配置 y = rnorm(12) x1 = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;) #要因1：A,かB x2 = rnorm(12) #共変量 dat_ancova = data.frame(y=y, x1=x1, x2=x2) result_ancova = lm(data = dat_ancova, y ~ x1 + x2) summary(result_ancova) ## ## Call: ## lm(formula = y ~ x1 + x2, data = dat_ancova) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0912 -0.5103 -0.1350 0.2195 2.1292 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1923 0.4395 -0.438 0.673 ## x1B 0.5891 0.6970 0.845 0.423 ## x1C 0.9224 1.2062 0.765 0.466 ## x2 0.2316 0.4456 0.520 0.617 ## ## Residual standard error: 1.047 on 8 degrees of freedom ## Multiple R-squared: 0.1074, Adjusted R-squared: -0.2273 ## F-statistic: 0.3208 on 3 and 8 DF, p-value: 0.8104 次回は更に，誤差分布及びリンク関数を別の物に変えた時に，どのような解析ができるのかについて検討する。 練習問題 問１は宿題にしない。問２を宿題とする。小数点第３位まで報告すること（小数点第４位以下は四捨五入）。 問１ 一般化線形モデルを構成する「線形予測子」，「誤差分布」，「リンク関数」それぞれを説明せよ。 問２ 以下のプログラムを読み込み，サンプルデータを作成する。 Height = c(149.9, 164.8, 172.8, 138.5, 166.3, 167.1, 156.3, 156.5, 156.4, 153.1, 145.2, 140, 142.2, 150.6, 159.6, 148.9, 144.9, 140.9, 141.6, 174.2) Class = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;,10)) A = c(rep(1, 10), rep(0,10)) Weight = c(56.3, 50.1, 50.6, 59.6, 48.1, 40.5, 60.7, 44.8, 54.8, 45.6, 66, 50.2, 47.9, 50, 38.7, 43.3, 33.2, 41.6, 52.1, 50.3) sample_data = data.frame(Height = Height, Class = Class, A = A, Weight = Weight) sample_data ## Height Class A Weight ## 1 149.9 A 1 56.3 ## 2 164.8 A 1 50.1 ## 3 172.8 A 1 50.6 ## 4 138.5 A 1 59.6 ## 5 166.3 A 1 48.1 ## 6 167.1 A 1 40.5 ## 7 156.3 A 1 60.7 ## 8 156.5 A 1 44.8 ## 9 156.4 A 1 54.8 ## 10 153.1 A 1 45.6 ## 11 145.2 B 0 66.0 ## 12 140.0 B 0 50.2 ## 13 142.2 B 0 47.9 ## 14 150.6 B 0 50.0 ## 15 159.6 B 0 38.7 ## 16 148.9 B 0 43.3 ## 17 144.9 B 0 33.2 ## 18 140.9 B 0 41.6 ## 19 141.6 B 0 52.1 ## 20 174.2 B 0 50.3 ある学校でAクラスとBクラスの二つのクラスで，生徒の身長と体重の測定を行ったとする。それぞれ，身長（Height），クラス(Class: A or B)，体重（Weight）が保存されている。更に，クラスAか否かを意味するダミー変数（A: 1=クラスA, 0=クラスB）もある。 このデータを使って，一般化線形モデルで身長を従属変数，体重とクラスを独立変数とした分析を行え。ただし，従属変数の誤差分布は正規分布に従うとし，リンク関数は恒等リンク関数とすること。 ヒント：つまり，「重回帰分析を行え」ということ。 結果を報告せよ（独立変数の係数及びp値を報告すること）。また，分析の結果から，独立変数の増減が従属変数にどう影響を及ぼす傾向にあるかを述べよ。 "],
["-1.html", "Chapter 10 一般化線形モデル 10.1 一般化線形モデル（再掲） 10.2 今回の予定 10.3 ロジスティック回帰 10.4 最尤法（＊） 10.5 ポアソン回帰分析（＊） 10.6 モデルの予測力 練習問題 参考文献", " Chapter 10 一般化線形モデル 誤差分布が正規分布以外の場合の「一般化線形モデル」について学ぶ。 ロジスティック回帰 最尤法 ポアソン回帰 install.packages(&quot;tidyverse&quot;, &quot;MASS&quot;) library(tidyverse) library(MASS) 10.1 一般化線形モデル（再掲） 一般化線形モデルとは，線形の式で従属変数と独立変数の関係を表した統計モデルのことをいう。一般化線形モデルを理解するには，以下の３つの要素を知っておく必要がある。 10.1.1 線形予測子 従属変数と独立変数を表した式。いわゆる，「回帰式」である。 \\[ \\hat{y} = \\beta_{0} + \\beta_{1} x \\tag{1}\\\\ \\] xを独立変数（説明変数），yを従属変数（応答変数または目的変数）と呼ぶ。独立変数は1個でなくても構わない。また，独立変数は質的変数でも量的変数でも構わない（その両方でもOK）。 線形予測子から，従属変数の予測値(\\(\\hat{y}\\))を推定する。 10.1.2 誤差分布 従属変数の実際の値(\\(y\\))と従属変数の予測値(\\(\\hat{y}\\))の誤差が従う確率分布のことをいう。 確率分布の復習 正規分布，二項分布，ポアソン分布などがあった。変数の種類によって，ある変数が従うだろうと仮定する確率分布が異なることを学んだ。 例えば，連続量ならば正規分布，二値の値ならば二項分布，カウントデータ（非負の離散値）ならばポアソン分布というように，変数の特徴によって仮定する確率分布が異なる。 10.1.3 リンク関数 線形予測子と従属変数との関係をリンクさせる関数。 恒等リンク（線形予測子を変形せずにそのまま利用すること），logリンク，logitリンク関数などがある（誤差分布によってどのリンク関数を指定するかは大体定まっている）。 10.1.4 一般化線形モデルと統計解析との関係 誤差分布やリンク関数の組み合わせによって，様々な解析を表現することが可能になる。 重回帰分析：誤差分布が正規分布，リンク関数には何も指定しない（線形式をそのまま使う） ロジスティック回帰：誤差分布が二項分布，リンク関数はロジット ポアソン回帰：誤差分布がポアソン分布，リンク関数はlog 対数線形分析：誤差分布がポアソン分布，リンク関数はlog（ポアソン回帰と同じだが，説明変数がカテゴリカルのみの場合） つまり，一般化線形モデルとはある分析手法を指す言葉というよりも，「あらゆる統計解析を共通の枠組みから包括的に理解する統計解析の考え方」といえる。 10.2 今回の予定 前回は，誤差分布が正規分布である場合の解析（回帰分析，t検定，分散分析など）を扱った。今回は，誤差分布として正規分布以外を仮定する場合の解析について扱う。 Figure: 一般化線形混合モデル，一般化線形モデル，線形モデルの関係図 10.3 ロジスティック回帰 ロジスティック回帰は，「ある事象が生じる確率を予測する回帰分析」である。誤差分布が二項分布に従う場合において，二項分布のパラメータq（ある事象が生じる確率）を独立変数から推定する回帰分析である。 10.3.1 二項分布 \\[ P(r) = {}_n\\mathrm{C}_rq^{r}(1-q)^{(n-r)} \\tag{2} \\] qはある事象が生じる確率（投げたコインが表の確率など），nがすべての試行数，rがある事象が生じた回数を意味する。 n=1のときは，「ベルヌーイ分布」と呼ばれる（例えば誰か1人を選んだときに，その人が病気であるか否かなど）。 二項分布は，生じる事象が2つのカテゴリに分けられる場合に当てはまる確率分布である。コインを投げたときに出る面が「表か裏」か，学生の中から選んだ人の性別が「男か女」か，ある意見について「賛成か反対」かなど。このような事象が生じる確率は，理論的には二項分布に従う。 10.3.2 例題 例えば次のようなデータを考えてみる。MASSパッケージに入っている生検の結果のサンプルデータを用いる。 library(MASS) b &lt;- biopsy b$classn[b$class==&quot;benign&quot;] &lt;- 0 b$classn[b$class==&quot;malignant&quot;] &lt;- 1 head(b) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class classn ## 1 1000025 5 1 1 1 2 1 3 1 1 benign 0 ## 2 1002945 5 4 4 5 7 10 3 2 1 benign 0 ## 3 1015425 3 1 1 1 2 2 3 1 1 benign 0 ## 4 1016277 6 8 8 1 3 4 3 7 1 benign 0 ## 5 1017023 4 1 1 3 2 1 3 1 1 benign 0 ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 1 V1は整数の変数，classnは1ならば癌，0ならば癌ではないことを意味する変数とする。 まず，独立変数V1と従属変数classnとの関係をプロットする。 p1 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + xlab(&quot;V1&quot;) + ylab(&quot;classn&quot;) p1 これに回帰分析のモデルを当てはめる。 fit_lm = lm(data=b, classn ~ V1) summary(fit_lm) ## ## Call: ## lm(formula = classn ~ V1, data = b) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.77804 -0.17331 -0.01994 0.06859 1.06859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.189535 0.023395 -8.102 2.43e-15 *** ## V1 0.120947 0.004467 27.078 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3323 on 697 degrees of freedom ## Multiple R-squared: 0.5127, Adjusted R-squared: 0.512 ## F-statistic: 733.2 on 1 and 697 DF, p-value: &lt; 2.2e-16 p2 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + stat_smooth(method=lm, se=F, size=2) + xlab(&quot;x&quot;) + ylab(&quot;q&quot;) p2 推定された回帰式をプロット上に当てはめる。 がんにかかるリスク（確率）を推定したいが，例えばxが10を超えると，従属変数の予測値として1以上の値が推定されてしまう。xが2.5を下回ったときも，0未満の数値が推定されてしまう。従属変数は0か1しか取らない変数なのに，それぞれを超える値が予測されてしまう。これは確率の推定としては不都合である。 そこで，この直線を以下のように変換してみたらどうだろうか。 p3 = ggplot(b, aes(x=V1, y=classn)) + geom_point(position=position_jitter(width=0.3, height=0.06), alpha=0.8, shape=21, size=3) + stat_smooth(method=glm, method.args = list(family = &quot;binomial&quot;),se=F, size=2) + xlab(&quot;x&quot;) + ylab(&quot;q&quot;) p3 この予測線は，以下の式で作成されたものである。 \\[ q_{i} = \\frac{1}{1+\\exp[-(\\beta_{0} + \\beta_{1}x_{i})]} \\tag{3} \\] qは「y=1が生じる確率」を意味する。\\(1/(1+\\exp(-y))\\)は，ロジスティック関数と呼ばれる。 xがどのような値をとっても，\\(q_{i}\\)は\\(0\\leq q \\leq1\\)となる。 更に，式(3)は，以下の式(4)に変換することができる（理由は付録参照）。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] 左辺のことを，ロジット関数（logit function）という。つまり，線形の式をロジット関数でリンクさせたものがロジスティック回帰である（リンク関数を”ロジット”とする理由）。 まとめると，ロジスティック回帰は誤差分布が二項分布，リンク関数をロジット関数とした一般化線形モデルのことをいう。 10.3.3 まとめ ロジスティック回帰は，独立変数がある事象が生じる確率に及ぼす影響を推定したいときに使う回帰分析である。従属変数が0か1を取る変数の場合には，ロジスティック回帰を行う。 一般化線形モデルの誤差分布を二項分布，リンク関数をロジット関数としたものが，ロジスティック回帰に相当する。 10.3.4 Rでのロジスティック回帰 glm()関数で行う。familyにbinomial, linkにlogitを設定すれば良い。先程のサンプルデータbを使って，glm()関数でロジスティック回帰を行ってみる。 fit_logistic = glm(data=b, classn ~ V1, family = binomial(link=&quot;logit&quot;)) summary(fit_logistic) ## ## Call: ## glm(formula = classn ~ V1, family = binomial(link = &quot;logit&quot;), ## data = b) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## V1 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 (link=“logit”)は省略しても良い。familyにbinomialを設定すれば，リンク関数はデフォルトでlogitとなる。 結果の見方は，重回帰と同じである。 [Coefficients]の部分を見る。Estimateが係数。更にその効果のp値が表示される。係数の推定値の解釈は重回帰分析のときと同じで，プラスならば従属変数が1の値，マイナスならば従属変数が0の値を取りやすいことを意味する。 10.4 最尤法（＊） 線形回帰のときは，切片及び傾きの値は最小二乗法で求めると学んだ。ロジスティック回帰の場合はどうなのだろうか？ 一般化線形モデルでは，最尤法（さいゆうほう）と呼ばれる手法を通してパラメータの推定を行う。 表が出る確率が未知のコインを何回か投げて，表が出る確率\\(\\theta\\)（シータ）を推定するとする。 1回目は，表が出た。この実験結果が生じる確率は\\(\\theta\\)である。 2回目は，裏が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)\\)である。 3回目は，表が出た。ここまでの実験結果が生じる確率は\\(\\theta(1-\\theta)*\\theta\\)である。 その後，4回目は裏，5回目は裏だったとする。この実験結果が生じる確率は以下のように表すことができる。 \\[ L = (1-\\theta)^3 \\theta^2 \\tag{5} \\] Lのことを尤度(likelihood)と呼ぶ（”ゆうど”と読む。”いぬど”ではない）。 尤度とは「もっともらしさ」を示すものである。イメージとしては，「今回の実験結果が得られる確率」である。今回の観測データに対して最も当てはまりが良くなる，すなわち最も尤度の高いパラメータを求めるのが，最尤法と呼ばれる手法である。 掛け算を扱う尤度は計算が困難なので，対数化して足し算にする。対数化した尤度を対数尤度と呼ぶ。対数尤度が最大となるパラメータを求めるのが，最尤法である。 \\[ \\log L = \\log(1-\\theta)+\\log(1-\\theta)+\\log(1-\\theta)+\\log(\\theta)+\\log(\\theta) \\tag{6} \\] パラメータ\\(\\theta\\)と対数尤度\\(\\log L\\)との関係を以下に示す。 対数尤度が最も大きくなるのは，\\(\\theta= 0.40\\) のときである（2/5と一致）。 一般化線形モデルでは，最尤法で対数尤度の高くなる切片及び傾きを推定する（上の例の\\(\\theta\\)を線形予測子に置き換えて同様の計算をする）。 なお，回帰分析で切片及び傾きを求めるときに最小二乗法を使うと前回説明したが，最尤法でも同じ値が推定される（証明は省略）。 10.5 ポアソン回帰分析（＊） 従属変数がカウントデータ（非負の整数。1個，2個,3個といった個数など）の場合は，理論的にはポアソン回帰分析をするのが適切である。 確率分布はポアソン分布(poisson)，リンク関数はlogを指定する。 10.5.1 ポアソン分布（＊） \\[ P(y) = \\frac{\\lambda^y\\exp(-\\lambda)}{y!} \\tag{7}\\\\ \\] yは0以上の整数（0, 1, 2, 3, …）とする。P(y)はyが生じる確率。 ポアソン分布のパラメータは\\(\\lambda\\)だけである。期待値（平均）は\\(\\lambda\\)，分散は\\(\\lambda\\)である。つまり，平均と分散が等しい分布である。 10.5.2 ポアソン回帰（＊） \\[ \\log\\lambda_{i}=\\beta_{0}+\\beta_{1}x_{i} \\tag{8}\\\\ \\] \\[ \\lambda_{i}=\\exp(\\beta_{0}+\\beta_{1}x_{i}) \\tag{9}\\\\ \\] ポアソン回帰では，誤差分布をポアソン分布，リンク関数を対数（log）に設定する。 10.5.3 例題（＊） 嶋田・阿部(2017)「Rで学ぶ統計学入門」 p. 167の演習問題（改題）。 鳥種Aの雄はときどき高い柱にとまって，自分の縄張りを見張る行動を示す。鳥の体重(wt)と見張り行動の発生数（mihari）を調べた。体重が見張り行動に及ぼす影響を調べる。 # サンプルデータ（嶋田・阿部(2017) p. 167より） mihari = c(7, 3, 3, 5, 10, 8, 3, 1, 2, 9, 8) wt = c(110, 88, 80, 95, 120, 103, 97, 84, 91, 114, 107) data_poisson &lt;- data.frame(mihari = mihari, wt = wt) head(data_poisson) ## mihari wt ## 1 7 110 ## 2 3 88 ## 3 3 80 ## 4 5 95 ## 5 10 120 ## 6 8 103 # family にpoisson，linkにlogを指定する。 result_poisson &lt;- glm(data = data_poisson, mihari ~ wt, family = poisson (link = &quot;log&quot;)) summary(result_poisson) ## ## Call: ## glm(formula = mihari ~ wt, family = poisson(link = &quot;log&quot;), data = data_poisson) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.06361 -0.55141 0.04139 0.52133 0.97431 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67643 1.17805 -2.272 0.023092 * ## wt 0.04263 0.01112 3.833 0.000127 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 19.8308 on 10 degrees of freedom ## Residual deviance: 4.2671 on 9 degrees of freedom ## AIC: 45.232 ## ## Number of Fisher Scoring iterations: 4 10.6 モデルの予測力 10.6.1 決定係数 回帰分析では，データに対する回帰分析のモデルの予測力を表す指標として，決定係数（R-squared）というものがある。 model_lm = lm(data=iris, Sepal.Length ~ Petal.Width) summary(model_lm) ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.38822 -0.29358 -0.04393 0.26429 1.34521 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.77763 0.07293 65.51 &lt;2e-16 *** ## Petal.Width 0.88858 0.05137 17.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.478 on 148 degrees of freedom ## Multiple R-squared: 0.669, Adjusted R-squared: 0.6668 ## F-statistic: 299.2 on 1 and 148 DF, p-value: &lt; 2.2e-16 summary(model_lm)$r.squared #r.squaredで決定係数のみを取り出すことができる。 ## [1] 0.6690277 これは，回帰式から求めた予測値と実測値の分散が，実際のデータの分散に占める割合を意味する指標である。つまり，回帰分析でどれだけ全データの分散を説明できているかを意味する。 \\[ R^2 = \\sum_{i=1}^{n} \\frac {(y_{i}-\\hat{y}_{i})^2}{(y_{i}-\\bar{y})^2} \\tag{10} \\] ただし，決定係数は単純に，独立変数が増えるほど大きくなる（説明できる分散の量が増える）。 model_lm2 = lm(data=iris, Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width) summary(model_lm2)$r.squared ## [1] 0.8586117 仮に独立変数に影響を及ぼさない変数を含めても，モデルのデータに対する説明力は上昇してしまう。 複雑なモデルは現在のデータによく当てはまるのは，ある意味当然である（単なる偶然で生じた誤差も無駄に説明してしまっている）。しかし，複雑なモデルは現在のデータに当てはまっても，未知のデータにもうまく当てはまるとは限らない（overfittingと呼ばれる問題）。 良いモデルとは，「予測力が高く，かつできるだけシンプルである」ことである。 決定係数は上記の目的をかなえるのに常に適切な指標であるとは言えない。 10.6.2 赤池情報量基準（AIC） 変数の少なさとモデルの予測力の高さのバランスを取った指標の一つとして，AIC(Akaike inoformation criteria)が知られている。AICは以下の式で計算される。 \\[ AIC = -2 \\log L + 2k \\tag{11}\\\\ \\] \\(\\log L\\)は最大対数尤度，kはパラメータ（独立変数）の数である。 AICの値が低いほど，データに対するモデルの予測力が高いと評価する。 AICは余計なパラメータが多くなる（kが大きくなる）ほど大きい値を取る。つまり，データをうまく予測しつつ，かつパラメータ数を抑えてシンプルなモデルを探ることにかなっている。 Rではglm関数でAICの結果も出力してくれる。 model=glm(data=b, classn ~ V1+V2+V3+V4+V5+V6+V7+V8+V9, family=binomial(link=&quot;logit&quot;)) summary(model) ## ## Call: ## glm(formula = classn ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + ## V9, family = binomial(link = &quot;logit&quot;), data = b) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4841 -0.1153 -0.0619 0.0222 2.4698 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.10394 1.17488 -8.600 &lt; 2e-16 *** ## V1 0.53501 0.14202 3.767 0.000165 *** ## V2 -0.00628 0.20908 -0.030 0.976039 ## V3 0.32271 0.23060 1.399 0.161688 ## V4 0.33064 0.12345 2.678 0.007400 ** ## V5 0.09663 0.15659 0.617 0.537159 ## V6 0.38303 0.09384 4.082 4.47e-05 *** ## V7 0.44719 0.17138 2.609 0.009073 ** ## V8 0.21303 0.11287 1.887 0.059115 . ## V9 0.53484 0.32877 1.627 0.103788 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 884.35 on 682 degrees of freedom ## Residual deviance: 102.89 on 673 degrees of freedom ## (16 observations deleted due to missingness) ## AIC: 122.89 ## ## Number of Fisher Scoring iterations: 8 10.6.3 モデル選択（＊） 最も予測力の高いモデルを探す手続きを「モデル選択（model selection）」という。 MASSパッケージに入っているstepAICという関数を使ってAICが最も低いモデルの探索を行うことができる（stepwise法）。 MASS::stepAIC(glm(data=b, classn ~ V1+V2+V3+V4+V5+V6+V7+V8+V9, family=binomial(link=&quot;logit&quot;))) ## Start: AIC=122.89 ## classn ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## - V2 1 102.89 120.89 ## - V5 1 103.27 121.27 ## - V3 1 104.74 122.74 ## &lt;none&gt; 102.89 122.89 ## - V9 1 106.61 124.61 ## - V8 1 106.66 124.66 ## - V4 1 110.31 128.31 ## - V7 1 110.33 128.33 ## - V1 1 120.72 138.72 ## - V6 1 122.07 140.07 ## ## Step: AIC=120.89 ## classn ~ V1 + V3 + V4 + V5 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## - V5 1 103.27 119.27 ## &lt;none&gt; 102.89 120.89 ## - V9 1 106.66 122.66 ## - V3 1 106.66 122.66 ## - V8 1 106.76 122.76 ## - V4 1 110.64 126.64 ## - V7 1 110.70 126.70 ## - V1 1 121.10 137.10 ## - V6 1 122.07 138.07 ## ## Step: AIC=119.27 ## classn ~ V1 + V3 + V4 + V6 + V7 + V8 + V9 ## ## Df Deviance AIC ## &lt;none&gt; 103.27 119.27 ## - V9 1 107.14 121.14 ## - V8 1 107.72 121.72 ## - V3 1 107.90 121.90 ## - V7 1 111.69 125.69 ## - V4 1 112.17 126.17 ## - V1 1 121.55 135.55 ## - V6 1 123.15 137.15 ## ## Call: glm(formula = classn ~ V1 + V3 + V4 + V6 + V7 + V8 + V9, family = binomial(link = &quot;logit&quot;), ## data = b) ## ## Coefficients: ## (Intercept) V1 V3 V4 V6 ## -9.9828 0.5340 0.3453 0.3425 0.3883 ## V7 V8 V9 ## 0.4619 0.2261 0.5312 ## ## Degrees of Freedom: 682 Total (i.e. Null); 675 Residual ## (16 observations deleted due to missingness) ## Null Deviance: 884.4 ## Residual Deviance: 103.3 AIC: 119.3 しかし，データに含まれるすべての変数を対象にしてstepwiseで最適なモデルを探して，出てきた結果を鵜呑みにするのは，あまり良い方法とは言えない。解析の前に研究者自身が仮説を立て，関連するであろう変数をモデルに入れる作業を繰り返してモデルを選択するのが仮説検証型の研究としてふさわしいメソッドである。 他にもモデル選択に使われる指標として，BIC（ベイズ情報量基準）などもある。 練習問題 宿題にはしない。 問１ 以下のプログラムを実行し，サンプルデータを作成する。 変数の意味は以下の通りである。 Disease: ある病気にかかっているか（1=かかっている，0=かかっていない） BMI: BMI（肥満度を表す指標） Exercise: 1週間あたりの運動時間（単位：時間） Sleep: 1日の睡眠時間（単位：時間） Disease = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) BMI = c(15, 16, 16, 18, 19, 20, 21, 22, 22, 23, 23, 23, 24, 24, 24, 30, 31, 31, 33, 34, 34, 34, 35, 36, 40, 40, 40, 41, 43, 43) Exercise = c(2, 1, 1, 2, 0, 3, 1, 1, 4, 4, 2, 3, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 0, 2, 2, 3, 0, 4) Sleep = c(7, 4, 5, 4, 4, 6, 5, 6, 4, 6, 4, 7, 4, 7, 4, 6, 5, 4, 5, 6, 7, 5, 4, 6, 4, 7, 5, 5, 4, 7) data_q01 &lt;- data.frame(Disease = Disease, BMI = BMI, Exercise = Exercise, Sleep = Sleep) 小問１ Diseaseを従属変数，BMI，Exercise，Sleepを独立変数としたロジスティック回帰を行い，結果について報告せよ。 ヒント: 重回帰のときと同じく，回帰係数とp値を報告すれば良い（b=x.xxx, p = .xxx）。有意な効果があった場合は，その係数の増減によって従属変数がどう影響するかを述べる。 小問２ BMIのみを独立変数とした場合(model 1)，BMIとExerciseを独立変数とした場合(model 2)，BMI, Exercise, 及びSleepを独立変数とした場合(model 3)について，いずれのモデルの方がデータに対する説明力が高いかを，それぞれのモデルのAICを報告した上で述べよ。 参考文献 久保拓弥 (2012) データ解析のための統計モデリング入門 岩波書店 嶋田正和・阿部真人 (2017) Rで学ぶ統計学入門 東京化学同人 "],
["section-11.html", "Chapter 11 一般化線形混合モデル 11.1 個人差や集団差の影響 11.2 一般化線形混合モデル 11.3 Rでの一般化線形混合モデル 11.4 一般化線型混合モデルの拡張（ロジスティック回帰） 練習問題 参考文献", " Chapter 11 一般化線形混合モデル 一般化線形混合モデルについて学ぶ。 個人差や集団差の影響 一般化線形混合モデルとは？ Rでの実践 install.packages(&quot;tidyverse&quot;) install.packages(&quot;lme4&quot;) install.packages(&quot;lmerTest&quot;) library(tidyverse) library(lme4) library(lmerTest) 一般化線形モデル（genelarized linear model: GLM）は，回帰分析やロジスティック回帰分析など，様々な統計分析を行うことのできる統計モデルであった。 一般化線形混合モデル（generalized linear mixed model: GLMM）は，一般化線形モデルを更に拡張させたものである。 11.1 個人差や集団差の影響 以下では，Rにデフォルトで入っている iris データを例として使う。irisデータには，3種類のあやめの種（Speicies: setosa, versicolor, virginica）ごとに，がくの長さ（Sepal.Length）やがくの幅（Sepal.Width）などのデータが入っている。 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa まず，がくの長さ（Sepal.Length）とがくの幅（Sepal.Width）の関係を散布図で示してみよう。 graph_1 = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point(size = 3) + theme_bw() graph_1 がくの幅を従属変数，がくの長さを独立変数とした回帰分析をしてみる。 iris_lm = lm(data = iris, Sepal.Width ~ Sepal.Length) summary(iris_lm) ## ## Call: ## lm(formula = Sepal.Width ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1095 -0.2454 -0.0167 0.2763 1.3338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.41895 0.25356 13.48 &lt;2e-16 *** ## Sepal.Length -0.06188 0.04297 -1.44 0.152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4343 on 148 degrees of freedom ## Multiple R-squared: 0.01382, Adjusted R-squared: 0.007159 ## F-statistic: 2.074 on 1 and 148 DF, p-value: 0.1519 がくの長さ（Sepal.Length）は，がくの幅に対して負の影響を持っていることがわかる。回帰式の直線を引くと，以下のようになる。 graph_lm = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point(size = 3) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() graph_lm では，先程の散布図を種（Species）ごとに色をわけて示してみる。 graph_2 = ggplot(data=iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species, shape = Species)) + geom_point(size = 3) + theme_bw() graph_2 種を無視して検討したところ，がくの長さとがくの幅の間には負の相関関係があるように思えたが，実際にはどの種でもがくの長さが大きくなるほど，がくの幅が大きくなる関係にあるように見える。 このあやめのデータのように，いくつかのデータが同じグループに属している構造の場合，グループの影響を統制しないと誤った結論を招いてしまう恐れがある。それらのデータ間には，統計的独立性が保証されていないためである。つまり，同じ種同士のものは似た傾向にある可能性が高い（データ間で相関が存在する）。 独立とは，各データが他のデータに影響されないという意味である。これまで学んできた確率分布では，統計的独立性が前提とされている。例えば，コインを数回投げて投げて表が出る回数は二項分布に従うと学んだが，表が出るかどうかは前の試行に影響されることはない（前回表が出たら次も表が出やすいということは起こりえないという前提を置く）。 しかし，上述の同じグループのデータは似ている傾向が強いなど，現実のデータでは独立性を前提とすることに無理がある場合がある。 この例に限らず，階層構造を持つデータや繰り返し測定データにも，同じことがいえる。学校ごとに学力テストを行った場合，同じ学校の生徒たちは成績が似通っている可能性がある（上位校の生徒は他の学校と比べて成績が良いなど）。同一参加者に複数の実験条件に参加してもらった場合，その参加者のデータは似たような傾向を取る可能性にある。 個人差や集団差の影響を統制して独立変数が従属変数に持つ効果を検討するための統計モデルが，一般化線形混合モデルである。 一般化線形混合モデルは「階層モデル」と呼ばれることもある。 一般化線形混合モデルでは，独立変数が従属変数に及ぼす効果（固定効果：fixed effect）だけではなく，個人差や集団差を表すランダム効果（random effect）と呼ばれる影響を考慮する。一般化線形モデルは固定効果のみを含むモデルである。固定効果だけではなく，ランダム効果も混ぜたモデルなので，混合モデル(mixed model)と呼ばれる。 11.2 一般化線形混合モデル 繰り返し測定されたデータを扱う。具体例として，以下のようなデータを分析する場合をイメージしてほしい。\\(i\\)がデータを意味する番号（何行目か），\\(j\\)を個人もしくはグループを意味する番号とする。例えば，個人jが\\(x=0\\)の場合と\\(x=1\\)の場合の2回\\(y\\)を測定している，あるいは同じ集団\\(j\\)から2人選ばれて\\(y\\)が測定された，といったケースが当てはまる。 i j y x 1 1 -2.35 0 2 1 0.43 1 3 2 0.51 0 4 2 -0.57 1 5 3 -0.55 0 6 3 -0.56 1 一般化線形モデルの線形予測子（回帰式）は，以下のような数式で表現される。 \\[ y_{i} = \\beta_{0} + \\beta_{1} x_{i} \\] \\(\\beta_{0}\\)が切片，\\(\\beta_{1}\\)が独立変数\\(x\\)に係る傾きを意味する。この式から従属変数の値を予測する。 それに対し，一般化線形混合モデルの線形予測子は以下の式で表現できる。 \\[ y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\alpha_{j} \\\\ \\alpha_{j} \\sim Normal(0, \\sigma_{\\alpha}) \\] 線形予測子に，\\(\\alpha_{j}\\)が加わっている。 つまり，すべての個人に共通して影響する切片\\(\\beta_{0}a\\)と，個人ないしはグループごとに異なる値を取る切片\\(\\alpha_{j}\\)を考慮している。 更に，この切片は平均をゼロ，\\(\\sigma_{\\alpha}\\)を標準偏差とする正規分布から生成されるという仮定を置いている（多くの個人が平均して0の切片を持つが，中には0よりも大きい，ないしは小さい値を切片として持つ個人もいるという仮定）。 これにより，同じグループ（例えば\\(j=1\\)）には同じ効果（\\(\\alpha_{1}\\)）が共通して係ることを表現できる。 一般化線形混合モデルでは，すべてのデータに共通する効果（\\(\\beta_{0}\\)，\\(\\beta_{1}\\)）を固定効果（fixed effect），個体ごとに異なる効果（\\(\\alpha_{j}\\)）をランダム効果と呼ぶ。 ランダム効果は切片に限らない。例えば傾きを\\(\\beta_{1, j}\\)にする，すなわち個人ごとに独立変数に係る効果が異なるという前提を置く場合もある。 しかし，傾きもランダム効果として考慮したモデルの推定は，最尤推定法でも場合によっては解が求まらない場合がある。多くの場合，個人差の影響（ランダム効果）は切片のみを考慮したモデルで表現されることが多い。 しかし最近は，ベイズ統計手法によってランダム効果として傾きを考慮したモデルも扱われている（この講義で扱う内容を超えるので，詳細は省く）。 11.3 Rでの一般化線形混合モデル Rで一般化線形混合モデルで解析を行うためには，外部パッケージが必要になる。様々なパッケージがあるが，lme4パッケージが有名である（lmerTestも必要）。以下では，lme4パッケージに含まれるlmer()を使った解析の例を示す。 model_lmm = lmer(data= iris, Sepal.Length ~ Sepal.Width + (1|Species)) #(1|Species)を加える summary(model_lmm) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Sepal.Length ~ Sepal.Width + (1 | Species) ## Data: iris ## ## REML criterion at convergence: 194.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9846 -0.5842 -0.1182 0.4422 3.2267 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Species (Intercept) 1.0198 1.010 ## Residual 0.1918 0.438 ## Number of obs: 150, groups: Species, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.4062 0.6683 3.4050 5.097 0.0107 * ## Sepal.Width 0.7972 0.1062 146.6648 7.506 5.45e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Sepal.Width -0.486 出力結果を見てみると，Fixed effectsという部分がある。ここに，固定効果の結果が表示されている。見方は，一般化線形モデルのときと同じである。回帰式の切片(intercept)と独立変数に係る傾きの係数の推定結果が表示されている（個体差にかかわらず，すべての個体共通に係る独立変数の効果）。 ランダム効果の推定結果が，Random effectsという部分に表示されている。 上述のサンプルプログラムでは，回帰式に個体やグループを意味する変数を(1|個体やグループを意味する変数)というかたちで加えた。これは，lmer()及びglmer()特有の書き方で，「ランダム効果を切片として入れよ」という命令である。 ランダム効果の出力結果に，Std.Dev.が表示されている。これが，さきほどの式の\\(\\sigma_{\\alpha}\\)の推定結果を意味している。 がくの幅（Sepal.Width)の回帰係数（Estimate）を見ると，lm()での推定結果とは逆に，プラスになっている。やはり，グループの違いを統制すると，実際にはがくの幅が大きくなるほど，がくの長さも大きくなる関係にあることが，lmer()による推定結果からわかる。 lmer()では，デフォルトでは係数のp値は表示されない。p値もみたいのならば，lmerTest()パッケージをインストールしておく必要がある。 11.3.1 対応のある要因を含む分散分析 以下に，サンプルデータを作成する。 要因１（Factor_1）はAとBの２つの水準，要因２（Factor_2）はX，Y，Zの３つの水準をもつとする。IDは参加者を識別する番号とする。 ある実験で，参加者はAかBのどれかの群に割り振られ，その中でX, Y, Zの3つの実験条件に参加したとする。つまり，要因１は参加者間要因，要因２は参加者内要因として配置されたとする。 Score = c(6,4,5,3,2,10,8,10,8,9,11,12,12,10,10,5,4,2,2,2,7,6,5,4,3,12,8,5,6,4) Factor_1 = c(rep(&quot;A&quot;, 15), rep(&quot;B&quot;, 15)) Factor_2 = rep(c(rep(&quot;X&quot;, 5),rep(&quot;Y&quot;, 5), rep(&quot;Z&quot;, 5)),2) ID = c(1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,6,7,8,9,10,6,7,8,9,10,6,7,8,9,10) dat_anova = data.frame(Score = Score, Factor_1 = Factor_1, Factor_2 = Factor_2, ID = ID) dat_anova = dat_anova %&gt;% arrange(ID) dat_anova ## Score Factor_1 Factor_2 ID ## 1 6 A X 1 ## 2 10 A Y 1 ## 3 11 A Z 1 ## 4 4 A X 2 ## 5 8 A Y 2 ## 6 12 A Z 2 ## 7 5 A X 3 ## 8 10 A Y 3 ## 9 12 A Z 3 ## 10 3 A X 4 ## 11 8 A Y 4 ## 12 10 A Z 4 ## 13 2 A X 5 ## 14 9 A Y 5 ## 15 10 A Z 5 ## 16 5 B X 6 ## 17 7 B Y 6 ## 18 12 B Z 6 ## 19 4 B X 7 ## 20 6 B Y 7 ## 21 8 B Z 7 ## 22 2 B X 8 ## 23 5 B Y 8 ## 24 5 B Z 8 ## 25 2 B X 9 ## 26 4 B Y 9 ## 27 6 B Z 9 ## 28 2 B X 10 ## 29 3 B Y 10 ## 30 4 B Z 10 dat_anova %&gt;% group_by(Factor_1, Factor_2) %&gt;% summarise(Mean = mean(Score), SD = sd(Score), N = length(Score)) #平均値と標準偏差を出力 ## # A tibble: 6 x 5 ## # Groups: Factor_1 [2] ## Factor_1 Factor_2 Mean SD N ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A X 4 1.58 5 ## 2 A Y 9 1 5 ## 3 A Z 11 1 5 ## 4 B X 3 1.41 5 ## 5 B Y 5 1.58 5 ## 6 B Z 7 3.16 5 こういった参加者内要因と参加者間要因を含む複雑な要因配置の分析も，一般化線形混合モデルならば簡単にモデルを立てることができる。 すなわち，従属変数をScore，独立変数をFactor_1とFactor_2の主効果と交互作用効果，参加者をランダム効果とした一般化線形混合モデルを考える（誤差分布は正規分布，リンク関数は恒等リンク）。 result = lmer(data = dat_anova, Score ~ Factor_1 * Factor_2 + (1|ID)) #２つの変数を*でつなげると，主効果と交互作用効果の組み合わせを全て独立変数として投入してくれる。 summary(result) #線形混合モデルの結果。Factor_1及びFactor_2について，自動でダミー変数を作ってくれる。 ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Score ~ Factor_1 * Factor_2 + (1 | ID) ## Data: dat_anova ## ## REML criterion at convergence: 93.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.2667 -0.2333 -0.1333 0.1333 2.4000 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 2.167 1.472 ## Residual 1.000 1.000 ## Number of obs: 30, groups: ID, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 4.0000 0.7958 12.3948 5.026 0.000268 *** ## Factor_1B -1.0000 1.1255 12.3948 -0.889 0.391177 ## Factor_2Y 5.0000 0.6325 16.0000 7.906 6.47e-07 *** ## Factor_2Z 7.0000 0.6325 16.0000 11.068 6.58e-09 *** ## Factor_1B:Factor_2Y -3.0000 0.8944 16.0000 -3.354 0.004032 ** ## Factor_1B:Factor_2Z -3.0000 0.8944 16.0000 -3.354 0.004032 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Fct_1B Fct_2Y Fct_2Z F_1B:F_2Y ## Factor_1B -0.707 ## Factor_2Y -0.397 0.281 ## Factor_2Z -0.397 0.281 0.500 ## Fct_1B:F_2Y 0.281 -0.397 -0.707 -0.354 ## Fct_1B:F_2Z 0.281 -0.397 -0.354 -0.707 0.500 anova(result) #分散分析表を出力したいときは，anova()を使う。主効果と交互作用効果の検定結果が出る。 ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Factor_1 9 9.0 1 8 9.0 0.017072 * ## Factor_2 155 77.5 2 16 77.5 5.875e-09 *** ## Factor_1:Factor_2 15 7.5 2 16 7.5 0.005036 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.4 一般化線型混合モデルの拡張（ロジスティック回帰） もちろん，回帰分析に限らず，ロジスティック回帰などでもランダム効果を含めることができる。 lme4のglmer()で，誤差分布として正規分布以外のものを指定した一般化線型混合モデルを行うことができる。以下では，ランダム効果を加えたロジスティック回帰分析の例を示す。 まず，サンプルデータを作る。 x1 = c(1.0, 2.0, 3.0, 4.2, 5.1, 3.1, 4.2, 5.0, 6.1, 7.0, 5.3, 6.0, 7.0, 8.1, 9.0) y1 = c(0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) ID = c(rep(&quot;a&quot;,5),rep(&quot;b&quot;,5),rep(&quot;c&quot;,5)) data_sample = data.frame(ID, x1, y1) data_sample ## ID x1 y1 ## 1 a 1.0 0 ## 2 a 2.0 0 ## 3 a 3.0 1 ## 4 a 4.2 1 ## 5 a 5.1 1 ## 6 b 3.1 0 ## 7 b 4.2 0 ## 8 b 5.0 0 ## 9 b 6.1 0 ## 10 b 7.0 1 ## 11 c 5.3 0 ## 12 c 6.0 1 ## 13 c 7.0 1 ## 14 c 8.1 1 ## 15 c 9.0 1 x1を独立変数（量的変数），y1を従属変数（0か1のいずれかを取る），IDが個体を示す変数とする。1つの個体からx1を変えて5回，y1が計測がされたデータをイメージしてほしい。 まずは，通常のロジスティック回帰分析の復習である。Rでは，glm()でロジスティック回帰分析を行うことができた。 オプションとして，誤差分布として二項分布（binomial）, リンク関数としてロジット（logit）を指定する。 model_logistic = glm(data = data_sample, y1 ~ x1, family = binomial(link=&quot;logit&quot;)) summary(model_logistic) ## ## Call: ## glm(formula = y1 ~ x1, family = binomial(link = &quot;logit&quot;), data = data_sample) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5695 -0.8681 0.3298 0.7439 1.7326 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.3155 1.9890 -1.667 0.0955 . ## x1 0.6889 0.3796 1.815 0.0696 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 20.728 on 14 degrees of freedom ## Residual deviance: 15.577 on 13 degrees of freedom ## AIC: 19.577 ## ## Number of Fisher Scoring iterations: 4 次に，個体差を統制する。lme4パッケージのglmer()を使う。 回帰式に個体を識別する変数（ID）を加える。以下のように，(1|ID)というかたちで入れる。あとのオプションの指定などは，glm()のときと同じである。 model_logistic_glmm = glmer(data = data_sample, y1 ~ x1 + (1|ID), family = binomial(link=&quot;logit&quot;)) summary(model_logistic_glmm) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: y1 ~ x1 + (1 | ID) ## Data: data_sample ## ## AIC BIC logLik deviance df.resid ## 14.3 16.4 -4.2 8.3 12 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.01928 0.00000 0.00000 0.00000 0.04031 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 67795 260.4 ## Number of obs: 15, groups: ID, 3 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -220.575 27.154 -8.123 4.54e-16 *** ## x1 38.996 4.804 8.117 4.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## x1 -0.987 練習問題 問１ carパッケージに入っているカナダにおける職業の威信度に関する調査データPrestigeを使う。102業種に関する調査結果が入っている。 library(car) head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof prestigeを従属変数，education, income及び womenを独立変数，typeをランダム効果（切片）とした一般化線型混合モデルを行え。ただし，誤差分布として正規分布，リンク関数は恒等リンクを用いるものとする。 分析の結果，有意な効果を持った独立変数を挙げ，結論を述べよ（その独立変数が大きくなるほど，従属変数がどう変化するか）。 なお，変数の意味は以下の通りである。 prestige：職業威信度（値が高いほど威信度が高い） education：在職者の平均教育年数 income：平均所得（単位はドル） women：女性の割合 type：職業のカテゴリ（bc=ブルーカラー，wc=ホワイトカラー，prof=専門職） ヒント：ランダム効果を考慮した単なる回帰分析の場合は，lme4パッケージのlmer()を使えば良い。なお，p値を出力したい場合は，lmerTest()パッケージも必要になる。 問２ 以下の問題は，嶋田・阿部 (2017)「Rで学ぶ統計学入門」東京化学同人 p.177より抜粋。 以下のデータは，寄生蜂が生んだ子供の性比に関するデータである。motherが母蜂を意味する番号（1〜12。合計12匹），wtが宿主の体重，yが生まれた子供の性別（オス=1, メス=0）を意味する。12匹の母蜂が4匹ずつ子供を生んだ場合，宿主の大きさによってオス・メスの産み分けがされているのかを検討する。 mother = sort(rep(seq(1:12), 4)) wt = c(0.28, 0.31, 0.15, 0.36, 0.21, 0.17, 0.16, 0.41, 0.22, 0.45, 0.22, 0.33, 0.11, 0.24, 0.36, 0.32, 0.51, 0.19, 0.36, 0.28, 0.30, 0.42, 0.11, 0.56, 0.33, 0.25, 0.35, 0.15, 0.35, 0.42, 0.26, 0.45, 0.31, 0.49, 0.31, 0.43, 0.35, 0.27, 0.6, 0.29, 0.35, 0.39, 0.26, 0.15, 0.26, 0.27, 0.51, 0.36) y = c(1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0) data_q2 = data.frame(mother = mother, wt = wt, y = y) data_q2 ## mother wt y ## 1 1 0.28 1 ## 2 1 0.31 1 ## 3 1 0.15 1 ## 4 1 0.36 0 ## 5 2 0.21 1 ## 6 2 0.17 1 ## 7 2 0.16 1 ## 8 2 0.41 0 ## 9 3 0.22 1 ## 10 3 0.45 1 ## 11 3 0.22 1 ## 12 3 0.33 1 ## 13 4 0.11 1 ## 14 4 0.24 1 ## 15 4 0.36 0 ## 16 4 0.32 0 ## 17 5 0.51 0 ## 18 5 0.19 1 ## 19 5 0.36 0 ## 20 5 0.28 0 ## 21 6 0.30 1 ## 22 6 0.42 0 ## 23 6 0.11 1 ## 24 6 0.56 0 ## 25 7 0.33 1 ## 26 7 0.25 0 ## 27 7 0.35 0 ## 28 7 0.15 1 ## 29 8 0.35 0 ## 30 8 0.42 0 ## 31 8 0.26 0 ## 32 8 0.45 0 ## 33 9 0.31 0 ## 34 9 0.49 0 ## 35 9 0.31 0 ## 36 9 0.43 0 ## 37 10 0.35 0 ## 38 10 0.27 0 ## 39 10 0.60 0 ## 40 10 0.29 0 ## 41 11 0.35 1 ## 42 11 0.39 0 ## 43 11 0.26 1 ## 44 11 0.15 1 ## 45 12 0.26 0 ## 46 12 0.27 0 ## 47 12 0.51 0 ## 48 12 0.36 0 生まれた子供の性別（y）を従属変数，宿主の体重(wt)を独立変数，母蜂(mother)をランダム効果（切片）として，一般化線形混合モデルによる分析を行え。そして，寄生蜂が宿主の大きさによってオス・メスの産み分けをしているのかについて，結論を述べよ。 ヒント：ランダム効果を含むロジスティック回帰を行う。誤差分布とリンク関数として何を指定すべきか？yは0か1を取る値なので，二項分布に従うという前提を置く。 参考文献 久保拓弥 (2012). データ解析のための統計モデリング入門 岩波書店 粕谷英一 (2012). 一般化線型モデル 共立出版 嶋田正和・阿部真人(2017). Rで学ぶ統計学入門 東京化学同人 "],
["tidyverse.html", "A tidyverseの解説 A.1 tibble A.2 ggplot2 A.3 dplyr A.4 purrr A.5 readr A.6 tidyr A.7 read_excel", " A tidyverseの解説 tidyverseパッケージを使うことで，「モダンな操作」が可能となる。 install.packages(&quot;tidyverse&quot;) library(tidyverse) tidyverseは様々なパッケージのセットである。Rを使いやすくするための便利なパッケージをまとめたものである。 具体的には，“tibble”, “ggplot2”, “dplyr”, “tidyr” といった様々なパッケージを含んでいる。ここでは，tidyverseの中に含まれるパッケージについて，「心理データ解析法」でよく使うものに限って解説をする。 以降のプログラムで関数の前についている「ggplot2::」などは省略しても良いが，つけておいたほうが無難。 tidyverse以外のパッケージも読み込んでいる場合，同じ名前の関数を持つパッケージが複数あるとエラーが生じてしまう。どのパッケージの関数を使うのかを指定してあげたほうが良い。 A.1 tibble tibbleとは，新たなRのデータ形式である。 これまでRで分析をする際には，データをデータフレーム（data.frame）という形式で使ってきた。 #データフレーム x = c(1, 2, 3) y = c(4, 5, 6) z = data.frame(x, y) z ## x y ## 1 1 4 ## 2 2 5 ## 3 3 6 as_tibble()でデータをtibble形式にできる。 z_tibble = as_tibble(z) z_tibble ## # A tibble: 3 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows tibbleは，data.frameよりも可読性を向上させたのが特徴である。コンソールにはデータ全てではなく，最初の10行程度で表示される（変数名も見やすい）。 自分でtibble型のデータを作ることも可能。 dat = tibble::tibble(x = c(1, 2, 3), c(4, 5, 6)) dat ## # A tibble: 3 x 2 ## x `c(4, 5, 6)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 ## 2 2 5 ## 3 3 6 A.2 ggplot2 グラフの作成に特化したパッケージである。Rにも標準でグラフィック関数が用意されているが，あまりきれいではないし，グラフの編集もやりにくい。ggplot2ならば，特に何もオプションを指定しなくても見やすいグラフを出力してくれる。 p &lt;- ggplot2::ggplot(data = iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point(aes(shape = Species)) p &lt;- p + xlab(&quot;Length of Sepal&quot;) + ylab(&quot;Length of Petal&quot;) p 軸のラベル，点の色，点の形などもユーザーが自由に指定できる。 ggplot2の使い方の詳細については，ここでは省略する。 gglot2パッケージのqplot()でも手っ取り早くグラフを作成することが出来る。とりあえずデータの傾向を見たいだけなら，qplot()を使うのが良い。x軸もしくはy軸に指定したい変数を入れればできるので，これくらいは覚えておくと良い。 x軸とy軸に表示する変数を2つ入れれば散布図，x軸の変数1つだけならヒストグラムがデフォルトで表示される。geomでグラフの種類を指定することも可能。 ggplot2::qplot(data = iris, x = Sepal.Length, y = Petal.Length) ggplot2::qplot(data = ChickWeight, x = weight) ggplot2::qplot(data = InsectSprays, x = spray, y = count, geom=&quot;boxplot&quot;) A.3 dplyr データを操作するのに特化した関数が用意されている。 * mutate(): 変数の追加 * filter(): 行の抽出 * select(): 編集の抽出 * summarise(): 複数の変数を1つにまとめる * arrange(): データを並べ替える * group_by(): グループにまとめる dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows dat2 = dplyr::mutate(dat, z = Sepal.Length * 2) #新しい変数zを追加する dat2 ## # A tibble: 150 x 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 10.2 ## 2 4.9 3 1.4 0.2 setosa 9.8 ## 3 4.7 3.2 1.3 0.2 setosa 9.4 ## 4 4.6 3.1 1.5 0.2 setosa 9.2 ## 5 5 3.6 1.4 0.2 setosa 10 ## 6 5.4 3.9 1.7 0.4 setosa 10.8 ## 7 4.6 3.4 1.4 0.3 setosa 9.2 ## 8 5 3.4 1.5 0.2 setosa 10 ## 9 4.4 2.9 1.4 0.2 setosa 8.8 ## 10 4.9 3.1 1.5 0.1 setosa 9.8 ## # … with 140 more rows dat2 = dplyr::filter(dat, Species == &quot;versicolor&quot;) #Speciesのうち，versicolorのみを取り出す。「イコール」は=ではなく，==にするのに注意（計算式と条件式ではイコールの表現が異なる）。 dat2 ## # A tibble: 50 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 7 3.2 4.7 1.4 versicolor ## 2 6.4 3.2 4.5 1.5 versicolor ## 3 6.9 3.1 4.9 1.5 versicolor ## 4 5.5 2.3 4 1.3 versicolor ## 5 6.5 2.8 4.6 1.5 versicolor ## 6 5.7 2.8 4.5 1.3 versicolor ## 7 6.3 3.3 4.7 1.6 versicolor ## 8 4.9 2.4 3.3 1 versicolor ## 9 6.6 2.9 4.6 1.3 versicolor ## 10 5.2 2.7 3.9 1.4 versicolor ## # … with 40 more rows dat2 = dplyr::select(dat, Sepal.Length, Petal.Length) #データから，Sepal.LengthとPetal.Lengthの列を取り出す。 dat2 ## # A tibble: 150 x 2 ## Sepal.Length Petal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5 1.4 ## 6 5.4 1.7 ## 7 4.6 1.4 ## 8 5 1.5 ## 9 4.4 1.4 ## 10 4.9 1.5 ## # … with 140 more rows dat2 = dplyr::summarise(dat, M = mean(Sepal.Length), SD = sd(Sepal.Length)) #Sepal.Lengthの平均を求め，Mという変数で保存する。同じく，SDという変数を新たに作る。 dat2 ## # A tibble: 1 x 2 ## M SD ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 0.828 dat2 = dplyr::arrange(dat, Sepal.Length) #Sepal.Lengthを小さい順に並び替える。 dat2 ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 4.3 3 1.1 0.1 setosa ## 2 4.4 2.9 1.4 0.2 setosa ## 3 4.4 3 1.3 0.2 setosa ## 4 4.4 3.2 1.3 0.2 setosa ## 5 4.5 2.3 1.3 0.3 setosa ## 6 4.6 3.1 1.5 0.2 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 4.6 3.6 1 0.2 setosa ## 9 4.6 3.2 1.4 0.2 setosa ## 10 4.7 3.2 1.3 0.2 setosa ## # … with 140 more rows dat2 = dplyr::group_by(dat, Species) #Speciesをグループとしてまとめる。新しくGroupsというものが作られている。（使い方については後述） dat2 ## # A tibble: 150 x 5 ## # Groups: Species [3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows A.4 purrr パイプ処理が使える。パイプ（%&gt;%）で関数をつなげることで，1行のプログラムで複数の処理をすることができる。 dat = tibble::as_tibble(iris) #irisデータをtibble型にして，datという名前で保存する dat ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows dat2 = dat %&gt;% dplyr::filter(Species == &quot;setosa&quot;) %&gt;% dplyr::select(Sepal.Width, Sepal.Length) %&gt;% arrange(Sepal.Width) #Speciesがsetosaだけの行を抽出し，更にSepal.WidthとSepal.Lengthの列を取り出し，Sepal.Widthの小さい順に並び替える。これを1行で実行 dat2 ## # A tibble: 50 x 2 ## Sepal.Width Sepal.Length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.3 4.5 ## 2 2.9 4.4 ## 3 3 4.9 ## 4 3 4.8 ## 5 3 4.3 ## 6 3 5 ## 7 3 4.4 ## 8 3 4.8 ## 9 3.1 4.6 ## 10 3.1 4.9 ## # … with 40 more rows dat2 = dat %&gt;% dplyr::group_by(Species) %&gt;% dplyr::summarise(M = mean(Sepal.Width), SD = sd(Sepal.Width), N = length(Sepal.Width)) #Speciesでグループにし，グループごとにSepal.Widthの平均，標準偏差，N数を出力する。 dat2 ## # A tibble: 3 x 4 ## Species M SD N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 setosa 3.43 0.379 50 ## 2 versicolor 2.77 0.314 50 ## 3 virginica 2.97 0.322 50 A.5 readr 様々な形式のファイルを高速で読み書きことを目標としたパッケージ。 dat = readr::read_csv(&quot;0_sample.csv&quot;) ## Parsed with column specification: ## cols( ## X = col_double(), ## Y = col_double(), ## Gender = col_character() ## ) dat ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F readr::write_excel_csv(dat, &quot;0_sample_2.csv&quot;) read_csv()ならば，csvのファイルをtibble形式で読み込んでくれる。 * read.csv()ではなく，read_csv()なので注意（ドットではなく，アンダースコア）。 ファイルを書き出すための関数も用意されている。 * ここでは，write_excel_csv() を使っている。単にwrite_csv()でも可。 A.6 tidyr 後日更新。 A.7 read_excel エクセル形式（xlsx）のファイルを読み込む。後日更新。 "],
["section-13.html", "B 使い方まとめ B.1 tidyverseパッケージのインストールとロード B.2 データの読み込み B.3 結果の集計 B.4 グラフの作成 B.5 新しい変数を作る B.6 データを保存する", " B 使い方まとめ 「心理学データ解析法」で使う手順に沿って，確認する。 B.1 tidyverseパッケージのインストールとロード 帝京大学のコンピュータルームのマシンは電源をいれるたびに設定がリセットされてしまうので，毎回はじめにこれを行う。 install.packages(&quot;tidyverse&quot;) library(tidyverse) B.2 データの読み込み read_csv()でcsvファイルを読み込む。tibble形式でデータが表示される。 dat = read_csv(&quot;0_sample.csv&quot;) dat ## # A tibble: 5 x 3 ## X Y Gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 6 M ## 2 7 3 F ## 3 7 6 M ## 4 3 3 M ## 5 4 4 F 今日の課題は以下だったとする。 性別（Gender）ごとに，変数X及びYの平均値と標準偏差を計算する。 変数XとYの散布図を出す。 新しい変数Z=2X+Yを作って，データに付け足す。 新しい変数を加えたデータを名前をつけて保存する。 B.3 結果の集計 group_byでグループにごとに出したい変数（i.e., Gender）を指定し， summariseでまとめる。 パイプ(%&gt;%)を使えば1つのプログラムで実行可能。 dat %&gt;% group_by(Gender) %&gt;% summarise(Mean_X = mean(X), SD_X = sd(X), Mean_Y = mean(Y), SD_Y = sd(Y)) ## # A tibble: 2 x 5 ## Gender Mean_X SD_X Mean_Y SD_Y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 5.5 2.12 3.5 0.707 ## 2 M 4.67 2.08 5 1.73 B.4 グラフの作成 ggplot2パッケージを使う。qplot()で十分。 qplot(data=dat, x=X, y=Y) B.5 新しい変数を作る mutate()を使う。 dat2 = dat %&gt;% mutate(Z=X+2*Y) dat2 ## # A tibble: 5 x 4 ## X Y Gender Z ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 6 M 16 ## 2 7 3 F 13 ## 3 7 6 M 19 ## 4 3 3 M 9 ## 5 4 4 F 12 #以下でも可（昔ながらの表記） dat2 = dat dat2$Z = dat2$X + 2*dat2$Y dat2 ## # A tibble: 5 x 4 ## X Y Gender Z ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 6 M 16 ## 2 7 3 F 13 ## 3 7 6 M 19 ## 4 3 3 M 9 ## 5 4 4 F 12 B.6 データを保存する write_excel_csv()を使う。 write_excel_csv(dat2, &quot;0_sample_2.csv&quot;) "],
["10.html", "C 第10章「一般化線形モデル」の付録 C.1 １．指数と対数 C.2 ２．ロジスティック関数とロジット関数の関係 C.3 ３．ロジスティック回帰の係数の意味 C.4 ４．ポアソン回帰で対数をリンク関数とする理由", " C 第10章「一般化線形モデル」の付録 以下の説明は，高校数学の知識があればわかる内容となっている（はず）。この付録を読まなくても，ロジスティック回帰の理解に差し障りはない（はず）。 C.1 １．指数と対数 指数と対数は，以下の関係にある。 \\[ y = e^x \\\\ \\log_{e}y = x \\\\ \\] \\(e\\)とはネイピア数と呼ばれるもので（自然対数の底とも呼ばれる），\\(e=\\) 2.7182818の一定の値を取る（円周率\\(\\pi\\)と同様に，数学において扱われる重要な定数である）。 一般的に，ネイピア数は指数関数の底（\\(f(x)=a^x\\)の\\(a\\)に当たる部分）の底としてよく用いられる。 なお，\\(e^x\\)は，\\(\\exp(x)\\)とも表記する。以降の記述でも，\\(\\exp(x)\\)の方を使う。 また，\\(e\\)を底とする対数（自然対数）は，\\(\\log_{e}y\\)は単に\\(\\log y\\)と底を省略して表記される。 すなわち，まとめるとネイピア数を底とする指数及び対数の関係は以下のようになる。 \\[ y = \\exp(x) \\\\ \\log y = x \\\\ \\] Rでは\\(\\exp(x)\\)の値を求める関数exp()が用意されている。また，\\(\\log x\\)もlog()関数で求めることができる。 exp(1) #x=1のとき，つまりネイピア数eが出力される ## [1] 2.718282 exp(-1) #xが負の値のとき ## [1] 0.3678794 exp(-1000) #xが負の値のとき ## [1] 0 対数と指数は逆関数の関係にある。 log(exp(100)) ## [1] 100 exp(log(100)) ## [1] 100 C.1.1 指数関数 以下が，\\(y=e^x\\)のグラフである。 x = seq(-3, 3, 0.01) y = exp(x) exp_graph = data.frame(y=y, x=x) ggplot(data = exp_graph, aes(x=x, y=y)) + geom_line() このように，指数関数は，xが負の値をとってもyは0に漸近するが0にはならないという性質がある。 C.1.2 対数関数 以下が，\\(y=\\log x\\)のグラフである。 x = seq(0, 3, 0.01) y = log(x) log_graph = data.frame(y=y, x=x) ggplot(data = log_graph, aes(x=x, y=y)) + geom_line() C.2 ２．ロジスティック関数とロジット関数の関係 以下のロジスティック関数（式(3)）は， \\[ q_{i} = \\frac{1}{1+\\exp[-(\\beta_{0} + \\beta_{1}x_{i})]} \\tag{3} \\] ロジット関数（式(4)）にも変形できる。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] なぜか。式(3)から式(4)へ整理してみる（\\(y_{i} = \\beta_{0} + \\beta_{1} x_{i}\\)とする）。 \\[ q_{i} = \\frac{1}{1+\\exp(-y_{i})} \\\\ q_{i}(1+\\exp(-y_{i})) =1 \\\\ q_{i} \\biggl\\{ 1+\\frac{1}{\\exp(y_{i})} \\biggl\\} = 1 \\\\ q_{i}+\\frac{1}{\\exp(y_{i})}q_{i} = 1 \\\\ q_{i}\\exp(y_{i})+q_{i} = \\exp(y_{i}) \\\\ q_{i} = \\exp(y_{i}) - q_{i}\\exp(y_{i}) \\\\ q_{i} = \\exp(y_{i})(1 - q_{i}) \\\\ \\frac{q_{i}}{1-q_{i}} = \\exp(y_{i})\\\\ \\log \\frac{q_{i}}{1-q_{i}} = y_{i}\\\\ \\] このように，ロジット関数はロジスティック関数へ変形することが可能。逆に，ロジスティック関数からロジット関数へ変形することも可能。ロジット関数はロジスティック関数の逆関数であり，ロジスティック関数はロジット関数の逆関数である。 C.3 ３．ロジスティック回帰の係数の意味 ロジスティック回帰で推定した回帰式で求める予測値yそのものは，確率を意味しない。 例えば，本文の例題で求めた係数をもとに，独立変数V1=1を代入してみると， y = -5.16017 + 0.93546 * 1 y ## [1] -4.22471 マイナスの値である。推定したいのは確率なのに，0から1の範囲に収まらない。 なぜならば，線形予測子から推定されるのは本文の式(4)で示したように，正確には\\(\\log q_{i}/(1-q_{i})\\)だからである。 \\[ \\log\\frac{q_{i}}{1-q_{i}} = \\beta_{0} + \\beta_{1} x_{i} \\tag{4}\\\\ \\] \\(q_{i}/(1-q_{i})\\)はオッズ(odds)と呼ばれるもので，ある事象が生じる確率と生じない確率の比を意味する（\\(q_{i} = 0.5\\)ならばオッズは1，\\(q_{i} = 0.9\\)ならばオッズは9になる）。 線形予測子から推定されるのは，オッズの対数である（対数オッズ）。 また，式(4)は，以下の式(4’)に変形できる。 \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(\\beta_{0} + \\beta_{1} x_{i}) \\tag{4&#39;}\\\\ \\] \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(\\beta_{0}) \\exp(\\beta_{1} x_{i}) \\\\ \\] 例題で推定された切片及び傾きのパラメータを入れると， \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94 x_{i}) \\\\ \\] 独立変数\\(x_{i}\\)が一単位増えた場合， \\[ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94 (x_{i}+1)) \\\\ \\frac{q_{i}}{1-q_{i}} = \\exp(-5.16) \\exp(0.94x_{i})\\exp(0.94) \\\\ \\] つまり，オッズ\\(q_{i}/(1-q_{i})\\)は，\\(\\exp(0.94)\\)倍になる。\\(\\exp(0.94)\\)は2.56であるので，2.56倍である。 exp(0.94) ## [1] 2.559981 すなわち，ロジスティック回帰の係数（の指数）が意味することは，1単位増えたときにオッズが何倍になるかである（オッズ比という。「XXXをすると，病気のリスクがしないときよりもX倍になる」というイメージ）。 係数が正だとオッズ比は1を超える値を取り，負だと0以上1未満の値を取る。 exp(1) ## [1] 2.718282 exp(-1) ## [1] 0.3678794 線形予測子から，確率\\(q_{i}\\)を直接求めるならば，式(3)を使えば良い。 1/(1+exp(-1*y)) ## [1] 0.01441864 C.4 ４．ポアソン回帰で対数をリンク関数とする理由 ポアソン分布を誤差分布とするとき，リンク関数は対数(log)を用いた。 \\(\\lambda\\)はポアソン分布の平均値を意味する。 \\[ \\log\\lambda = \\beta_{0} + \\beta_{1} x \\\\ \\] これを変形すると， \\[ \\exp(\\log \\lambda) = \\exp(\\beta_{0} + \\beta_{1} x) \\\\ \\lambda = \\exp(\\beta_{0} + \\beta_{1} x) \\\\ \\] となる。 線形予測子が負の値であっても，exp(線形予測子)は正の値を取る（指数関数の式を参照）。ポアソン分布のパラメータ\\(\\lambda\\)は\\(\\lambda&gt;0\\)でなければならないが，対数をリンク関数とすることでその前提が満たされる。 "]
]
